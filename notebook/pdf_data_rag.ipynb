{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cb3f9e2",
   "metadata": {},
   "source": [
    "# Data Ingestion Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b8b2758",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader \n",
    "\n",
    "loader = TextLoader(\"../data/text_files/llm_learning.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b889d58b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '../data/text_files/llm_learning.txt'}, page_content=\"Large Language Models (LLMs) are advanced artificial intelligence systems designed to understand and generate human-like text. They are trained on vast amounts of textual data and can perform a variety of tasks such as answering questions, summarizing content, translating languages, and even writing code.\\n\\n### Key Concepts\\n\\n1. **Training Data**: LLMs learn from large datasets containing books, articles, websites, and more. The quality and diversity of this data influence the model's capabilities.\\n\\n2. **Tokens**: LLMs process text as sequences of tokens, which are often words or subwords. Understanding tokenization is important for working with LLMs.\\n\\n3. **Prompting**: You interact with an LLM by providing a prompt—a piece of text or a question. The model generates a response based on this input.\\n\\n4. **Fine-tuning**: While base LLMs are trained on general data, they can be fine-tuned on specific datasets to specialize in certain tasks or domains.\\n\\n### Common Use Cases\\n\\n- **Chatbots and Virtual Assistants**\\n- **Content Generation**\\n- **Text Summarization**\\n- **Code Generation**\\n- **Question Answering**\\n\\n### Getting Started\\n\\nTo use an LLM, you typically need access to an API or a library such as OpenAI's GPT, Google's PaLM, or open-source models like Llama or Falcon. You can interact with these models using Python libraries such as `langchain`.\\n\\nExample (pseudo-code):\\n\")]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document = loader.load()\n",
    "document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77d3d4cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': '../data/text_files/llm_learning.txt'}, page_content=\"Large Language Models (LLMs) are advanced artificial intelligence systems designed to understand and generate human-like text. They are trained on vast amounts of textual data and can perform a variety of tasks such as answering questions, summarizing content, translating languages, and even writing code.\\n\\n### Key Concepts\\n\\n1. **Training Data**: LLMs learn from large datasets containing books, articles, websites, and more. The quality and diversity of this data influence the model's capabilities.\\n\\n2. **Tokens**: LLMs process text as sequences of tokens, which are often words or subwords. Understanding tokenization is important for working with LLMs.\\n\\n3. **Prompting**: You interact with an LLM by providing a prompt—a piece of text or a question. The model generates a response based on this input.\\n\\n4. **Fine-tuning**: While base LLMs are trained on general data, they can be fine-tuned on specific datasets to specialize in certain tasks or domains.\\n\\n### Common Use Cases\\n\\n- **Chatbots and Virtual Assistants**\\n- **Content Generation**\\n- **Text Summarization**\\n- **Code Generation**\\n- **Question Answering**\\n\\n### Getting Started\\n\\nTo use an LLM, you typically need access to an API or a library such as OpenAI's GPT, Google's PaLM, or open-source models like Llama or Falcon. You can interact with these models using Python libraries such as `langchain`.\\n\\nExample (pseudo-code):\\n\"), Document(metadata={'source': '../data/text_files/langchain_learning.txt'}, page_content='LangChain is a powerful framework designed to simplify the development of applications powered by large language models (LLMs). It provides tools and abstractions for connecting LLMs to various data sources, chaining together multiple components, and building advanced workflows such as Retrieval-Augmented Generation (RAG), chatbots, and more.\\n\\n### Key Concepts\\n\\n1. **Chains**: Chains are sequences of calls (to LLMs, APIs, or other utilities) that can be combined to perform complex tasks. For example, you can chain together a prompt template, an LLM, and a post-processing step.\\n\\n2. **Agents**: Agents use LLMs to decide which actions to take, such as querying a database or searching the web, based on user input.\\n\\n3. **Tools**: Tools are external functions or APIs that agents can use to perform specific tasks, like searching documents or performing calculations.\\n\\n4. **Memory**: LangChain provides memory modules to help applications remember previous interactions, enabling more context-aware conversations.\\n\\n5. **Document Loaders and Embeddings**: LangChain can ingest data from various sources (PDFs, CSVs, web pages, etc.), split them into chunks, and generate embeddings for semantic search and retrieval.\\n\\n### Common Use Cases\\n\\n- **Conversational AI and Chatbots**\\n- **Retrieval-Augmented Generation (RAG)**\\n- **Question Answering over Documents**\\n- **Automated Workflows and Agents**\\n- **Custom LLM Applications**\\n\\n### Getting Started\\n\\nTo use LangChain, install it via pip:\\n1. Install LangChain and its dependencies using pip:\\n\\n   ```\\n   pip install langchain\\n   ```\\n\\n2. Import the necessary modules in your Python script. For example, to use a language model and a prompt template:\\n\\n   ```python\\n   from langchain.llms import OpenAI\\n   from langchain.prompts import PromptTemplate\\n   ```\\n\\n3. Create a prompt template to structure the input for your language model. Prompt templates help standardize the way you interact with LLMs:\\n\\n   ```python\\n   prompt = PromptTemplate(\\n       input_variables=[\"topic\"],\\n       template=\"Tell me about {topic}.\"\\n   )\\n   ```\\n\\n4. Initialize your language model. LangChain supports various providers, including OpenAI, Hugging Face, and more. Here’s an example with OpenAI:\\n\\n   ```python\\n   llm = OpenAI(openai_api_key=\"your-api-key\")\\n   ```\\n\\n5. Combine the prompt and the LLM into a chain. Chains allow you to link together multiple steps, such as formatting a prompt and generating a response:\\n\\n   ```python\\n   from langchain.chains import LLMChain\\n\\n   chain = LLMChain(llm=llm, prompt=prompt)\\n   ```\\n\\n6. Run the chain with your desired input to generate a response:\\n\\n   ```python\\n   response = chain.run(\"LangChain\")\\n   print(response)\\n   ```\\n\\n7. To work with documents, use document loaders. LangChain provides loaders for PDFs, text files, web pages, and more. For example, to load a text file:\\n\\n   ```python\\n   from langchain.document_loaders import TextLoader\\n\\n   loader = TextLoader(\"example.txt\")\\n   documents = loader.load()\\n   ```\\n\\n8. Split documents into manageable chunks for processing and embedding. This is useful for tasks like semantic search:\\n\\n   ```python\\n   from langchain.text_splitter import CharacterTextSplitter\\n\\n   text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\\n   docs = text_splitter.split_documents(documents)\\n   ```\\n\\n9. Generate embeddings for each chunk to enable semantic search and retrieval. LangChain integrates with embedding providers like OpenAI and Hugging Face:\\n\\n   ```python\\n   from langchain.embeddings import OpenAIEmbeddings\\n\\n   embeddings = OpenAIEmbeddings(openai_api_key=\"your-api-key\")\\n   doc_embeddings = embeddings.embed_documents([doc.page_content for doc in docs])\\n   ```\\n\\n10. Store and search embeddings using a vector database. LangChain supports vector stores like FAISS, Pinecone, and Chroma. For example, using FAISS:\\n\\n    ```python\\n    from langchain.vectorstores import FAISS\\n\\n    db = FAISS.from_documents(docs, embeddings)\\n    results = db.similarity_search(\"What is LangChain?\", k=3)\\n    for result in results:\\n        print(result.page_content)\\n    ```\\n\\nThese steps provide a basic workflow for building applications with LangChain. The framework is highly modular, allowing you to customize each component to fit your specific needs. You can chain together multiple LLMs, integrate external APIs as tools, and build sophisticated agents that reason and act based on user input.\\n\\nLangChain also supports memory modules, which enable your applications to remember previous interactions. This is especially useful for building conversational agents that need to maintain context over multiple turns.\\n\\nFor more advanced use cases, you can create custom tools and integrate them with agents. Tools can be anything from a calculator to a web search API, and agents can decide when and how to use them based on the conversation.\\n\\nLangChain’s document loaders and text splitters make it easy to ingest and preprocess data from a variety of sources. This is essential for building retrieval-augmented generation (RAG) systems that combine LLMs with external knowledge.\\n\\nThe framework is open-source and has an active community. You can find extensive documentation, tutorials, and example projects on the official LangChain GitHub repository and website.\\n\\nBy leveraging LangChain, you can rapidly prototype and deploy powerful LLM-driven applications, from chatbots and virtual assistants to knowledge management systems and automated workflows.\\n\\nWhether you are a beginner or an experienced developer, LangChain provides the building blocks you need to harness the full potential of large language models in your projects.\\n\\n')]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "# Example usage:\n",
    "loader = DirectoryLoader(\"../data/text_files/\", loader_cls=TextLoader, loader_kwargs={\"encoding\": \"utf-8\"}, show_progress=False, glob=\"*.txt\")\n",
    "documents = loader.load()\n",
    "print(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "948629ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:04+00:00', 'source': '../data/pdf_files/agentic_ai_tutorial.pdf', 'file_path': '../data/pdf_files/agentic_ai_tutorial.pdf', 'total_pages': 74, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:22:04+00:00', 'trapped': '', 'modDate': 'D:20250918142204Z', 'creationDate': \"D:20250918142204+00'00'\", 'page': 0}, page_content='Agentic AI Tutorial: Building Intelligent\\nAutonomous Systems\\nTable of Contents\\n1. Introduction to Agentic AI\\n2. Core Concepts and Architecture\\n3. Agent Frameworks\\n4. Planning and Reasoning\\n5. Memory and Knowledge Management\\n6. Tool Use and Actions\\n7. Multi-Agent Systems\\n8. Real-World Applications\\n9. Implementation Guide\\n10. Best Practices\\nIntroduction to Agentic AI\\nAgentic AI refers to AI systems that can act autonomously to achieve goals, make decisions, and interact with their environment. Unlike traditional AI that responds to\\nprompts, agentic systems proactively plan, execute, and adapt their behavior.\\nKey Characteristics:\\nAutonomy: Can operate without constant human supervision\\nGoal-oriented: Pursues objectives over multiple steps\\nAdaptive: Learns from experience and adjusts strategies\\nInteractive: Communicates with humans and other systems\\nPersistent: Maintains context and memory across sessions\\nAgent Types:\\nReactive Agents: Respond to immediate stimuli\\nDeliberative Agents: Plan before acting\\nHybrid Agents: Combine reactive and deliberative approaches\\nLearning Agents: Improve performance over time\\nCore Concepts and Architecture\\nAgent Architecture Components'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:04+00:00', 'source': '../data/pdf_files/agentic_ai_tutorial.pdf', 'file_path': '../data/pdf_files/agentic_ai_tutorial.pdf', 'total_pages': 74, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:22:04+00:00', 'trapped': '', 'modDate': 'D:20250918142204Z', 'creationDate': \"D:20250918142204+00'00'\", 'page': 1}, page_content='from abc import ABC, abstractmethod \\nfrom typing import Dict, List, Any, Optional \\nimport json \\nclass Agent(ABC): \\n    \"\"\"Base agent architecture\"\"\" \\n     \\n    def __init__(self, name: str, goals: List[str]): \\n        self.name = name \\n        self.goals = goals \\n        self.memory = {} \\n        self.knowledge_base = {} \\n        self.tools = {} \\n         \\n    @abstractmethod \\n    def perceive(self, environment: Dict) -> Dict: \\n        \"\"\"Perceive the current environment state\"\"\" \\n        pass \\n     \\n    @abstractmethod \\n    def plan(self, observations: Dict) -> List[Dict]: \\n        \"\"\"Create action plan based on observations\"\"\" \\n        pass \\n     \\n    @abstractmethod \\n    def act(self, action: Dict) -> Dict: \\n        \"\"\"Execute an action\"\"\" \\n        pass \\n     \\n    def update_memory(self, experience: Dict): \\n        \"\"\"Update agent\\'s memory with new experience\"\"\" \\n        timestamp = experience.get(\\'timestamp\\', \\'unknown\\') \\n        self.memory[timestamp] = experience \\n     \\n    def reflect(self): \\n        \"\"\"Reflect on past experiences and update strategies\"\"\" \\n        # Analyze recent experiences \\n        recent_experiences = list(self.memory.values())[-10:] \\n         \\n        # Extract patterns and lessons \\n        successful_actions = [exp for exp in recent_experiences if exp.get(\\'success\\', False)] \\n        failed_actions = [exp for exp in recent_experiences if not exp.get(\\'success\\', True)] \\n         \\n        # Update knowledge base \\n        self.knowledge_base[\\'successful_patterns\\'] = successful_actions \\n        self.knowledge_base[\\'failure_patterns\\'] = failed_actions \\nPerception-Planning-Action Loop'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:04+00:00', 'source': '../data/pdf_files/agentic_ai_tutorial.pdf', 'file_path': '../data/pdf_files/agentic_ai_tutorial.pdf', 'total_pages': 74, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:22:04+00:00', 'trapped': '', 'modDate': 'D:20250918142204Z', 'creationDate': \"D:20250918142204+00'00'\", 'page': 2}, page_content='class AutonomousAgent(Agent): \\n    \"\"\"Agent with perception-planning-action loop\"\"\" \\n     \\n    def __init__(self, name: str, goals: List[str], llm_model): \\n        super().__init__(name, goals) \\n        self.llm = llm_model \\n         \\n    def perceive(self, environment: Dict) -> Dict: \\n        \"\"\"Analyze environment and extract relevant information\"\"\" \\n        perception_prompt = f\"\"\" \\n        Analyze the current environment and identify: \\n        1. Relevant objects, entities, or data \\n        2. Current state and context \\n        3. Opportunities for action \\n        4. Potential obstacles or constraints \\n         \\n        Environment: {environment} \\n        Agent Goals: {self.goals} \\n        \"\"\" \\n         \\n        response = self.llm.generate(perception_prompt) \\n        return {\"observations\": response, \"environment\": environment} \\n     \\n    def plan(self, observations: Dict) -> List[Dict]: \\n        \"\"\"Generate step-by-step action plan\"\"\" \\n        planning_prompt = f\"\"\" \\n        Given these observations and goals, create a detailed action plan: \\n         \\n        Observations: {observations[\\'observations\\']} \\n        Goals: {self.goals} \\n        Available Tools: {list(self.tools.keys())} \\n        Past Experience: {self.knowledge_base} \\n         \\n        Provide a step-by-step plan with: \\n        - Action type \\n        - Required tools \\n        - Expected outcomes \\n        - Contingency plans \\n        \"\"\" \\n         \\n        plan_response = self.llm.generate(planning_prompt) \\n        # Parse response into structured actions \\n        return self._parse_plan(plan_response) \\n     \\n    def act(self, action: Dict) -> Dict: \\n        \"\"\"Execute a single action\"\"\" \\n        action_type = action.get(\\'type\\') \\n         \\n        if action_type == \\'tool_use\\': \\n            return self._use_tool(action) \\n        elif action_type == \\'communication\\': \\n            return self._communicate(action) \\n        elif action_type == \\'analysis\\': \\n            return self._analyze(action) \\n        else: \\n            return {\"success\": False, \"error\": f\"Unknown action type: {action_type}\"} \\n     \\n    def run(self, environment: Dict, max_iterations: int = 10): \\n        \"\"\"Main agent execution loop\"\"\" \\n        for iteration in range(max_iterations): \\n            # Perceive \\n            observations = self.perceive(environment)'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:04+00:00', 'source': '../data/pdf_files/agentic_ai_tutorial.pdf', 'file_path': '../data/pdf_files/agentic_ai_tutorial.pdf', 'total_pages': 74, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:22:04+00:00', 'trapped': '', 'modDate': 'D:20250918142204Z', 'creationDate': \"D:20250918142204+00'00'\", 'page': 3}, page_content='# Plan \\n            actions = self.plan(observations) \\n             \\n            # Act \\n            for action in actions: \\n                result = self.act(action) \\n                 \\n                # Update memory \\n                experience = { \\n                    \\'iteration\\': iteration, \\n                    \\'action\\': action, \\n                    \\'result\\': result, \\n                    \\'success\\': result.get(\\'success\\', False), \\n                    \\'timestamp\\': f\"iter_{iteration}\" \\n                } \\n                self.update_memory(experience) \\n                 \\n                # Check if goal achieved \\n                if self._goal_achieved(result): \\n                    return {\"status\": \"success\", \"iterations\": iteration + 1} \\n                 \\n                # Update environment based on action result \\n                environment = self._update_environment(environment, result) \\n             \\n            # Reflect on progress \\n            if iteration % 3 == 0:  # Reflect every 3 iterations \\n                self.reflect() \\n         \\n        return {\"status\": \"incomplete\", \"iterations\": max_iterations} \\nAgent Frameworks\\nLangGraph Agent'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:04+00:00', 'source': '../data/pdf_files/agentic_ai_tutorial.pdf', 'file_path': '../data/pdf_files/agentic_ai_tutorial.pdf', 'total_pages': 74, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:22:04+00:00', 'trapped': '', 'modDate': 'D:20250918142204Z', 'creationDate': \"D:20250918142204+00'00'\", 'page': 4}, page_content='from langgraph.graph import StateGraph, END \\nfrom langchain_core.messages import BaseMessage \\nfrom typing import TypedDict, List \\nclass AgentState(TypedDict): \\n    messages: List[BaseMessage] \\n    plan: List[str] \\n    current_step: int \\n    tools_used: List[str] \\n    goal_status: str \\ndef create_langgraph_agent(): \\n    \"\"\"Create agent using LangGraph framework\"\"\" \\n     \\n    def planning_node(state: AgentState): \\n        # Generate plan based on current messages \\n        planner_prompt = f\"Create a plan to achieve: {state[\\'messages\\'][-1].content}\" \\n        # Use LLM to generate plan \\n        plan = [\"step1\", \"step2\", \"step3\"]  # Simplified \\n        return {\"plan\": plan, \"current_step\": 0} \\n     \\n    def execution_node(state: AgentState): \\n        current_step = state[\"current_step\"] \\n        if current_step < len(state[\"plan\"]): \\n            # Execute current step \\n            step = state[\"plan\"][current_step] \\n            # Simulate step execution \\n            return { \\n                \"current_step\": current_step + 1, \\n                \"tools_used\": state[\"tools_used\"] + [f\"tool_for_{step}\"] \\n            } \\n        return {\"goal_status\": \"completed\"} \\n     \\n    def should_continue(state: AgentState): \\n        if state.get(\"goal_status\") == \"completed\": \\n            return \"end\" \\n        elif state[\"current_step\"] >= len(state.get(\"plan\", [])): \\n            return \"replan\" \\n        else: \\n            return \"execute\" \\n     \\n    # Build graph \\n    workflow = StateGraph(AgentState) \\n    workflow.add_node(\"planner\", planning_node) \\n    workflow.add_node(\"executor\", execution_node) \\n     \\n    workflow.set_entry_point(\"planner\") \\n    workflow.add_edge(\"planner\", \"executor\") \\n    workflow.add_conditional_edges( \\n        \"executor\", \\n        should_continue, \\n        { \\n            \"execute\": \"executor\", \\n            \"replan\": \"planner\", \\n            \"end\": END \\n        } \\n    )\\n     \\n    return workflow.compile() \\nCrewAI Multi-Agent System'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:04+00:00', 'source': '../data/pdf_files/agentic_ai_tutorial.pdf', 'file_path': '../data/pdf_files/agentic_ai_tutorial.pdf', 'total_pages': 74, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:22:04+00:00', 'trapped': '', 'modDate': 'D:20250918142204Z', 'creationDate': \"D:20250918142204+00'00'\", 'page': 5}, page_content='# Example CrewAI-style multi-agent setup \\nclass CrewAgent: \\n    def __init__(self, role: str, goal: str, backstory: str, tools: List): \\n        self.role = role \\n        self.goal = goal \\n        self.backstory = backstory \\n        self.tools = tools \\n        self.memory = [] \\n     \\n    def execute_task(self, task: str) -> str: \\n        # Simulate task execution \\n        context = f\"Role: {self.role}\\\\nGoal: {self.goal}\\\\nTask: {task}\" \\n        # Use LLM with tools \\n        result = f\"Completed {task} as {self.role}\" \\n        self.memory.append({\"task\": task, \"result\": result}) \\n        return result \\ndef create_research_crew(): \\n    \"\"\"Create a crew of agents for research tasks\"\"\" \\n     \\n    researcher = CrewAgent( \\n        role=\"Research Analyst\", \\n        goal=\"Gather comprehensive information on assigned topics\", \\n        backstory=\"Expert researcher with access to various data sources\", \\n        tools=[\"web_search\", \"database_query\", \"document_analysis\"] \\n    )\\n     \\n    writer = CrewAgent( \\n        role=\"Content Writer\", \\n        goal=\"Create well-structured, engaging content\", \\n        backstory=\"Experienced writer who specializes in technical content\", \\n        tools=[\"text_editor\", \"grammar_checker\", \"style_guide\"] \\n    )\\n     \\n    reviewer = CrewAgent( \\n        role=\"Quality Reviewer\", \\n        goal=\"Ensure content meets quality standards\", \\n        backstory=\"Senior editor with expertise in fact-checking\", \\n        tools=[\"fact_checker\", \"plagiarism_detector\", \"quality_scorer\"] \\n    )\\n     \\n    return [researcher, writer, reviewer] \\ndef execute_crew_task(crew, task): \\n    \"\"\"Execute task through crew collaboration\"\"\" \\n    results = [] \\n     \\n    # Sequential execution \\n    for agent in crew: \\n        if results: \\n            # Pass previous results as context \\n            context_task = f\"{task}\\\\nPrevious work: {results[-1]}\" \\n        else: \\n            context_task = task \\n         \\n        result = agent.execute_task(context_task) \\n        results.append(result) \\n     \\n    return results \\nPlanning and Reasoning'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:04+00:00', 'source': '../data/pdf_files/agentic_ai_tutorial.pdf', 'file_path': '../data/pdf_files/agentic_ai_tutorial.pdf', 'total_pages': 74, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:22:04+00:00', 'trapped': '', 'modDate': 'D:20250918142204Z', 'creationDate': \"D:20250918142204+00'00'\", 'page': 6}, page_content='Hierarchical Task Planning'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:04+00:00', 'source': '../data/pdf_files/agentic_ai_tutorial.pdf', 'file_path': '../data/pdf_files/agentic_ai_tutorial.pdf', 'total_pages': 74, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:22:04+00:00', 'trapped': '', 'modDate': 'D:20250918142204Z', 'creationDate': \"D:20250918142204+00'00'\", 'page': 7}, page_content='class TaskPlanner: \\n    \"\"\"Hierarchical task decomposition and planning\"\"\" \\n     \\n    def __init__(self, llm_model): \\n        self.llm = llm_model \\n        self.task_hierarchy = {} \\n     \\n    def decompose_task(self, high_level_task: str) -> Dict: \\n        \"\"\"Break down high-level task into subtasks\"\"\" \\n        decomposition_prompt = f\"\"\" \\n        Break down this high-level task into smaller, actionable subtasks: \\n         \\n        Task: {high_level_task} \\n         \\n        Provide: \\n        1. Subtasks in order of execution \\n        2. Dependencies between subtasks \\n        3. Success criteria for each subtask \\n        4. Estimated effort/time for each \\n         \\n        Format as structured data. \\n        \"\"\" \\n         \\n        response = self.llm.generate(decomposition_prompt) \\n        return self._parse_task_breakdown(response) \\n     \\n    def create_execution_plan(self, task_breakdown: Dict) -> List[Dict]: \\n        \"\"\"Create detailed execution plan\"\"\" \\n        subtasks = task_breakdown.get(\\'subtasks\\', []) \\n        dependencies = task_breakdown.get(\\'dependencies\\', {}) \\n         \\n        # Topological sort based on dependencies \\n        execution_order = self._topological_sort(subtasks, dependencies) \\n         \\n        plan = [] \\n        for task in execution_order: \\n            plan.append({ \\n                \\'task\\': task, \\n                \\'dependencies\\': dependencies.get(task, []), \\n                \\'status\\': \\'pending\\', \\n                \\'estimated_effort\\': task_breakdown.get(\\'efforts\\', {}).get(task, \\'medium\\') \\n            }) \\n         \\n        return plan \\n     \\n    def adaptive_replanning(self, current_plan: List[Dict], execution_results: List[Dict]) -> List[Dict]: \\n        \"\"\"Adapt plan based on execution results\"\"\" \\n        failed_tasks = [r for r in execution_results if not r.get(\\'success\\', False)] \\n         \\n        if failed_tasks: \\n            replanning_prompt = f\"\"\" \\n            The following tasks failed during execution: \\n            {failed_tasks} \\n             \\n            Original plan: {current_plan} \\n             \\n            Please provide: \\n            1. Root cause analysis of failures \\n            2. Alternative approaches for failed tasks \\n            3. Updated execution plan \\n            4. Risk mitigation strategies \\n            \"\"\"'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:04+00:00', 'source': '../data/pdf_files/agentic_ai_tutorial.pdf', 'file_path': '../data/pdf_files/agentic_ai_tutorial.pdf', 'total_pages': 74, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:22:04+00:00', 'trapped': '', 'modDate': 'D:20250918142204Z', 'creationDate': \"D:20250918142204+00'00'\", 'page': 8}, page_content='response = self.llm.generate(replanning_prompt) \\n            return self._parse_updated_plan(response) \\n         \\n        return current_plan \\n# Chain of Thought Reasoning \\nclass ReasoningAgent: \\n    \"\"\"Agent with explicit reasoning capabilities\"\"\" \\n     \\n    def __init__(self, llm_model): \\n        self.llm = llm_model \\n        self.reasoning_history = [] \\n     \\n    def reason_step_by_step(self, problem: str) -> Dict: \\n        \"\"\"Perform step-by-step reasoning\"\"\" \\n        reasoning_prompt = f\"\"\" \\n        Let\\'s think step by step to solve this problem: \\n         \\n        Problem: {problem} \\n         \\n        Please provide: \\n        1. Problem understanding and key constraints \\n        2. Relevant facts and assumptions \\n        3. Step-by-step reasoning process \\n        4. Intermediate conclusions \\n        5. Final answer with confidence level \\n         \\n        Be explicit about your reasoning at each step. \\n        \"\"\" \\n         \\n        response = self.llm.generate(reasoning_prompt) \\n         \\n        reasoning_result = { \\n            \\'problem\\': problem, \\n            \\'reasoning_steps\\': self._extract_reasoning_steps(response), \\n            \\'conclusion\\': self._extract_conclusion(response), \\n            \\'confidence\\': self._extract_confidence(response) \\n        } \\n         \\n        self.reasoning_history.append(reasoning_result) \\n        return reasoning_result \\n     \\n    def validate_reasoning(self, reasoning_result: Dict) -> Dict: \\n        \"\"\"Validate reasoning for logical consistency\"\"\" \\n        validation_prompt = f\"\"\" \\n        Please review this reasoning for logical consistency and correctness: \\n         \\n        Problem: {reasoning_result[\\'problem\\']} \\n        Reasoning Steps: {reasoning_result[\\'reasoning_steps\\']} \\n        Conclusion: {reasoning_result[\\'conclusion\\']} \\n         \\n        Check for: \\n        1. Logical fallacies \\n        2. Inconsistencies \\n        3. Missing steps \\n        4. Alternative solutions \\n         \\n        Provide validation report. \\n        \"\"\" \\n         \\n        validation_response = self.llm.generate(validation_prompt) \\n        return self._parse_validation_report(validation_response)'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:04+00:00', 'source': '../data/pdf_files/agentic_ai_tutorial.pdf', 'file_path': '../data/pdf_files/agentic_ai_tutorial.pdf', 'total_pages': 74, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:22:04+00:00', 'trapped': '', 'modDate': 'D:20250918142204Z', 'creationDate': \"D:20250918142204+00'00'\", 'page': 9}, page_content='Memory and Knowledge Management\\nEpisodic and Semantic Memory'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:04+00:00', 'source': '../data/pdf_files/agentic_ai_tutorial.pdf', 'file_path': '../data/pdf_files/agentic_ai_tutorial.pdf', 'total_pages': 74, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:22:04+00:00', 'trapped': '', 'modDate': 'D:20250918142204Z', 'creationDate': \"D:20250918142204+00'00'\", 'page': 10}, page_content='import sqlite3 \\nfrom datetime import datetime \\nimport json \\nclass AgentMemory: \\n    \"\"\"Comprehensive memory system for agents\"\"\" \\n     \\n    def __init__(self, db_path: str = \"agent_memory.db\"): \\n        self.db_path = db_path \\n        self.init_database() \\n        self.working_memory = {}  # Short-term memory \\n        self.episodic_memory = []  # Experience memory \\n        self.semantic_memory = {}  # Knowledge memory \\n     \\n    def init_database(self): \\n        \"\"\"Initialize memory database\"\"\" \\n        conn = sqlite3.connect(self.db_path) \\n        cursor = conn.cursor() \\n         \\n        # Episodic memory table \\n        cursor.execute(\\'\\'\\' \\n            CREATE TABLE IF NOT EXISTS episodes ( \\n                id INTEGER PRIMARY KEY, \\n                timestamp TEXT, \\n                context TEXT, \\n                action TEXT, \\n                result TEXT, \\n                success BOOLEAN, \\n                importance REAL \\n            ) \\n        \\'\\'\\') \\n         \\n        # Semantic memory table \\n        cursor.execute(\\'\\'\\' \\n            CREATE TABLE IF NOT EXISTS knowledge ( \\n                id INTEGER PRIMARY KEY, \\n                concept TEXT, \\n                description TEXT, \\n                confidence REAL, \\n                sources TEXT, \\n                last_updated TEXT \\n            ) \\n        \\'\\'\\') \\n         \\n        conn.commit() \\n        conn.close() \\n     \\n    def store_episode(self, context: str, action: str, result: str, success: bool, importance: float = 0.5): \\n        \"\"\"Store episodic memory\"\"\" \\n        conn = sqlite3.connect(self.db_path) \\n        cursor = conn.cursor() \\n         \\n        cursor.execute(\\'\\'\\' \\n            INSERT INTO episodes (timestamp, context, action, result, success, importance) \\n            VALUES (?, ?, ?, ?, ?, ?) \\n        \\'\\'\\', (datetime.now().isoformat(), context, action, result, success, importance)) \\n         \\n        conn.commit() \\n        conn.close() \\n         \\n        # Also store in working memory \\n        self.episodic_memory.append({'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:04+00:00', 'source': '../data/pdf_files/agentic_ai_tutorial.pdf', 'file_path': '../data/pdf_files/agentic_ai_tutorial.pdf', 'total_pages': 74, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:22:04+00:00', 'trapped': '', 'modDate': 'D:20250918142204Z', 'creationDate': \"D:20250918142204+00'00'\", 'page': 11}, page_content='\\'timestamp\\': datetime.now().isoformat(), \\n            \\'context\\': context, \\n            \\'action\\': action, \\n            \\'result\\': result, \\n            \\'success\\': success, \\n            \\'importance\\': importance \\n        })\\n     \\n    def retrieve_similar_episodes(self, current_context: str, limit: int = 5) -> List[Dict]: \\n        \"\"\"Retrieve similar past episodes\"\"\" \\n        # Simplified similarity (in practice, use embeddings) \\n        similar_episodes = [] \\n         \\n        conn = sqlite3.connect(self.db_path) \\n        cursor = conn.cursor() \\n         \\n        cursor.execute(\\'\\'\\' \\n            SELECT * FROM episodes  \\n            WHERE context LIKE ?  \\n            ORDER BY importance DESC, timestamp DESC  \\n            LIMIT ? \\n        \\'\\'\\', (f\\'%{current_context}%\\', limit)) \\n         \\n        episodes = cursor.fetchall() \\n        conn.close() \\n         \\n        return [dict(zip([\\'id\\', \\'timestamp\\', \\'context\\', \\'action\\', \\'result\\', \\'success\\', \\'importance\\'], ep)) for ep in episodes]\\n     \\n    def update_knowledge(self, concept: str, description: str, confidence: float, sources: List[str]): \\n        \"\"\"Update semantic knowledge\"\"\" \\n        conn = sqlite3.connect(self.db_path) \\n        cursor = conn.cursor() \\n         \\n        # Check if concept exists \\n        cursor.execute(\\'SELECT id FROM knowledge WHERE concept = ?\\', (concept,)) \\n        existing = cursor.fetchone() \\n         \\n        sources_json = json.dumps(sources) \\n         \\n        if existing: \\n            cursor.execute(\\'\\'\\' \\n                UPDATE knowledge  \\n                SET description = ?, confidence = ?, sources = ?, last_updated = ? \\n                WHERE concept = ? \\n            \\'\\'\\', (description, confidence, sources_json, datetime.now().isoformat(), concept)) \\n        else: \\n            cursor.execute(\\'\\'\\' \\n                INSERT INTO knowledge (concept, description, confidence, sources, last_updated) \\n                VALUES (?, ?, ?, ?, ?) \\n            \\'\\'\\', (concept, description, confidence, sources_json, datetime.now().isoformat())) \\n         \\n        conn.commit() \\n        conn.close() \\n     \\n    def get_knowledge(self, concept: str) -> Optional[Dict]: \\n        \"\"\"Retrieve knowledge about a concept\"\"\" \\n        conn = sqlite3.connect(self.db_path) \\n        cursor = conn.cursor() \\n         \\n        cursor.execute(\\'SELECT * FROM knowledge WHERE concept = ?\\', (concept,)) \\n        result = cursor.fetchone() \\n        conn.close()'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:04+00:00', 'source': '../data/pdf_files/agentic_ai_tutorial.pdf', 'file_path': '../data/pdf_files/agentic_ai_tutorial.pdf', 'total_pages': 74, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:22:04+00:00', 'trapped': '', 'modDate': 'D:20250918142204Z', 'creationDate': \"D:20250918142204+00'00'\", 'page': 12}, page_content='if result: \\n            return dict(zip([\\'id\\', \\'concept\\', \\'description\\', \\'confidence\\', \\'sources\\', \\'last_updated\\'], result)) \\n        return None \\n     \\n    def consolidate_memory(self): \\n        \"\"\"Consolidate working memory into long-term storage\"\"\" \\n        # Move important experiences from working memory to database \\n        # Implement memory consolidation algorithms \\n        pass \\nTool Use and Actions\\nTool Integration Framework'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:04+00:00', 'source': '../data/pdf_files/agentic_ai_tutorial.pdf', 'file_path': '../data/pdf_files/agentic_ai_tutorial.pdf', 'total_pages': 74, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:22:04+00:00', 'trapped': '', 'modDate': 'D:20250918142204Z', 'creationDate': \"D:20250918142204+00'00'\", 'page': 13}, page_content='from typing import Callable, Any \\nimport inspect \\nimport json \\nclass ToolRegistry: \\n    \"\"\"Registry for agent tools and actions\"\"\" \\n     \\n    def __init__(self): \\n        self.tools = {} \\n        self.tool_descriptions = {} \\n     \\n    def register_tool(self, name: str, func: Callable, description: str = \"\"): \\n        \"\"\"Register a new tool\"\"\" \\n        # Get function signature for automatic parameter extraction \\n        sig = inspect.signature(func) \\n        parameters = {} \\n         \\n        for param_name, param in sig.parameters.items(): \\n            parameters[param_name] = { \\n                \\'type\\': param.annotation.__name__ if param.annotation != inspect.Parameter.empty else \\'Any\\', \\n                \\'default\\': param.default if param.default != inspect.Parameter.empty else None \\n            } \\n         \\n        self.tools[name] = func \\n        self.tool_descriptions[name] = { \\n            \\'description\\': description, \\n            \\'parameters\\': parameters, \\n            \\'function\\': func \\n        } \\n     \\n    def get_tool_list(self) -> List[Dict]: \\n        \"\"\"Get list of available tools with descriptions\"\"\" \\n        return [ \\n            { \\n                \\'name\\': name, \\n                \\'description\\': info[\\'description\\'], \\n                \\'parameters\\': info[\\'parameters\\'] \\n            } \\n            for name, info in self.tool_descriptions.items() \\n        ] \\n     \\n    def execute_tool(self, tool_name: str, **kwargs) -> Dict: \\n        \"\"\"Execute a tool with given parameters\"\"\" \\n        if tool_name not in self.tools: \\n            return {\\'success\\': False, \\'error\\': f\\'Tool {tool_name} not found\\'} \\n         \\n        try: \\n            result = self.tools[tool_name](**kwargs) \\n            return {\\'success\\': True, \\'result\\': result} \\n        except Exception as e: \\n            return {\\'success\\': False, \\'error\\': str(e)} \\n# Example tools \\ndef web_search(query: str, max_results: int = 5) -> List[Dict]: \\n    \"\"\"Search the web for information\"\"\" \\n    # Simulated web search \\n    return [ \\n        {\\'title\\': f\\'Result {i+1} for {query}\\', \\'url\\': f\\'https://example{i+1}.com\\', \\'snippet\\': f\\'Information about {query}\\'} \\n        for i in range(max_results) \\n    ]\\ndef calculator(expression: str) -> float:'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:04+00:00', 'source': '../data/pdf_files/agentic_ai_tutorial.pdf', 'file_path': '../data/pdf_files/agentic_ai_tutorial.pdf', 'total_pages': 74, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:22:04+00:00', 'trapped': '', 'modDate': 'D:20250918142204Z', 'creationDate': \"D:20250918142204+00'00'\", 'page': 14}, page_content='\"\"\"Perform mathematical calculations\"\"\" \\n    try: \\n        # Safe evaluation (use ast.literal_eval in production) \\n        return eval(expression) \\n    except: \\n        raise ValueError(f\"Invalid expression: {expression}\") \\ndef file_writer(filename: str, content: str) -> bool: \\n    \"\"\"Write content to a file\"\"\" \\n    try: \\n        with open(filename, \\'w\\') as f: \\n            f.write(content) \\n        return True \\n    except Exception as e: \\n        raise IOError(f\"Failed to write file: {e}\") \\n# Tool-using agent \\nclass ToolUsingAgent(AutonomousAgent): \\n    \"\"\"Agent that can use external tools\"\"\" \\n     \\n    def __init__(self, name: str, goals: List[str], llm_model, tool_registry: ToolRegistry): \\n        super().__init__(name, goals, llm_model) \\n        self.tool_registry = tool_registry \\n     \\n    def select_tool(self, task_description: str) -> str: \\n        \"\"\"Select appropriate tool for a task\"\"\" \\n        available_tools = self.tool_registry.get_tool_list() \\n         \\n        tool_selection_prompt = f\"\"\" \\n        Select the most appropriate tool for this task: \\n         \\n        Task: {task_description} \\n         \\n        Available tools: \\n        {json.dumps(available_tools, indent=2)} \\n         \\n        Respond with just the tool name. \\n        \"\"\" \\n         \\n        tool_name = self.llm.generate(tool_selection_prompt).strip() \\n        return tool_name \\n     \\n    def generate_tool_parameters(self, tool_name: str, task_description: str) -> Dict: \\n        \"\"\"Generate parameters for tool execution\"\"\" \\n        tool_info = self.tool_registry.tool_descriptions.get(tool_name, {}) \\n         \\n        param_generation_prompt = f\"\"\" \\n        Generate appropriate parameters for this tool: \\n         \\n        Tool: {tool_name} \\n        Task: {task_description} \\n        Required parameters: {tool_info.get(\\'parameters\\', {})} \\n         \\n        Respond with JSON object containing parameter values. \\n        \"\"\" \\n         \\n        params_response = self.llm.generate(param_generation_prompt) \\n        try: \\n            return json.loads(params_response) \\n        except: \\n            return {} \\n     \\n    def use_tool(self, task_description: str) -> Dict:'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:04+00:00', 'source': '../data/pdf_files/agentic_ai_tutorial.pdf', 'file_path': '../data/pdf_files/agentic_ai_tutorial.pdf', 'total_pages': 74, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:22:04+00:00', 'trapped': '', 'modDate': 'D:20250918142204Z', 'creationDate': \"D:20250918142204+00'00'\", 'page': 15}, page_content='\"\"\"Use appropriate tool to complete a task\"\"\" \\n        # Select tool \\n        tool_name = self.select_tool(task_description) \\n         \\n        # Generate parameters \\n        parameters = self.generate_tool_parameters(tool_name, task_description) \\n         \\n        # Execute tool \\n        result = self.tool_registry.execute_tool(tool_name, **parameters) \\n         \\n        # Store experience \\n        self.store_tool_experience(task_description, tool_name, parameters, result) \\n         \\n        return result \\n     \\n    def store_tool_experience(self, task: str, tool: str, params: Dict, result: Dict): \\n        \"\"\"Store tool usage experience for learning\"\"\" \\n        experience = { \\n            \\'task\\': task, \\n            \\'tool_used\\': tool, \\n            \\'parameters\\': params, \\n            \\'result\\': result, \\n            \\'success\\': result.get(\\'success\\', False) \\n        } \\n         \\n        # Store in memory for future tool selection \\n        if \\'tool_experiences\\' not in self.knowledge_base: \\n            self.knowledge_base[\\'tool_experiences\\'] = [] \\n         \\n        self.knowledge_base[\\'tool_experiences\\'].append(experience) \\nBest Practices\\nError Handling and Recovery'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:04+00:00', 'source': '../data/pdf_files/agentic_ai_tutorial.pdf', 'file_path': '../data/pdf_files/agentic_ai_tutorial.pdf', 'total_pages': 74, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:22:04+00:00', 'trapped': '', 'modDate': 'D:20250918142204Z', 'creationDate': \"D:20250918142204+00'00'\", 'page': 16}, page_content='class RobustAgent(AutonomousAgent): \\n    \"\"\"Agent with robust error handling\"\"\" \\n     \\n    def __init__(self, name: str, goals: List[str], llm_model): \\n        super().__init__(name, goals, llm_model) \\n        self.max_retries = 3 \\n        self.fallback_strategies = {} \\n     \\n    def robust_execute(self, action: Dict) -> Dict: \\n        \"\"\"Execute action with error handling and retries\"\"\" \\n        for attempt in range(self.max_retries): \\n            try: \\n                result = self.act(action) \\n                 \\n                if result.get(\\'success\\', False): \\n                    return result \\n                else: \\n                    # Try fallback strategy \\n                    if action[\\'type\\'] in self.fallback_strategies: \\n                        fallback_action = self.fallback_strategies[action[\\'type\\']] \\n                        return self.act(fallback_action) \\n                     \\n            except Exception as e: \\n                if attempt == self.max_retries - 1: \\n                    return {\\'success\\': False, \\'error\\': f\\'Max retries exceeded: {e}\\'} \\n                 \\n                # Exponential backoff \\n                time.sleep(2 ** attempt) \\n         \\n        return {\\'success\\': False, \\'error\\': \\'All attempts failed\\'} \\n     \\n    def add_fallback_strategy(self, action_type: str, fallback_action: Dict): \\n        \"\"\"Add fallback strategy for specific action types\"\"\" \\n        self.fallback_strategies[action_type] = fallback_action \\n# Safety and Alignment \\nclass SafeAgent(RobustAgent): \\n    \"\"\"Agent with safety constraints and alignment checks\"\"\" \\n     \\n    def __init__(self, name: str, goals: List[str], llm_model, safety_rules: List[str]): \\n        super().__init__(name, goals, llm_model) \\n        self.safety_rules = safety_rules \\n        self.violation_log = [] \\n     \\n    def check_safety(self, action: Dict) -> bool: \\n        \"\"\"Check if action violates safety rules\"\"\" \\n        safety_check_prompt = f\"\"\" \\n        Check if this action violates any safety rules: \\n         \\n        Action: {action} \\n        Safety Rules: {self.safety_rules} \\n         \\n        Respond with \\'SAFE\\' or \\'UNSAFE\\' and explanation. \\n        \"\"\" \\n         \\n        response = self.llm.generate(safety_check_prompt) \\n         \\n        if \\'UNSAFE\\' in response.upper(): \\n            self.violation_log.append({ \\n                \\'action\\': action, \\n                \\'violation_reason\\': response, \\n                \\'timestamp\\': datetime.now().isoformat()'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:04+00:00', 'source': '../data/pdf_files/agentic_ai_tutorial.pdf', 'file_path': '../data/pdf_files/agentic_ai_tutorial.pdf', 'total_pages': 74, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:22:04+00:00', 'trapped': '', 'modDate': 'D:20250918142204Z', 'creationDate': \"D:20250918142204+00'00'\", 'page': 17}, page_content='}) \\n            return False \\n         \\n        return True \\n     \\n    def act(self, action: Dict) -> Dict: \\n        \"\"\"Execute action with safety checks\"\"\" \\n        if not self.check_safety(action): \\n            return { \\n                \\'success\\': False, \\n                \\'error\\': \\'Action violates safety rules\\', \\n                \\'action\\': action \\n            } \\n         \\n        return super().act(action) \\n# Performance Monitoring \\nclass MonitoredAgent(SafeAgent): \\n    \"\"\"Agent with performance monitoring\"\"\" \\n     \\n    def __init__(self, name: str, goals: List[str], llm_model, safety_rules: List[str]): \\n        super().__init__(name, goals, llm_model, safety_rules) \\n        self.performance_metrics = { \\n            \\'total_actions\\': 0, \\n            \\'successful_actions\\': 0, \\n            \\'failed_actions\\': 0, \\n            \\'average_response_time\\': 0, \\n            \\'goal_completion_rate\\': 0 \\n        } \\n     \\n    def update_metrics(self, action_result: Dict, response_time: float): \\n        \"\"\"Update performance metrics\"\"\" \\n        self.performance_metrics[\\'total_actions\\'] += 1 \\n         \\n        if action_result.get(\\'success\\', False): \\n            self.performance_metrics[\\'successful_actions\\'] += 1 \\n        else: \\n            self.performance_metrics[\\'failed_actions\\'] += 1 \\n         \\n        # Update average response time \\n        total_actions = self.performance_metrics[\\'total_actions\\'] \\n        current_avg = self.performance_metrics[\\'average_response_time\\'] \\n        self.performance_metrics[\\'average_response_time\\'] = ( \\n            (current_avg * (total_actions - 1) + response_time) / total_actions \\n        ) \\n     \\n    def get_performance_report(self) -> Dict: \\n        \"\"\"Generate performance report\"\"\" \\n        total = self.performance_metrics[\\'total_actions\\'] \\n        if total == 0: \\n            return self.performance_metrics \\n         \\n        success_rate = self.performance_metrics[\\'successful_actions\\'] / total \\n        failure_rate = self.performance_metrics[\\'failed_actions\\'] / total \\n         \\n        return { \\n            **self.performance_metrics, \\n            \\'success_rate\\': success_rate, \\n            \\'failure_rate\\': failure_rate \\n        }'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:04+00:00', 'source': '../data/pdf_files/agentic_ai_tutorial.pdf', 'file_path': '../data/pdf_files/agentic_ai_tutorial.pdf', 'total_pages': 74, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:22:04+00:00', 'trapped': '', 'modDate': 'D:20250918142204Z', 'creationDate': \"D:20250918142204+00'00'\", 'page': 18}, page_content='Conclusion\\nAgentic AI represents a paradigm shift toward autonomous, goal-oriented AI systems. This tutorial covered:\\nCore agent architectures and design patterns\\nPlanning, reasoning, and decision-making capabilities\\nMemory systems for learning and adaptation\\nTool integration and multi-agent collaboration\\nSafety, robustness, and performance monitoring\\nKey Principles:\\n1. Goal-oriented design: Agents should have clear objectives\\n2. Modular architecture: Separate perception, planning, and action\\n3. Memory integration: Learn from experience\\n4. Safety first: Implement constraints and monitoring\\n5. Adaptive behavior: Adjust strategies based on outcomes\\nImplementation Strategy:\\n1. Start with simple reactive agents\\n2. Add planning and reasoning capabilities\\n3. Integrate memory and learning systems\\n4. Implement multi-agent coordination\\n5. Deploy with safety measures and monitoring\\nNext Steps:\\nExplore advanced planning algorithms\\nImplement multi-modal agents (vision, speech, etc.)\\nStudy reinforcement learning for agent training\\nBuild domain-specific agent applications\\nContribute to open-source agent frameworks\\nThe future of AI lies in systems that can act autonomously while remaining aligned with human values and goals.\\nHugging Face Tutorial: Complete Guide to\\nTransformers and Model Hub\\nTable of Contents\\n1. Introduction to Hugging Face\\n2. Installation and Setup\\n3. Using Pre-trained Models\\n4. Transformers Library\\n5. Datasets Library\\n6. Tokenizers\\n7. Fine-tuning Models\\n8. Model Hub and Sharing\\n9. Inference Endpoints\\n10. Best Practices\\nIntroduction to Hugging Face\\nHugging Face is the leading platform for machine learning models, providing:\\n\\x00 Transformers: State-of-the-art ML models for PyTorch, TensorFlow, and JAX\\n\\x00 Datasets: The largest collection of ready-to-use datasets\\n\\x00 Model Hub: Over 300,000+ models shared by the community'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:04+00:00', 'source': '../data/pdf_files/agentic_ai_tutorial.pdf', 'file_path': '../data/pdf_files/agentic_ai_tutorial.pdf', 'total_pages': 74, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:22:04+00:00', 'trapped': '', 'modDate': 'D:20250918142204Z', 'creationDate': \"D:20250918142204+00'00'\", 'page': 19}, page_content='\\x00 Spaces: Collaborative platform for ML demos and applications\\nKey Ecosystems:\\nNLP: BERT, GPT, RoBERTa, T5, and more\\nComputer Vision: Vision Transformer, CLIP, DETR\\nAudio: Wav2Vec2, Whisper, SpeechT5\\nMultimodal: CLIP, DALL-E, Flamingo\\nReinforcement Learning: Decision Transformers\\nInstallation and Setup\\nCore Installation\\n# Basic installation \\npip install transformers \\n# With PyTorch \\npip install transformers[torch] \\n# With TensorFlow \\npip install transformers[tf] \\n# Full installation with all dependencies \\npip install transformers[all] \\n# Additional libraries \\npip install datasets \\npip install tokenizers \\npip install accelerate \\npip install peft  # For parameter-efficient fine-tuning \\npip install bitsandbytes  # For quantization \\nEnvironment Setup\\nimport torch \\nfrom transformers import ( \\n    AutoTokenizer,  \\n    AutoModel,  \\n    AutoModelForSequenceClassification, \\n    pipeline, \\n    TrainingArguments, \\n    Trainer \\n) \\nfrom datasets import Dataset, load_dataset \\nimport numpy as np \\n# Check if CUDA is available \\nprint(f\"CUDA available: {torch.cuda.is_available()}\") \\nprint(f\"Device: {torch.cuda.get_device_name() if torch.cuda.is_available() else \\'CPU\\'}\") \\n# Set device \\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \\nHugging Face Hub Authentication'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:04+00:00', 'source': '../data/pdf_files/agentic_ai_tutorial.pdf', 'file_path': '../data/pdf_files/agentic_ai_tutorial.pdf', 'total_pages': 74, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:22:04+00:00', 'trapped': '', 'modDate': 'D:20250918142204Z', 'creationDate': \"D:20250918142204+00'00'\", 'page': 20}, page_content='from huggingface_hub import login, HfApi \\nimport os \\n# Method 1: Login interactively \\nlogin() \\n# Method 2: Use token from environment \\nos.environ[\"HUGGINGFACE_HUB_TOKEN\"] = \"your_token_here\" \\n# Method 3: Programmatic login \\nlogin(token=\"your_token_here\") \\n# Check login status \\napi = HfApi() \\nuser = api.whoami() \\nprint(f\"Logged in as: {user[\\'name\\']}\") \\nUsing Pre-trained Models\\nQuick Start with Pipelines\\n# Sentiment Analysis \\nsentiment_pipeline = pipeline(\"sentiment-analysis\") \\nresult = sentiment_pipeline(\"I love Hugging Face!\") \\nprint(result)  # [{\\'label\\': \\'POSITIVE\\', \\'score\\': 0.9998}] \\n# Text Generation \\ngenerator = pipeline(\"text-generation\", model=\"gpt2\") \\noutput = generator(\"The future of AI is\", max_length=50, num_return_sequences=1) \\nprint(output[0][\\'generated_text\\']) \\n# Question Answering \\nqa_pipeline = pipeline(\"question-answering\") \\ncontext = \"Hugging Face is a company that democratizes AI through open-source and open science.\" \\nquestion = \"What does Hugging Face do?\" \\nanswer = qa_pipeline(question=question, context=context) \\nprint(answer) \\n# Named Entity Recognition \\nner_pipeline = pipeline(\"ner\", aggregation_strategy=\"simple\") \\ntext = \"My name is Sarah and I work at Google in California.\" \\nentities = ner_pipeline(text) \\nprint(entities) \\n# Translation \\ntranslator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-fr\") \\nfrench_text = translator(\"Hello, how are you?\") \\nprint(french_text) \\nAvailable Pipeline Tasks'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:04+00:00', 'source': '../data/pdf_files/agentic_ai_tutorial.pdf', 'file_path': '../data/pdf_files/agentic_ai_tutorial.pdf', 'total_pages': 74, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:22:04+00:00', 'trapped': '', 'modDate': 'D:20250918142204Z', 'creationDate': \"D:20250918142204+00'00'\", 'page': 21}, page_content='# Get all available tasks \\nfrom transformers import PIPELINE_REGISTRY \\nprint(\"Available tasks:\", list(PIPELINE_REGISTRY.supported_tasks.keys())) \\n# Specific pipeline examples \\npipelines_examples = { \\n    \"text-classification\": \"distilbert-base-uncased-finetuned-sst-2-english\", \\n    \"token-classification\": \"dbmdz/bert-large-cased-finetuned-conll03-english\", \\n    \"question-answering\": \"distilbert-base-cased-distilled-squad\", \\n    \"fill-mask\": \"bert-base-uncased\", \\n    \"summarization\": \"facebook/bart-large-cnn\", \\n    \"translation\": \"t5-base\", \\n    \"text-generation\": \"gpt2\", \\n    \"text2text-generation\": \"t5-small\", \\n    \"zero-shot-classification\": \"facebook/bart-large-mnli\", \\n    \"image-classification\": \"google/vit-base-patch16-224\", \\n    \"object-detection\": \"facebook/detr-resnet-50\", \\n    \"image-segmentation\": \"facebook/detr-resnet-50-panoptic\", \\n    \"automatic-speech-recognition\": \"facebook/wav2vec2-base-960h\", \\n    \"text-to-speech\": \"microsoft/speecht5_tts\" \\n} \\n# Use any pipeline \\nfor task, model_name in pipelines_examples.items(): \\n    try: \\n        pipe = pipeline(task, model=model_name) \\n        print(f\"✓ {task}: {model_name}\") \\n    except Exception as e: \\n        print(f\"✗ {task}: {e}\") \\nTransformers Library\\nLoading Models and Tokenizers\\n# Method 1: Auto classes (recommended) \\nmodel_name = \"bert-base-uncased\" \\ntokenizer = AutoTokenizer.from_pretrained(model_name) \\nmodel = AutoModel.from_pretrained(model_name) \\n# Method 2: Specific model classes \\nfrom transformers import BertTokenizer, BertModel \\ntokenizer = BertTokenizer.from_pretrained(model_name) \\nmodel = BertModel.from_pretrained(model_name) \\n# Load model for specific tasks \\nmodel = AutoModelForSequenceClassification.from_pretrained( \\n    \"cardiffnlp/twitter-roberta-base-sentiment-latest\" \\n) \\n# Load with specific configurations \\nfrom transformers import AutoConfig \\nconfig = AutoConfig.from_pretrained(model_name) \\nconfig.output_hidden_states = True \\nmodel = AutoModel.from_pretrained(model_name, config=config) \\nText Processing'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:04+00:00', 'source': '../data/pdf_files/agentic_ai_tutorial.pdf', 'file_path': '../data/pdf_files/agentic_ai_tutorial.pdf', 'total_pages': 74, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:22:04+00:00', 'trapped': '', 'modDate': 'D:20250918142204Z', 'creationDate': \"D:20250918142204+00'00'\", 'page': 22}, page_content='def process_text_with_bert(text, model_name=\"bert-base-uncased\"): \\n    tokenizer = AutoTokenizer.from_pretrained(model_name) \\n    model = AutoModel.from_pretrained(model_name) \\n     \\n    # Tokenize \\n    inputs = tokenizer( \\n        text, \\n        padding=True, \\n        truncation=True, \\n        max_length=512, \\n        return_tensors=\"pt\" \\n    )\\n     \\n    # Get model outputs \\n    with torch.no_grad(): \\n        outputs = model(**inputs) \\n     \\n    # Extract embeddings \\n    last_hidden_states = outputs.last_hidden_state \\n    pooled_output = outputs.pooler_output if hasattr(outputs, \\'pooler_output\\') else None \\n     \\n    return { \\n        \"input_ids\": inputs[\"input_ids\"], \\n        \"attention_mask\": inputs[\"attention_mask\"], \\n        \"last_hidden_states\": last_hidden_states, \\n        \"pooled_output\": pooled_output, \\n        \"tokens\": tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0]) \\n    }\\n# Usage \\ntext = \"Hugging Face transformers are amazing!\" \\nresult = process_text_with_bert(text) \\nprint(f\"Sequence length: {result[\\'last_hidden_states\\'].shape[1]}\") \\nprint(f\"Hidden size: {result[\\'last_hidden_states\\'].shape[2]}\") \\nBatch Processing'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:04+00:00', 'source': '../data/pdf_files/agentic_ai_tutorial.pdf', 'file_path': '../data/pdf_files/agentic_ai_tutorial.pdf', 'total_pages': 74, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:22:04+00:00', 'trapped': '', 'modDate': 'D:20250918142204Z', 'creationDate': \"D:20250918142204+00'00'\", 'page': 23}, page_content='def batch_encode_texts(texts, model_name=\"bert-base-uncased\", batch_size=16): \\n    tokenizer = AutoTokenizer.from_pretrained(model_name) \\n    model = AutoModel.from_pretrained(model_name) \\n     \\n    all_embeddings = [] \\n     \\n    for i in range(0, len(texts), batch_size): \\n        batch_texts = texts[i:i+batch_size] \\n         \\n        # Tokenize batch \\n        inputs = tokenizer( \\n            batch_texts, \\n            padding=True, \\n            truncation=True, \\n            max_length=512, \\n            return_tensors=\"pt\" \\n        ) \\n         \\n        # Get embeddings \\n        with torch.no_grad(): \\n            outputs = model(**inputs) \\n            # Use mean pooling for sentence embeddings \\n            embeddings = outputs.last_hidden_state.mean(dim=1) \\n            all_embeddings.append(embeddings) \\n     \\n    return torch.cat(all_embeddings, dim=0) \\n# Usage \\ntexts = [ \\n    \"I love machine learning\", \\n    \"Natural language processing is fascinating\", \\n    \"Transformers revolutionized NLP\", \\n    \"BERT is a powerful model\" \\n] \\nembeddings = batch_encode_texts(texts) \\nprint(f\"Embeddings shape: {embeddings.shape}\") \\nModel Comparison'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:04+00:00', 'source': '../data/pdf_files/agentic_ai_tutorial.pdf', 'file_path': '../data/pdf_files/agentic_ai_tutorial.pdf', 'total_pages': 74, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:22:04+00:00', 'trapped': '', 'modDate': 'D:20250918142204Z', 'creationDate': \"D:20250918142204+00'00'\", 'page': 24}, page_content='def compare_models(text, models): \\n    results = {} \\n     \\n    for model_name in models: \\n        try: \\n            # Load model and tokenizer \\n            tokenizer = AutoTokenizer.from_pretrained(model_name) \\n            model = AutoModel.from_pretrained(model_name) \\n             \\n            # Process text \\n            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512) \\n             \\n            with torch.no_grad(): \\n                outputs = model(**inputs) \\n                embedding = outputs.last_hidden_state.mean(dim=1) \\n             \\n            results[model_name] = { \\n                \"embedding_dim\": embedding.shape[1], \\n                \"vocab_size\": len(tokenizer), \\n                \"max_position\": tokenizer.model_max_length, \\n                \"embedding_norm\": torch.norm(embedding).item() \\n            } \\n        except Exception as e: \\n            results[model_name] = {\"error\": str(e)} \\n     \\n    return results \\n# Compare different models \\nmodels_to_compare = [ \\n    \"bert-base-uncased\", \\n    \"roberta-base\",  \\n    \"distilbert-base-uncased\", \\n    \"albert-base-v2\" \\n] \\ncomparison = compare_models(\"This is a test sentence.\", models_to_compare) \\nfor model, stats in comparison.items(): \\n    print(f\"{model}:\") \\n    for key, value in stats.items(): \\n        print(f\"  {key}: {value}\") \\n    print() \\nDatasets Library\\nLoading Datasets'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:04+00:00', 'source': '../data/pdf_files/agentic_ai_tutorial.pdf', 'file_path': '../data/pdf_files/agentic_ai_tutorial.pdf', 'total_pages': 74, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:22:04+00:00', 'trapped': '', 'modDate': 'D:20250918142204Z', 'creationDate': \"D:20250918142204+00'00'\", 'page': 25}, page_content='from datasets import load_dataset, Dataset, DatasetDict \\n# Load popular datasets \\nimdb = load_dataset(\"imdb\") \\nsquad = load_dataset(\"squad\") \\nglue_sst2 = load_dataset(\"glue\", \"sst2\") \\n# Load specific splits \\ntrain_data = load_dataset(\"imdb\", split=\"train\") \\ntest_data = load_dataset(\"imdb\", split=\"test[:1000]\")  # First 1000 examples \\n# Load from local files \\nlocal_dataset = load_dataset(\"csv\", data_files=\"my_data.csv\") \\njson_dataset = load_dataset(\"json\", data_files=\"my_data.jsonl\") \\nprint(f\"IMDB dataset: {imdb}\") \\nprint(f\"Features: {imdb[\\'train\\'].features}\") \\nprint(f\"Number of examples: {len(imdb[\\'train\\'])}\") \\nDataset Exploration\\ndef explore_dataset(dataset): \\n    \"\"\"Explore dataset characteristics\"\"\" \\n    print(f\"Dataset: {dataset}\") \\n    print(f\"Splits: {list(dataset.keys())}\") \\n     \\n    for split_name, split_data in dataset.items(): \\n        print(f\"\\\\n{split_name.upper()} SPLIT:\") \\n        print(f\"  Size: {len(split_data)}\") \\n        print(f\"  Features: {split_data.features}\") \\n         \\n        # Show first few examples \\n        print(f\"  First example: {split_data[0]}\") \\n         \\n        # Show data types and statistics \\n        for feature_name, feature_type in split_data.features.items(): \\n            print(f\"  {feature_name}: {feature_type}\") \\n# Explore IMDB dataset \\nexplore_dataset(imdb) \\nData Preprocessing'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:04+00:00', 'source': '../data/pdf_files/agentic_ai_tutorial.pdf', 'file_path': '../data/pdf_files/agentic_ai_tutorial.pdf', 'total_pages': 74, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:22:04+00:00', 'trapped': '', 'modDate': 'D:20250918142204Z', 'creationDate': \"D:20250918142204+00'00'\", 'page': 26}, page_content='def preprocess_imdb_data(examples, tokenizer, max_length=512): \\n    \"\"\"Preprocess IMDB dataset for BERT\"\"\" \\n    return tokenizer( \\n        examples[\"text\"], \\n        truncation=True, \\n        padding=\"max_length\", \\n        max_length=max_length, \\n        return_tensors=\"pt\" \\n    )\\n# Load tokenizer \\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\") \\n# Preprocess dataset \\ndef tokenize_function(examples): \\n    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=256) \\n# Apply preprocessing \\ntokenized_imdb = imdb.map(tokenize_function, batched=True) \\n# Remove original text column and rename label \\ntokenized_imdb = tokenized_imdb.remove_columns([\"text\"]) \\ntokenized_imdb = tokenized_imdb.rename_column(\"label\", \"labels\") \\n# Set format for PyTorch \\ntokenized_imdb.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"]) \\nprint(\"Preprocessed dataset:\") \\nprint(tokenized_imdb[\"train\"][0]) \\nCustom Dataset Creation'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:04+00:00', 'source': '../data/pdf_files/agentic_ai_tutorial.pdf', 'file_path': '../data/pdf_files/agentic_ai_tutorial.pdf', 'total_pages': 74, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:22:04+00:00', 'trapped': '', 'modDate': 'D:20250918142204Z', 'creationDate': \"D:20250918142204+00'00'\", 'page': 27}, page_content='def create_custom_dataset(): \\n    \"\"\"Create a custom dataset\"\"\" \\n    # Sample data \\n    data = { \\n        \"text\": [ \\n            \"I love this movie!\", \\n            \"This film is terrible.\", \\n            \"Great acting and storyline.\", \\n            \"Boring and predictable.\", \\n            \"Amazing cinematography!\" \\n        ],\\n        \"label\": [1, 0, 1, 0, 1]  # 1=positive, 0=negative \\n    }\\n     \\n    # Create dataset \\n    dataset = Dataset.from_dict(data) \\n     \\n    # Split into train/test \\n    train_test = dataset.train_test_split(test_size=0.2) \\n     \\n    return train_test \\n# Create and use custom dataset \\ncustom_data = create_custom_dataset() \\nprint(custom_data) \\n# Add preprocessing \\ndef preprocess_custom(examples): \\n    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\") \\n    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=128) \\ncustom_data = custom_data.map(preprocess_custom, batched=True) \\ncustom_data.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"]) \\nData Augmentation'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:04+00:00', 'source': '../data/pdf_files/agentic_ai_tutorial.pdf', 'file_path': '../data/pdf_files/agentic_ai_tutorial.pdf', 'total_pages': 74, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:22:04+00:00', 'trapped': '', 'modDate': 'D:20250918142204Z', 'creationDate': \"D:20250918142204+00'00'\", 'page': 28}, page_content='def augment_text_data(dataset, augmentation_factor=2): \\n    \"\"\"Simple data augmentation for text\"\"\" \\n    import random \\n     \\n    def augment_example(example): \\n        text = example[\"text\"] \\n         \\n        # Simple augmentations \\n        augmented_texts = [text]  # Original \\n         \\n        # Synonym replacement (simplified) \\n        synonyms = { \\n            \"good\": [\"great\", \"excellent\", \"wonderful\"], \\n            \"bad\": [\"terrible\", \"awful\", \"horrible\"], \\n            \"nice\": [\"pleasant\", \"lovely\", \"delightful\"] \\n        } \\n         \\n        for word, syns in synonyms.items(): \\n            if word in text.lower(): \\n                for syn in syns: \\n                    augmented_texts.append(text.lower().replace(word, syn)) \\n         \\n        # Return multiple examples \\n        return { \\n            \"text\": augmented_texts[:augmentation_factor], \\n            \"label\": [example[\"label\"]] * min(len(augmented_texts), augmentation_factor) \\n        } \\n     \\n    # Apply augmentation \\n    augmented = dataset.map(augment_example, remove_columns=dataset.column_names, batched=False) \\n     \\n    return augmented \\nTokenizers\\nUnderstanding Tokenization'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:04+00:00', 'source': '../data/pdf_files/agentic_ai_tutorial.pdf', 'file_path': '../data/pdf_files/agentic_ai_tutorial.pdf', 'total_pages': 74, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:22:04+00:00', 'trapped': '', 'modDate': 'D:20250918142204Z', 'creationDate': \"D:20250918142204+00'00'\", 'page': 29}, page_content='from transformers import AutoTokenizer \\ndef analyze_tokenization(text, model_names): \\n    \"\"\"Analyze how different tokenizers handle the same text\"\"\" \\n    print(f\"Original text: \\'{text}\\'\\\\n\") \\n     \\n    for model_name in model_names: \\n        tokenizer = AutoTokenizer.from_pretrained(model_name) \\n         \\n        # Tokenize \\n        tokens = tokenizer.tokenize(text) \\n        token_ids = tokenizer.encode(text) \\n        decoded = tokenizer.decode(token_ids) \\n         \\n        print(f\"Model: {model_name}\") \\n        print(f\"  Tokens: {tokens}\") \\n        print(f\"  Token IDs: {token_ids}\") \\n        print(f\"  Decoded: \\'{decoded}\\'\") \\n        print(f\"  Vocab size: {tokenizer.vocab_size}\") \\n        print(f\"  Number of tokens: {len(tokens)}\") \\n        print() \\n# Compare different tokenizers \\ntext = \"Hello, I\\'m using Hugging Face transformers!\" \\nmodels = [\\n    \"bert-base-uncased\", \\n    \"gpt2\", \\n    \"roberta-base\", \\n    \"t5-base\" \\n] \\nanalyze_tokenization(text, models) \\nCustom Tokenization'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:04+00:00', 'source': '../data/pdf_files/agentic_ai_tutorial.pdf', 'file_path': '../data/pdf_files/agentic_ai_tutorial.pdf', 'total_pages': 74, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:22:04+00:00', 'trapped': '', 'modDate': 'D:20250918142204Z', 'creationDate': \"D:20250918142204+00'00'\", 'page': 30}, page_content='def custom_tokenization_pipeline(texts, model_name=\"bert-base-uncased\"): \\n    \"\"\"Custom tokenization with special handling\"\"\" \\n    tokenizer = AutoTokenizer.from_pretrained(model_name) \\n     \\n    results = [] \\n    for text in texts: \\n        # Basic tokenization \\n        basic_tokens = tokenizer(text) \\n         \\n        # Add special tokens handling \\n        encoded = tokenizer( \\n            text, \\n            add_special_tokens=True, \\n            padding=\"max_length\", \\n            truncation=True, \\n            max_length=128, \\n            return_tensors=\"pt\", \\n            return_attention_mask=True, \\n            return_token_type_ids=True if \"bert\" in model_name else False \\n        ) \\n         \\n        # Token analysis \\n        tokens = tokenizer.convert_ids_to_tokens(encoded[\"input_ids\"][0]) \\n         \\n        results.append({ \\n            \"original_text\": text, \\n            \"tokens\": tokens, \\n            \"input_ids\": encoded[\"input_ids\"], \\n            \"attention_mask\": encoded[\"attention_mask\"], \\n            \"special_tokens\": { \\n                \"CLS\": tokenizer.cls_token, \\n                \"SEP\": tokenizer.sep_token, \\n                \"PAD\": tokenizer.pad_token, \\n                \"UNK\": tokenizer.unk_token \\n            } \\n        })\\n     \\n    return results \\n# Usage \\ntexts = [ \\n    \"Short text\", \\n    \"This is a much longer text that might need truncation depending on the model\\'s maximum sequence length\", \\n    \"Text with special characters: @#$%!\" \\n] \\ntokenization_results = custom_tokenization_pipeline(texts) \\nfor i, result in enumerate(tokenization_results): \\n    print(f\"Example {i+1}:\") \\n    print(f\"  Original: {result[\\'original_text\\']}\") \\n    print(f\"  Tokens: {result[\\'tokens\\'][:10]}...\")  # First 10 tokens \\n    print(f\"  Length: {len(result[\\'tokens\\'])}\") \\n    print() \\nFast Tokenizers'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:04+00:00', 'source': '../data/pdf_files/agentic_ai_tutorial.pdf', 'file_path': '../data/pdf_files/agentic_ai_tutorial.pdf', 'total_pages': 74, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:22:04+00:00', 'trapped': '', 'modDate': 'D:20250918142204Z', 'creationDate': \"D:20250918142204+00'00'\", 'page': 31}, page_content='from transformers import AutoTokenizer \\nimport time \\ndef compare_tokenizer_speed(texts, model_name=\"bert-base-uncased\"): \\n    \"\"\"Compare fast vs slow tokenizer performance\"\"\" \\n     \\n    # Load both versions \\n    fast_tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True) \\n    slow_tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False) \\n     \\n    # Benchmark fast tokenizer \\n    start_time = time.time() \\n    fast_results = fast_tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\") \\n    fast_time = time.time() - start_time \\n     \\n    # Benchmark slow tokenizer   \\n    start_time = time.time() \\n    slow_results = slow_tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\") \\n    slow_time = time.time() - start_time \\n     \\n    print(f\"Fast tokenizer: {fast_time:.4f} seconds\") \\n    print(f\"Slow tokenizer: {slow_time:.4f} seconds\") \\n    print(f\"Speedup: {slow_time/fast_time:.2f}x\") \\n     \\n    return fast_results, slow_results \\n# Test with many texts \\nlarge_texts = [\"This is a test sentence.\"] * 1000 \\nfast_result, slow_result = compare_tokenizer_speed(large_texts) \\nFine-tuning Models\\nBasic Fine-tuning Setup'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:04+00:00', 'source': '../data/pdf_files/agentic_ai_tutorial.pdf', 'file_path': '../data/pdf_files/agentic_ai_tutorial.pdf', 'total_pages': 74, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:22:04+00:00', 'trapped': '', 'modDate': 'D:20250918142204Z', 'creationDate': \"D:20250918142204+00'00'\", 'page': 32}, page_content='from transformers import TrainingArguments, Trainer \\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support \\ndef setup_fine_tuning(model_name, num_labels): \\n    \"\"\"Setup model and tokenizer for fine-tuning\"\"\" \\n    tokenizer = AutoTokenizer.from_pretrained(model_name) \\n    model = AutoModelForSequenceClassification.from_pretrained( \\n        model_name, \\n        num_labels=num_labels \\n    )\\n     \\n    # Add padding token if needed \\n    if tokenizer.pad_token is None: \\n        tokenizer.pad_token = tokenizer.eos_token \\n        model.config.pad_token_id = tokenizer.eos_token_id \\n     \\n    return model, tokenizer \\n# Define compute metrics function \\ndef compute_metrics(eval_pred): \\n    predictions, labels = eval_pred \\n    predictions = predictions.argmax(axis=-1) \\n     \\n    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average=\\'weighted\\') \\n    accuracy = accuracy_score(labels, predictions) \\n     \\n    return { \\n        \\'accuracy\\': accuracy, \\n        \\'f1\\': f1, \\n        \\'precision\\': precision, \\n        \\'recall\\': recall \\n    }\\nText Classification Fine-tuning'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:04+00:00', 'source': '../data/pdf_files/agentic_ai_tutorial.pdf', 'file_path': '../data/pdf_files/agentic_ai_tutorial.pdf', 'total_pages': 74, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:22:04+00:00', 'trapped': '', 'modDate': 'D:20250918142204Z', 'creationDate': \"D:20250918142204+00'00'\", 'page': 33}, page_content='def fine_tune_text_classifier(): \\n    \"\"\"Fine-tune BERT for text classification\"\"\" \\n     \\n    # Load and preprocess data \\n    dataset = load_dataset(\"imdb\") \\n    model, tokenizer = setup_fine_tuning(\"bert-base-uncased\", num_labels=2) \\n     \\n    def preprocess_function(examples): \\n        return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=256) \\n     \\n    # Preprocess datasets \\n    encoded_dataset = dataset.map(preprocess_function, batched=True) \\n    encoded_dataset = encoded_dataset.remove_columns([\"text\"]) \\n    encoded_dataset = encoded_dataset.rename_column(\"label\", \"labels\") \\n    encoded_dataset.set_format(\"torch\") \\n     \\n    # Use smaller subset for demo \\n    train_dataset = encoded_dataset[\"train\"].shuffle().select(range(1000)) \\n    eval_dataset = encoded_dataset[\"test\"].shuffle().select(range(200)) \\n     \\n    # Training arguments \\n    training_args = TrainingArguments( \\n        output_dir=\"./results\", \\n        num_train_epochs=3, \\n        per_device_train_batch_size=16, \\n        per_device_eval_batch_size=16, \\n        warmup_steps=500, \\n        weight_decay=0.01, \\n        logging_dir=\"./logs\", \\n        logging_steps=10, \\n        evaluation_strategy=\"epoch\", \\n        save_strategy=\"epoch\", \\n        load_best_model_at_end=True, \\n        metric_for_best_model=\"accuracy\", \\n        greater_is_better=True, \\n    )\\n     \\n    # Initialize trainer \\n    trainer = Trainer( \\n        model=model, \\n        args=training_args, \\n        train_dataset=train_dataset, \\n        eval_dataset=eval_dataset, \\n        tokenizer=tokenizer, \\n        compute_metrics=compute_metrics, \\n    )\\n     \\n    # Train \\n    trainer.train() \\n     \\n    # Evaluate \\n    eval_results = trainer.evaluate() \\n    print(f\"Evaluation results: {eval_results}\") \\n     \\n    return model, tokenizer, trainer \\n# Run fine-tuning (commented out for demo) \\n# model, tokenizer, trainer = fine_tune_text_classifier() \\nParameter-Efficient Fine-tuning (LoRA)'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:04+00:00', 'source': '../data/pdf_files/agentic_ai_tutorial.pdf', 'file_path': '../data/pdf_files/agentic_ai_tutorial.pdf', 'total_pages': 74, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:22:04+00:00', 'trapped': '', 'modDate': 'D:20250918142204Z', 'creationDate': \"D:20250918142204+00'00'\", 'page': 34}, page_content='from peft import get_peft_model, LoraConfig, TaskType \\ndef setup_lora_fine_tuning(model_name, num_labels): \\n    \"\"\"Setup LoRA fine-tuning for efficient training\"\"\" \\n     \\n    # Load base model \\n    model = AutoModelForSequenceClassification.from_pretrained( \\n        model_name, \\n        num_labels=num_labels \\n    )\\n     \\n    # LoRA configuration \\n    peft_config = LoraConfig( \\n        task_type=TaskType.SEQ_CLS,  # Sequence classification \\n        inference_mode=False, \\n        r=8,                        # Rank \\n        lora_alpha=32,              # LoRA scaling parameter \\n        lora_dropout=0.1,           # LoRA dropout \\n        target_modules=[\"query\", \"value\"],  # Target attention modules \\n    )\\n     \\n    # Apply LoRA \\n    model = get_peft_model(model, peft_config) \\n    model.print_trainable_parameters() \\n     \\n    return model \\n# Usage \\nlora_model = setup_lora_fine_tuning(\"bert-base-uncased\", num_labels=2) \\nCustom Training Loop'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:04+00:00', 'source': '../data/pdf_files/agentic_ai_tutorial.pdf', 'file_path': '../data/pdf_files/agentic_ai_tutorial.pdf', 'total_pages': 74, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:22:04+00:00', 'trapped': '', 'modDate': 'D:20250918142204Z', 'creationDate': \"D:20250918142204+00'00'\", 'page': 35}, page_content='def custom_training_loop(model, tokenizer, train_dataloader, eval_dataloader, num_epochs=3): \\n    \"\"\"Custom training loop with more control\"\"\" \\n    from torch.optim import AdamW \\n    from transformers import get_linear_schedule_with_warmup \\n     \\n    # Optimizer and scheduler \\n    optimizer = AdamW(model.parameters(), lr=2e-5) \\n    total_steps = len(train_dataloader) * num_epochs \\n    scheduler = get_linear_schedule_with_warmup( \\n        optimizer, \\n        num_warmup_steps=0, \\n        num_training_steps=total_steps \\n    )\\n     \\n    # Training loop \\n    model.train() \\n    for epoch in range(num_epochs): \\n        total_loss = 0 \\n         \\n        for batch_idx, batch in enumerate(train_dataloader): \\n            # Forward pass \\n            outputs = model( \\n                input_ids=batch[\"input_ids\"], \\n                attention_mask=batch[\"attention_mask\"], \\n                labels=batch[\"labels\"] \\n            ) \\n             \\n            loss = outputs.loss \\n            total_loss += loss.item() \\n             \\n            # Backward pass \\n            loss.backward() \\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) \\n             \\n            optimizer.step() \\n            scheduler.step() \\n            optimizer.zero_grad() \\n             \\n            if batch_idx % 50 == 0: \\n                print(f\"Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item():.4f}\") \\n         \\n        avg_loss = total_loss / len(train_dataloader) \\n        print(f\"Epoch {epoch} completed. Average loss: {avg_loss:.4f}\") \\n         \\n        # Evaluation \\n        model.eval() \\n        eval_accuracy = evaluate_model(model, eval_dataloader) \\n        print(f\"Evaluation accuracy: {eval_accuracy:.4f}\") \\n        model.train() \\ndef evaluate_model(model, dataloader): \\n    \"\"\"Evaluate model accuracy\"\"\" \\n    correct = 0 \\n    total = 0 \\n     \\n    with torch.no_grad(): \\n        for batch in dataloader: \\n            outputs = model( \\n                input_ids=batch[\"input_ids\"], \\n                attention_mask=batch[\"attention_mask\"] \\n            )'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:04+00:00', 'source': '../data/pdf_files/agentic_ai_tutorial.pdf', 'file_path': '../data/pdf_files/agentic_ai_tutorial.pdf', 'total_pages': 74, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:22:04+00:00', 'trapped': '', 'modDate': 'D:20250918142204Z', 'creationDate': \"D:20250918142204+00'00'\", 'page': 36}, page_content='predictions = torch.argmax(outputs.logits, dim=-1) \\n            correct += (predictions == batch[\"labels\"]).sum().item() \\n            total += batch[\"labels\"].size(0) \\n     \\n    return correct / total \\nModel Hub and Sharing\\nExploring the Hub\\nfrom huggingface_hub import HfApi, list_models, model_info \\napi = HfApi() \\n# List models with filters \\nmodels = list_models( \\n    task=\"text-classification\", \\n    library=\"transformers\", \\n    language=\"en\", \\n    sort=\"downloads\", \\n    limit=10 \\n) \\nprint(\"Top 10 English text classification models:\") \\nfor model in models: \\n    print(f\"- {model.id} ({model.downloads} downloads)\") \\n# Get detailed model information \\nmodel_details = model_info(\"bert-base-uncased\") \\nprint(f\"\\\\nModel: {model_details.id}\") \\nprint(f\"Library: {model_details.library_name}\") \\nprint(f\"Downloads: {model_details.downloads}\") \\nprint(f\"Tags: {model_details.tags}\") \\nUploading Models'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:04+00:00', 'source': '../data/pdf_files/agentic_ai_tutorial.pdf', 'file_path': '../data/pdf_files/agentic_ai_tutorial.pdf', 'total_pages': 74, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:22:04+00:00', 'trapped': '', 'modDate': 'D:20250918142204Z', 'creationDate': \"D:20250918142204+00'00'\", 'page': 37}, page_content='from huggingface_hub import Repository, upload_folder \\ndef upload_model_to_hub(model, tokenizer, repo_name, commit_message=\"Upload model\"): \\n    \"\"\"Upload trained model to Hugging Face Hub\"\"\" \\n     \\n    # Save model locally first \\n    model.save_pretrained(f\"./{repo_name}\") \\n    tokenizer.save_pretrained(f\"./{repo_name}\") \\n     \\n    # Create model card \\n    model_card_content = f\"\"\" \\n--- \\nlanguage: en \\nlicense: apache-2.0 \\ntags:\\n- text-classification \\n- sentiment-analysis \\ndatasets: \\n- imdb \\nmetrics: \\n- accuracy\\n--- \\n# {repo_name} \\nThis model is a fine-tuned version of BERT for sentiment analysis on the IMDB dataset. \\n## Usage \\n```python \\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification \\ntokenizer = AutoTokenizer.from_pretrained(\"your-username/{repo_name}\") \\nmodel = AutoModelForSequenceClassification.from_pretrained(\"your-username/{repo_name}\") \\n# Use the model \\ninputs = tokenizer(\"I love this movie!\", return_tensors=\"pt\") \\noutputs = model(**inputs) \\npredictions = torch.nn.functional.softmax(outputs.logits, dim=-1) \\nTraining Data\\nThe model was trained on the IMDB movie reviews dataset.\\nTraining Procedure\\nLearning rate: 2e-5\\nBatch size: 16\\nNumber of epochs: 3\\nEvaluation Results\\nAccuracy: 92.5%\\nF1-score: 0.925 \"\"\"\\nwith open(f\".//README.md\", \"w\") as f: f.write(model_card_content)\\nUpload to hub'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:04+00:00', 'source': '../data/pdf_files/agentic_ai_tutorial.pdf', 'file_path': '../data/pdf_files/agentic_ai_tutorial.pdf', 'total_pages': 74, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:22:04+00:00', 'trapped': '', 'modDate': 'D:20250918142204Z', 'creationDate': \"D:20250918142204+00'00'\", 'page': 38}, page_content='upload_folder( folder_path=f\"./\", repo_id=f\"your-username/\", repo_type=\"model\", commit_message=commit_message )\\nUsage (example)\\nupload_model_to_hub(fine_tuned_model,\\ntokenizer, \"my-sentiment-model\")\\n### Model Versioning and Management \\n```python \\ndef manage_model_versions(repo_id): \\n    \"\"\"Manage different versions of a model\"\"\" \\n    api = HfApi() \\n     \\n    # List all commits (versions) \\n    commits = api.list_repo_commits(repo_id) \\n    print(f\"Model versions for {repo_id}:\") \\n    for commit in commits[:5]:  # Show last 5 versions \\n        print(f\"- {commit.commit_id[:8]}: {commit.title}\") \\n     \\n    # Load specific version \\n    specific_version_model = AutoModel.from_pretrained( \\n        repo_id, \\n        revision=commits[1].commit_id  # Load second-to-last version \\n    )\\n     \\n    return specific_version_model \\n# Usage \\n# model_versions = manage_model_versions(\"bert-base-uncased\") \\nInference Endpoints\\nLocal Inference Optimization'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:04+00:00', 'source': '../data/pdf_files/agentic_ai_tutorial.pdf', 'file_path': '../data/pdf_files/agentic_ai_tutorial.pdf', 'total_pages': 74, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:22:04+00:00', 'trapped': '', 'modDate': 'D:20250918142204Z', 'creationDate': \"D:20250918142204+00'00'\", 'page': 39}, page_content='import torch \\nfrom transformers import pipeline \\ndef optimize_for_inference(model_name): \\n    \"\"\"Optimize model for faster inference\"\"\" \\n     \\n    # Load with optimizations \\n    classifier = pipeline( \\n        \"text-classification\", \\n        model=model_name, \\n        torch_dtype=torch.float16,  # Use half precision \\n        device_map=\"auto\"           # Automatic device mapping \\n    )\\n     \\n    # Batch processing function \\n    def batch_predict(texts, batch_size=32): \\n        results = [] \\n        for i in range(0, len(texts), batch_size): \\n            batch = texts[i:i+batch_size] \\n            batch_results = classifier(batch) \\n            results.extend(batch_results) \\n        return results \\n     \\n    return classifier, batch_predict \\n# Usage \\nclassifier, batch_predict = optimize_for_inference(\"cardiffnlp/twitter-roberta-base-sentiment-latest\") \\n# Test batch prediction \\ntexts = [\"I love this!\", \"This is terrible\", \"Not bad\"] * 100 \\nresults = batch_predict(texts) \\nprint(f\"Processed {len(results)} texts\") \\nQuantization for Edge Deployment'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:04+00:00', 'source': '../data/pdf_files/agentic_ai_tutorial.pdf', 'file_path': '../data/pdf_files/agentic_ai_tutorial.pdf', 'total_pages': 74, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:22:04+00:00', 'trapped': '', 'modDate': 'D:20250918142204Z', 'creationDate': \"D:20250918142204+00'00'\", 'page': 40}, page_content='from transformers import AutoModelForSequenceClassification, BitsAndBytesConfig \\ndef quantize_model(model_name, quantization_type=\"8bit\"): \\n    \"\"\"Quantize model for reduced memory usage\"\"\" \\n     \\n    if quantization_type == \"8bit\": \\n        quantization_config = BitsAndBytesConfig(load_in_8bit=True) \\n    elif quantization_type == \"4bit\": \\n        quantization_config = BitsAndBytesConfig( \\n            load_in_4bit=True, \\n            bnb_4bit_compute_dtype=torch.float16, \\n            bnb_4bit_quant_type=\"nf4\", \\n            bnb_4bit_use_double_quant=True \\n        ) \\n    else: \\n        quantization_config = None \\n     \\n    model = AutoModelForSequenceClassification.from_pretrained( \\n        model_name, \\n        quantization_config=quantization_config, \\n        device_map=\"auto\" \\n    )\\n     \\n    return model \\n# Usage \\nquantized_model = quantize_model(\"bert-base-uncased\", \"8bit\") \\nprint(f\"Model memory footprint reduced with 8-bit quantization\") \\nONNX Export for Production\\ndef export_to_onnx(model_name, output_path=\"model.onnx\"): \\n    \"\"\"Export model to ONNX format for production deployment\"\"\" \\n    from transformers.onnx import export \\n    from transformers import AutoTokenizer, AutoModel \\n    from pathlib import Path \\n     \\n    # Load model and tokenizer \\n    tokenizer = AutoTokenizer.from_pretrained(model_name) \\n    model = AutoModel.from_pretrained(model_name) \\n     \\n    # Export to ONNX \\n    onnx_path = Path(output_path) \\n    export( \\n        preprocessor=tokenizer, \\n        model=model, \\n        config=model.config, \\n        opset=14, \\n        output=onnx_path \\n    )\\n     \\n    return onnx_path \\n# Usage \\n# onnx_path = export_to_onnx(\"distilbert-base-uncased\") \\n# print(f\"Model exported to {onnx_path}\") \\nBest Practices'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:04+00:00', 'source': '../data/pdf_files/agentic_ai_tutorial.pdf', 'file_path': '../data/pdf_files/agentic_ai_tutorial.pdf', 'total_pages': 74, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:22:04+00:00', 'trapped': '', 'modDate': 'D:20250918142204Z', 'creationDate': \"D:20250918142204+00'00'\", 'page': 41}, page_content='Memory Management\\nimport gc \\nimport torch \\ndef optimize_memory_usage(): \\n    \"\"\"Best practices for memory management\"\"\" \\n     \\n    # Clear cache \\n    torch.cuda.empty_cache() \\n    gc.collect() \\n     \\n    # Use gradient checkpointing for large models \\n    model.gradient_checkpointing_enable() \\n     \\n    # Use mixed precision training \\n    from torch.cuda.amp import autocast, GradScaler \\n     \\n    scaler = GradScaler() \\n     \\n    # Training with mixed precision \\n    with autocast(): \\n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels) \\n        loss = outputs.loss \\n     \\n    scaler.scale(loss).backward() \\n    scaler.step(optimizer) \\n    scaler.update() \\ndef monitor_gpu_memory(): \\n    \"\"\"Monitor GPU memory usage\"\"\" \\n    if torch.cuda.is_available(): \\n        print(f\"GPU memory allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\") \\n        print(f\"GPU memory cached: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\") \\n        print(f\"GPU memory free: {torch.cuda.mem_get_info()[0] / 1024**3:.2f} GB\") \\nError Handling and Robustness'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:04+00:00', 'source': '../data/pdf_files/agentic_ai_tutorial.pdf', 'file_path': '../data/pdf_files/agentic_ai_tutorial.pdf', 'total_pages': 74, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:22:04+00:00', 'trapped': '', 'modDate': 'D:20250918142204Z', 'creationDate': \"D:20250918142204+00'00'\", 'page': 42}, page_content='def robust_model_loading(model_name, fallback_model=\"distilbert-base-uncased\"): \\n    \"\"\"Robust model loading with fallback\"\"\" \\n    try: \\n        tokenizer = AutoTokenizer.from_pretrained(model_name) \\n        model = AutoModel.from_pretrained(model_name) \\n        print(f\"Successfully loaded {model_name}\") \\n        return model, tokenizer \\n     \\n    except Exception as e: \\n        print(f\"Failed to load {model_name}: {e}\") \\n        print(f\"Falling back to {fallback_model}\") \\n         \\n        try: \\n            tokenizer = AutoTokenizer.from_pretrained(fallback_model) \\n            model = AutoModel.from_pretrained(fallback_model) \\n            return model, tokenizer \\n        except Exception as e: \\n            print(f\"Failed to load fallback model: {e}\") \\n            raise \\ndef safe_inference(model, tokenizer, text, max_retries=3): \\n    \"\"\"Safe inference with retry logic\"\"\" \\n    for attempt in range(max_retries): \\n        try: \\n            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True) \\n             \\n            with torch.no_grad(): \\n                outputs = model(**inputs) \\n             \\n            return outputs \\n             \\n        except RuntimeError as e: \\n            if \"out of memory\" in str(e).lower(): \\n                print(f\"OOM error, attempt {attempt + 1}/{max_retries}\") \\n                torch.cuda.empty_cache() \\n                # Reduce batch size or sequence length \\n                if attempt < max_retries - 1: \\n                    continue \\n            raise \\n        except Exception as e: \\n            print(f\"Inference error: {e}\") \\n            if attempt == max_retries - 1: \\n                raise \\nPerformance Monitoring'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:04+00:00', 'source': '../data/pdf_files/agentic_ai_tutorial.pdf', 'file_path': '../data/pdf_files/agentic_ai_tutorial.pdf', 'total_pages': 74, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:22:04+00:00', 'trapped': '', 'modDate': 'D:20250918142204Z', 'creationDate': \"D:20250918142204+00'00'\", 'page': 43}, page_content='import time \\nfrom functools import wraps \\ndef benchmark_function(func): \\n    \"\"\"Decorator to benchmark function execution time\"\"\" \\n    @wraps(func) \\n    def wrapper(*args, **kwargs): \\n        start_time = time.time() \\n        result = func(*args, **kwargs) \\n        end_time = time.time() \\n        print(f\"{func.__name__} executed in {end_time - start_time:.4f} seconds\") \\n        return result \\n    return wrapper \\n@benchmark_function \\ndef benchmark_model_inference(model, tokenizer, texts): \\n    \"\"\"Benchmark model inference speed\"\"\" \\n    all_results = [] \\n     \\n    for text in texts: \\n        inputs = tokenizer(text, return_tensors=\"pt\") \\n        with torch.no_grad(): \\n            outputs = model(**inputs) \\n        all_results.append(outputs) \\n     \\n    return all_results \\n# Usage \\ntexts = [\"Sample text for benchmarking\"] * 100 \\n# results = benchmark_model_inference(model, tokenizer, texts) \\nModel Selection Guide'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:04+00:00', 'source': '../data/pdf_files/agentic_ai_tutorial.pdf', 'file_path': '../data/pdf_files/agentic_ai_tutorial.pdf', 'total_pages': 74, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:22:04+00:00', 'trapped': '', 'modDate': 'D:20250918142204Z', 'creationDate': \"D:20250918142204+00'00'\", 'page': 44}, page_content='def recommend_model(task, performance_priority=\"balanced\"): \\n    \"\"\"Recommend model based on task and performance requirements\"\"\" \\n     \\n    recommendations = { \\n        \"text-classification\": { \\n            \"speed\": \"distilbert-base-uncased\", \\n            \"balanced\": \"bert-base-uncased\",  \\n            \"accuracy\": \"roberta-large\" \\n        },\\n        \"question-answering\": { \\n            \"speed\": \"distilbert-base-cased-distilled-squad\", \\n            \"balanced\": \"bert-base-cased\", \\n            \"accuracy\": \"roberta-large-squad2\" \\n        },\\n        \"text-generation\": { \\n            \"speed\": \"gpt2\", \\n            \"balanced\": \"gpt2-medium\", \\n            \"accuracy\": \"gpt2-large\" \\n        },\\n        \"summarization\": { \\n            \"speed\": \"facebook/bart-base\", \\n            \"balanced\": \"facebook/bart-large-cnn\", \\n            \"accuracy\": \"google/pegasus-large\" \\n        } \\n    }\\n     \\n    if task in recommendations: \\n        return recommendations[task].get(performance_priority, recommendations[task][\"balanced\"]) \\n    else: \\n        return \"bert-base-uncased\"  # Default fallback \\n# Usage \\nmodel_name = recommend_model(\"text-classification\", \"speed\") \\nprint(f\"Recommended model: {model_name}\") \\nConclusion\\nHugging Face provides a comprehensive ecosystem for working with transformer models. This tutorial covered:\\nUsing pre-trained models with pipelines\\nUnderstanding the Transformers library architecture\\nWorking with datasets and tokenizers\\nFine-tuning models for custom tasks\\nSharing models on the Hub\\nOptimizing models for production\\nKey Takeaways:\\n1. Start with pipelines for quick prototyping\\n2. Use Auto classes for flexibility\\n3. Preprocess data carefully for best results\\n4. Consider parameter-efficient fine-tuning (LoRA/QLoRA)\\n5. Optimize models for production deployment\\n6. Monitor performance and memory usage\\nNext Steps:\\n1. Explore domain-specific models on the Hub\\n2. Experiment with multimodal models (vision + language)\\n3. Try advanced fine-tuning techniques\\n4. Build end-to-end applications with Gradio/Streamlit\\n5. Contribute models back to the community'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:04+00:00', 'source': '../data/pdf_files/agentic_ai_tutorial.pdf', 'file_path': '../data/pdf_files/agentic_ai_tutorial.pdf', 'total_pages': 74, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:22:04+00:00', 'trapped': '', 'modDate': 'D:20250918142204Z', 'creationDate': \"D:20250918142204+00'00'\", 'page': 45}, page_content='Additional Resources:\\nHugging Face Documentation (https://huggingface.co/docs)\\nTransformers Course (https://huggingface.co/course)\\nCommunity Forums (https://discuss.huggingface.co)\\nModel Hub (https://huggingface.co/models)\\nLangGraph Tutorial: Building Complex\\nAgent Workflows\\nTable of Contents\\n1. Introduction to LangGraph\\n2. Core Concepts\\n3. Installation and Setup\\n4. Basic Graph Construction\\n5. Advanced Patterns\\n6. Real-World Examples\\n7. Best Practices\\nIntroduction to LangGraph\\nLangGraph is a library for building stateful, multi-actor applications with LLMs, built on top of LangChain. It extends the LangChain Expression Language with the ability to\\ncoordinate multiple chains (or actors) across multiple steps of computation in a cyclic manner.\\nKey Features:\\nStateful: Maintains state across multiple turns of conversation\\nMulti-actor: Coordinate multiple LLM chains or agents\\nCyclic: Support for loops and conditional branching\\nHuman-in-the-loop: Easy integration of human feedback\\nStreaming: Real-time streaming of intermediate results\\nCore Concepts\\n1. Nodes\\nNodes represent individual processing units in your graph. Each node is a function that takes the current state and returns an updated state.\\ndef my_node(state): \\n    # Process the state \\n    return {\"messages\": state[\"messages\"] + [new_message]} \\n2. Edges\\nEdges define the flow between nodes. LangGraph supports:\\nNormal edges: Direct connections between nodes\\nConditional edges: Branching based on state evaluation\\nStart/End edges: Entry and exit points\\n3. State\\nState is the shared data structure that flows through your graph. It\\'s typically a dictionary that gets updated by each node.\\n4. Checkpoints'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:04+00:00', 'source': '../data/pdf_files/agentic_ai_tutorial.pdf', 'file_path': '../data/pdf_files/agentic_ai_tutorial.pdf', 'total_pages': 74, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:22:04+00:00', 'trapped': '', 'modDate': 'D:20250918142204Z', 'creationDate': \"D:20250918142204+00'00'\", 'page': 46}, page_content='Checkpoints allow you to save and restore the state at any point in the execution.\\nInstallation and Setup\\n# Install LangGraph \\npip install langgraph \\n# For development \\npip install langgraph[dev] \\n# With additional integrations \\npip install \"langgraph[anthropic,openai]\" \\nEnvironment Setup\\nimport os \\nfrom langgraph.graph import StateGraph, END \\nfrom langgraph.prebuilt import ToolExecutor \\nfrom langchain_openai import ChatOpenAI \\nfrom langchain_core.messages import HumanMessage, AIMessage \\n# Set your API keys \\nos.environ[\"OPENAI_API_KEY\"] = \"your-api-key-here\" \\nBasic Graph Construction\\nSimple Linear Flow\\nfrom langgraph.graph import StateGraph, END \\nfrom typing import TypedDict, List \\nfrom langchain_core.messages import BaseMessage \\nclass GraphState(TypedDict): \\n    messages: List[BaseMessage] \\ndef chatbot(state: GraphState): \\n    messages = state[\"messages\"] \\n    llm = ChatOpenAI() \\n    response = llm.invoke(messages) \\n    return {\"messages\": [response]} \\ndef create_simple_graph(): \\n    workflow = StateGraph(GraphState) \\n     \\n    # Add nodes \\n    workflow.add_node(\"chatbot\", chatbot) \\n     \\n    # Add edges \\n    workflow.set_entry_point(\"chatbot\") \\n    workflow.add_edge(\"chatbot\", END) \\n     \\n    return workflow.compile() \\n# Usage \\napp = create_simple_graph() \\nresult = app.invoke({\"messages\": [HumanMessage(content=\"Hello!\")]}) \\nprint(result[\"messages\"][-1].content)'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:04+00:00', 'source': '../data/pdf_files/agentic_ai_tutorial.pdf', 'file_path': '../data/pdf_files/agentic_ai_tutorial.pdf', 'total_pages': 74, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:22:04+00:00', 'trapped': '', 'modDate': 'D:20250918142204Z', 'creationDate': \"D:20250918142204+00'00'\", 'page': 47}, page_content='Conditional Branching\\ndef should_continue(state: GraphState) -> str: \\n    messages = state[\"messages\"] \\n    last_message = messages[-1] \\n     \\n    if \"FINAL ANSWER\" in last_message.content: \\n        return \"end\" \\n    else: \\n        return \"continue\" \\ndef researcher(state: GraphState): \\n    # Research logic here \\n    return {\"messages\": state[\"messages\"] + [AIMessage(content=\"Research complete\")]} \\ndef writer(state: GraphState): \\n    # Writing logic here   \\n    return {\"messages\": state[\"messages\"] + [AIMessage(content=\"FINAL ANSWER: Here\\'s the result\")]} \\ndef create_conditional_graph(): \\n    workflow = StateGraph(GraphState) \\n     \\n    workflow.add_node(\"researcher\", researcher) \\n    workflow.add_node(\"writer\", writer) \\n     \\n    workflow.set_entry_point(\"researcher\") \\n    workflow.add_conditional_edges( \\n        \"researcher\", \\n        should_continue, \\n        { \\n            \"continue\": \"writer\",  \\n            \"end\": END \\n        } \\n    )\\n    workflow.add_edge(\"writer\", END) \\n     \\n    return workflow.compile() \\nAdvanced Patterns\\nMulti-Agent Collaboration'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:04+00:00', 'source': '../data/pdf_files/agentic_ai_tutorial.pdf', 'file_path': '../data/pdf_files/agentic_ai_tutorial.pdf', 'total_pages': 74, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:22:04+00:00', 'trapped': '', 'modDate': 'D:20250918142204Z', 'creationDate': \"D:20250918142204+00'00'\", 'page': 48}, page_content='from langgraph.prebuilt import create_react_agent \\nfrom langchain_core.tools import Tool \\nclass MultiAgentState(TypedDict): \\n    messages: List[BaseMessage] \\n    next_agent: str \\n     \\ndef create_multi_agent_system(): \\n    # Create specialized agents \\n    researcher = create_react_agent( \\n        ChatOpenAI(),  \\n        [search_tool, calculator_tool] \\n    )\\n     \\n    writer = create_react_agent( \\n        ChatOpenAI(), \\n        [writing_tool, fact_checker_tool] \\n    )\\n     \\n    def agent_node(state, agent, name): \\n        result = agent.invoke(state) \\n        return { \\n            \"messages\": [AIMessage(content=result[\"output\"])], \\n            \"next_agent\": determine_next_agent(result) \\n        } \\n     \\n    workflow = StateGraph(MultiAgentState) \\n    workflow.add_node(\"researcher\", lambda x: agent_node(x, researcher, \"researcher\")) \\n    workflow.add_node(\"writer\", lambda x: agent_node(x, writer, \"writer\")) \\n     \\n    # Routing logic \\n    def route_next(state): \\n        return state.get(\"next_agent\", \"writer\") \\n     \\n    workflow.add_conditional_edges(\"researcher\", route_next) \\n    workflow.add_conditional_edges(\"writer\", route_next) \\n     \\n    return workflow.compile() \\nHuman-in-the-Loop'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:04+00:00', 'source': '../data/pdf_files/agentic_ai_tutorial.pdf', 'file_path': '../data/pdf_files/agentic_ai_tutorial.pdf', 'total_pages': 74, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:22:04+00:00', 'trapped': '', 'modDate': 'D:20250918142204Z', 'creationDate': \"D:20250918142204+00'00'\", 'page': 49}, page_content='from langgraph.checkpoint.sqlite import SqliteSaver \\ndef create_human_in_loop_graph(): \\n    memory = SqliteSaver.from_conn_string(\":memory:\") \\n     \\n    def human_feedback(state): \\n        # This will pause execution and wait for human input \\n        pass \\n     \\n    workflow = StateGraph(GraphState) \\n    workflow.add_node(\"agent\", chatbot) \\n    workflow.add_node(\"human\", human_feedback) \\n     \\n    workflow.set_entry_point(\"agent\") \\n    workflow.add_edge(\"agent\", \"human\") \\n    workflow.add_edge(\"human\", END) \\n     \\n    return workflow.compile(checkpointer=memory, interrupt_before=[\"human\"]) \\n# Usage with interruption \\napp = create_human_in_loop_graph() \\nconfig = {\"configurable\": {\"thread_id\": \"1\"}} \\n# Initial run - will stop at human node \\nresult = app.invoke({\"messages\": [HumanMessage(\"Analyze this data\")]}, config) \\n# Continue after human feedback \\napp.update_state(config, {\"messages\": [HumanMessage(\"User feedback here\")]}) \\nresult = app.invoke(None, config)  # Resume from checkpoint \\nParallel Processing\\ndef create_parallel_processing_graph(): \\n    def parallel_task_1(state): \\n        return {\"task1_result\": \"Result from task 1\"} \\n     \\n    def parallel_task_2(state): \\n        return {\"task2_result\": \"Result from task 2\"} \\n     \\n    def combine_results(state): \\n        combined = f\"{state.get(\\'task1_result\\', \\'\\')} + {state.get(\\'task2_result\\', \\'\\')}\" \\n        return {\"final_result\": combined} \\n     \\n    workflow = StateGraph(dict) \\n    workflow.add_node(\"task1\", parallel_task_1) \\n    workflow.add_node(\"task2\", parallel_task_2) \\n    workflow.add_node(\"combine\", combine_results) \\n     \\n    # Both tasks run in parallel \\n    workflow.set_entry_point(\"task1\") \\n    workflow.set_entry_point(\"task2\") \\n     \\n    # Both feed into combine \\n    workflow.add_edge(\"task1\", \"combine\") \\n    workflow.add_edge(\"task2\", \"combine\") \\n    workflow.add_edge(\"combine\", END) \\n     \\n    return workflow.compile() \\nReal-World Examples'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:04+00:00', 'source': '../data/pdf_files/agentic_ai_tutorial.pdf', 'file_path': '../data/pdf_files/agentic_ai_tutorial.pdf', 'total_pages': 74, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:22:04+00:00', 'trapped': '', 'modDate': 'D:20250918142204Z', 'creationDate': \"D:20250918142204+00'00'\", 'page': 50}, page_content='Research Assistant\\ndef create_research_assistant(): \\n    class ResearchState(TypedDict): \\n        query: str \\n        research_results: List[str] \\n        summary: str \\n        citations: List[str] \\n     \\n    def search_step(state: ResearchState): \\n        # Implement web search \\n        results = web_search(state[\"query\"]) \\n        return {\"research_results\": results} \\n     \\n    def analyze_step(state: ResearchState): \\n        # Analyze search results \\n        analysis = llm_analyze(state[\"research_results\"]) \\n        return {\"summary\": analysis} \\n     \\n    def cite_step(state: ResearchState): \\n        # Generate citations \\n        citations = extract_citations(state[\"research_results\"]) \\n        return {\"citations\": citations} \\n     \\n    workflow = StateGraph(ResearchState) \\n    workflow.add_node(\"search\", search_step) \\n    workflow.add_node(\"analyze\", analyze_step)  \\n    workflow.add_node(\"cite\", cite_step) \\n     \\n    workflow.set_entry_point(\"search\") \\n    workflow.add_edge(\"search\", \"analyze\") \\n    workflow.add_edge(\"analyze\", \"cite\") \\n    workflow.add_edge(\"cite\", END) \\n     \\n    return workflow.compile() \\nCustomer Service Agent'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:04+00:00', 'source': '../data/pdf_files/agentic_ai_tutorial.pdf', 'file_path': '../data/pdf_files/agentic_ai_tutorial.pdf', 'total_pages': 74, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:22:04+00:00', 'trapped': '', 'modDate': 'D:20250918142204Z', 'creationDate': \"D:20250918142204+00'00'\", 'page': 51}, page_content='def create_customer_service_agent(): \\n    class ServiceState(TypedDict): \\n        customer_input: str \\n        intent: str \\n        customer_data: dict \\n        response: str \\n        escalate: bool \\n     \\n    def classify_intent(state: ServiceState): \\n        intent = classify_customer_intent(state[\"customer_input\"]) \\n        return {\"intent\": intent} \\n     \\n    def fetch_customer_data(state: ServiceState): \\n        data = get_customer_info(state.get(\"customer_id\")) \\n        return {\"customer_data\": data} \\n     \\n    def handle_request(state: ServiceState): \\n        response = generate_response( \\n            state[\"intent\"],  \\n            state[\"customer_data\"],  \\n            state[\"customer_input\"] \\n        ) \\n        return {\"response\": response, \"escalate\": should_escalate(response)} \\n     \\n    def escalate_to_human(state: ServiceState): \\n        # Escalation logic \\n        return {\"response\": \"Transferring to human agent...\"} \\n     \\n    def should_escalate_decision(state: ServiceState): \\n        return \"escalate\" if state.get(\"escalate\") else \"respond\" \\n     \\n    workflow = StateGraph(ServiceState) \\n    workflow.add_node(\"classify\", classify_intent) \\n    workflow.add_node(\"fetch_data\", fetch_customer_data) \\n    workflow.add_node(\"handle\", handle_request) \\n    workflow.add_node(\"escalate\", escalate_to_human) \\n     \\n    workflow.set_entry_point(\"classify\") \\n    workflow.add_edge(\"classify\", \"fetch_data\") \\n    workflow.add_edge(\"fetch_data\", \"handle\") \\n    workflow.add_conditional_edges( \\n        \"handle\", \\n        should_escalate_decision, \\n        {\"escalate\": \"escalate\", \"respond\": END} \\n    )\\n    workflow.add_edge(\"escalate\", END) \\n     \\n    return workflow.compile() \\nBest Practices\\n1. State Design\\nKeep state minimal and focused\\nUse TypedDict for type safety\\nAvoid deeply nested structures\\nMake state serializable for checkpoints\\n2. Error Handling'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:04+00:00', 'source': '../data/pdf_files/agentic_ai_tutorial.pdf', 'file_path': '../data/pdf_files/agentic_ai_tutorial.pdf', 'total_pages': 74, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:22:04+00:00', 'trapped': '', 'modDate': 'D:20250918142204Z', 'creationDate': \"D:20250918142204+00'00'\", 'page': 52}, page_content='def robust_node(state): \\n    try: \\n        # Your logic here \\n        result = process_data(state) \\n        return {\"result\": result, \"error\": None} \\n    except Exception as e: \\n        return {\"result\": None, \"error\": str(e)} \\ndef error_recovery(state): \\n    if state.get(\"error\"): \\n        # Implement recovery logic \\n        return {\"error\": None, \"retry_count\": state.get(\"retry_count\", 0) + 1} \\n    return state \\n3. Testing Strategies\\ndef test_graph(): \\n    app = create_your_graph() \\n     \\n    # Test individual nodes \\n    test_state = {\"messages\": [HumanMessage(\"test\")]} \\n    result = app.get_node(\"your_node\").invoke(test_state) \\n    assert \"expected_key\" in result \\n     \\n    # Test full flow \\n    final_result = app.invoke(test_state) \\n    assert final_result[\"messages\"][-1].content is not None \\n4. Performance Optimization\\nUse streaming for long-running operations\\nImplement proper caching strategies\\nConsider parallel execution where possible\\nMonitor state size and complexity\\n5. Monitoring and Debugging\\nfrom langgraph.prebuilt import ToolExecutor \\nimport logging \\nlogging.basicConfig(level=logging.INFO) \\ndef debug_node(state): \\n    logging.info(f\"Node input: {state}\") \\n    result = your_processing_function(state) \\n    logging.info(f\"Node output: {result}\") \\n    return result \\nConclusion\\nLangGraph provides a powerful framework for building complex, stateful AI applications. By understanding its core concepts and patterns, you can create sophisticated multi-\\nagent systems that handle real-world complexity.\\nNext Steps:\\n1. Experiment with the examples provided\\n2. Build your own custom nodes and edges\\n3. Explore the LangGraph documentation for advanced features\\n4. Join the LangChain community for support and updates'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:04+00:00', 'source': '../data/pdf_files/agentic_ai_tutorial.pdf', 'file_path': '../data/pdf_files/agentic_ai_tutorial.pdf', 'total_pages': 74, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:22:04+00:00', 'trapped': '', 'modDate': 'D:20250918142204Z', 'creationDate': \"D:20250918142204+00'00'\", 'page': 53}, page_content=\"Additional Resources:\\nLangGraph Documentation (https://python.langchain.com/docs/langgraph)\\nLangGraph Examples Repository (https://github.com/langchain-ai/langgraph/tree/main/examples)\\nLangChain Community (https://discord.gg/cU2adEyC7w)\\nOpenAI API Tutorial: Complete Guide to\\nGPT Models and Beyond\\nTable of Contents\\n1. Introduction to OpenAI API\\n2. Setup and Authentication\\n3. Chat Completions\\n4. Text Generation\\n5. Function Calling\\n6. Embeddings\\n7. Vision Models\\n8. Audio and Speech\\n9. Fine-tuning\\n10. Best Practices\\nIntroduction to OpenAI API\\nThe OpenAI API provides access to powerful AI models including GPT-4, GPT-3.5, DALL-E, Whisper, and more. This tutorial covers everything you need to know to integrate\\nOpenAI's models into your applications.\\nAvailable Models:\\nGPT-4: Most capable model for complex reasoning\\nGPT-3.5: Fast and efficient for most tasks\\nGPT-4 Vision: Understands images and text\\nDALL-E 3: Generate images from text\\nWhisper: Speech-to-text transcription\\nTTS: Text-to-speech synthesis\\nSetup and Authentication\\nInstallation\\n# Install the OpenAI Python library \\npip install openai \\n# For async support \\npip install openai[async] \\n# Latest version with all features \\npip install openai>=1.0.0 \\nAPI Key Setup\"), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:04+00:00', 'source': '../data/pdf_files/agentic_ai_tutorial.pdf', 'file_path': '../data/pdf_files/agentic_ai_tutorial.pdf', 'total_pages': 74, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:22:04+00:00', 'trapped': '', 'modDate': 'D:20250918142204Z', 'creationDate': \"D:20250918142204+00'00'\", 'page': 54}, page_content='import openai \\nimport os \\nfrom openai import OpenAI \\n# Method 1: Environment variable (recommended) \\nos.environ[\"OPENAI_API_KEY\"] = \"your-api-key-here\" \\nclient = OpenAI() \\n# Method 2: Direct initialization \\nclient = OpenAI(api_key=\"your-api-key-here\") \\n# Method 3: Using Azure OpenAI \\nfrom openai import AzureOpenAI \\nazure_client = AzureOpenAI( \\n    api_key=\"your-azure-key\", \\n    api_version=\"2023-12-01-preview\", \\n    azure_endpoint=\"https://your-endpoint.openai.azure.com/\" \\n) \\nBasic Configuration\\n# Set default parameters \\nclient = OpenAI( \\n    api_key=\"your-key\", \\n    organization=\"your-org-id\",  # Optional \\n    project=\"your-project-id\",   # Optional \\n    base_url=\"https://api.openai.com/v1\",  # Custom endpoint if needed \\n    default_headers={\"Custom-Header\": \"value\"} \\n) \\nChat Completions\\nBasic Chat Completion\\ndef basic_chat_completion(): \\n    response = client.chat.completions.create( \\n        model=\"gpt-4\", \\n        messages=[ \\n            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, \\n            {\"role\": \"user\", \"content\": \"Hello! How can you help me today?\"} \\n        ],\\n        max_tokens=150, \\n        temperature=0.7 \\n    )\\n     \\n    return response.choices[0].message.content \\nprint(basic_chat_completion()) \\nAdvanced Parameters'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:04+00:00', 'source': '../data/pdf_files/agentic_ai_tutorial.pdf', 'file_path': '../data/pdf_files/agentic_ai_tutorial.pdf', 'total_pages': 74, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:22:04+00:00', 'trapped': '', 'modDate': 'D:20250918142204Z', 'creationDate': \"D:20250918142204+00'00'\", 'page': 55}, page_content='def advanced_chat_completion(): \\n    response = client.chat.completions.create( \\n        model=\"gpt-4\", \\n        messages=[ \\n            {\"role\": \"system\", \"content\": \"You are an expert Python developer.\"}, \\n            {\"role\": \"user\", \"content\": \"Explain list comprehensions with examples.\"} \\n        ],\\n        max_tokens=500, \\n        temperature=0.3,           # Lower = more focused \\n        top_p=0.9,                # Nucleus sampling \\n        frequency_penalty=0.0,     # Reduce repetition \\n        presence_penalty=0.0,      # Encourage new topics \\n        stop=[\"\\\\n\\\\n\", \"###\"],      # Stop sequences \\n        seed=42                    # For reproducible outputs \\n    )\\n     \\n    return response.choices[0].message.content \\nStreaming Responses\\ndef streaming_chat(): \\n    stream = client.chat.completions.create( \\n        model=\"gpt-4\", \\n        messages=[{\"role\": \"user\", \"content\": \"Tell me a long story about AI\"}], \\n        stream=True \\n    )\\n     \\n    full_response = \"\" \\n    for chunk in stream: \\n        if chunk.choices[0].delta.content is not None: \\n            content = chunk.choices[0].delta.content \\n            print(content, end=\"\", flush=True) \\n            full_response += content \\n     \\n    return full_response \\nConversation Management'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:04+00:00', 'source': '../data/pdf_files/agentic_ai_tutorial.pdf', 'file_path': '../data/pdf_files/agentic_ai_tutorial.pdf', 'total_pages': 74, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:22:04+00:00', 'trapped': '', 'modDate': 'D:20250918142204Z', 'creationDate': \"D:20250918142204+00'00'\", 'page': 56}, page_content='class ChatManager: \\n    def __init__(self, system_message=\"You are a helpful assistant.\"): \\n        self.messages = [{\"role\": \"system\", \"content\": system_message}] \\n        self.client = OpenAI() \\n     \\n    def add_user_message(self, content): \\n        self.messages.append({\"role\": \"user\", \"content\": content}) \\n     \\n    def add_assistant_message(self, content): \\n        self.messages.append({\"role\": \"assistant\", \"content\": content}) \\n     \\n    def get_response(self, user_input): \\n        self.add_user_message(user_input) \\n         \\n        response = self.client.chat.completions.create( \\n            model=\"gpt-4\", \\n            messages=self.messages, \\n            max_tokens=500, \\n            temperature=0.7 \\n        ) \\n         \\n        assistant_message = response.choices[0].message.content \\n        self.add_assistant_message(assistant_message) \\n         \\n        return assistant_message \\n     \\n    def clear_history(self, keep_system=True): \\n        if keep_system and self.messages[0][\"role\"] == \"system\": \\n            self.messages = [self.messages[0]] \\n        else: \\n            self.messages = [] \\n# Usage \\nchat = ChatManager() \\nresponse1 = chat.get_response(\"What is machine learning?\") \\nresponse2 = chat.get_response(\"Can you give me an example?\") \\nText Generation\\nLegacy Completions (GPT-3.5-turbo-instruct)\\ndef text_completion(): \\n    response = client.completions.create( \\n        model=\"gpt-3.5-turbo-instruct\", \\n        prompt=\"Complete this sentence: The future of AI is\", \\n        max_tokens=100, \\n        temperature=0.8, \\n        stop=[\"\\\\n\"] \\n    )\\n     \\n    return response.choices[0].text.strip() \\nCreative Writing Assistant'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:04+00:00', 'source': '../data/pdf_files/agentic_ai_tutorial.pdf', 'file_path': '../data/pdf_files/agentic_ai_tutorial.pdf', 'total_pages': 74, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:22:04+00:00', 'trapped': '', 'modDate': 'D:20250918142204Z', 'creationDate': \"D:20250918142204+00'00'\", 'page': 57}, page_content='def creative_writer(prompt, style=\"narrative\", length=\"medium\"): \\n    length_map = { \\n        \"short\": 200, \\n        \"medium\": 500, \\n        \"long\": 1000 \\n    }\\n     \\n    system_message = f\"\"\"You are a creative writing assistant specializing in {style} writing.  \\n    Create engaging, well-structured content that captures the reader\\'s attention.\"\"\" \\n     \\n    response = client.chat.completions.create( \\n        model=\"gpt-4\", \\n        messages=[ \\n            {\"role\": \"system\", \"content\": system_message}, \\n            {\"role\": \"user\", \"content\": f\"Write a {style} piece based on: {prompt}\"} \\n        ],\\n        max_tokens=length_map.get(length, 500), \\n        temperature=0.8, \\n        presence_penalty=0.1 \\n    )\\n     \\n    return response.choices[0].message.content \\nFunction Calling\\nBasic Function Calling'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:04+00:00', 'source': '../data/pdf_files/agentic_ai_tutorial.pdf', 'file_path': '../data/pdf_files/agentic_ai_tutorial.pdf', 'total_pages': 74, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:22:04+00:00', 'trapped': '', 'modDate': 'D:20250918142204Z', 'creationDate': \"D:20250918142204+00'00'\", 'page': 58}, page_content='import json \\ndef get_weather(location, unit=\"celsius\"): \\n    \"\"\"Simulate weather API call\"\"\" \\n    return { \\n        \"location\": location, \\n        \"temperature\": \"22\", \\n        \"unit\": unit, \\n        \"condition\": \"sunny\" \\n    }\\ndef function_calling_example(): \\n    # Define the function schema \\n    tools = [ \\n        { \\n            \"type\": \"function\", \\n            \"function\": { \\n                \"name\": \"get_weather\", \\n                \"description\": \"Get current weather for a location\", \\n                \"parameters\": { \\n                    \"type\": \"object\", \\n                    \"properties\": { \\n                        \"location\": { \\n                            \"type\": \"string\", \\n                            \"description\": \"City name\" \\n                        }, \\n                        \"unit\": { \\n                            \"type\": \"string\", \\n                            \"enum\": [\"celsius\", \"fahrenheit\"] \\n                        } \\n                    }, \\n                    \"required\": [\"location\"] \\n                } \\n            } \\n        } \\n    ]\\n     \\n    messages = [ \\n        {\"role\": \"user\", \"content\": \"What\\'s the weather like in Paris?\"} \\n    ]\\n     \\n    # First call - model decides to use function \\n    response = client.chat.completions.create( \\n        model=\"gpt-4\", \\n        messages=messages, \\n        tools=tools, \\n        tool_choice=\"auto\" \\n    )\\n     \\n    # Check if model wants to call a function \\n    if response.choices[0].message.tool_calls: \\n        tool_call = response.choices[0].message.tool_calls[0] \\n        function_name = tool_call.function.name \\n        function_args = json.loads(tool_call.function.arguments) \\n         \\n        # Execute the function \\n        if function_name == \"get_weather\": \\n            function_result = get_weather(**function_args) \\n             \\n            # Add function result to conversation \\n            messages.append(response.choices[0].message) \\n            messages.append({'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:04+00:00', 'source': '../data/pdf_files/agentic_ai_tutorial.pdf', 'file_path': '../data/pdf_files/agentic_ai_tutorial.pdf', 'total_pages': 74, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:22:04+00:00', 'trapped': '', 'modDate': 'D:20250918142204Z', 'creationDate': \"D:20250918142204+00'00'\", 'page': 59}, page_content='\"role\": \"tool\", \\n                \"content\": json.dumps(function_result), \\n                \"tool_call_id\": tool_call.id \\n            }) \\n             \\n            # Get final response \\n            final_response = client.chat.completions.create( \\n                model=\"gpt-4\", \\n                messages=messages \\n            ) \\n             \\n            return final_response.choices[0].message.content \\n     \\n    return response.choices[0].message.content \\nMultiple Function Agent'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:04+00:00', 'source': '../data/pdf_files/agentic_ai_tutorial.pdf', 'file_path': '../data/pdf_files/agentic_ai_tutorial.pdf', 'total_pages': 74, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:22:04+00:00', 'trapped': '', 'modDate': 'D:20250918142204Z', 'creationDate': \"D:20250918142204+00'00'\", 'page': 60}, page_content='class FunctionAgent: \\n    def __init__(self): \\n        self.client = OpenAI() \\n        self.functions = { \\n            \"calculator\": self.calculate, \\n            \"search\": self.search, \\n            \"save_note\": self.save_note \\n        } \\n         \\n        self.tools = [ \\n            { \\n                \"type\": \"function\", \\n                \"function\": { \\n                    \"name\": \"calculator\", \\n                    \"description\": \"Perform mathematical calculations\", \\n                    \"parameters\": { \\n                        \"type\": \"object\", \\n                        \"properties\": { \\n                            \"expression\": {\"type\": \"string\", \"description\": \"Math expression\"} \\n                        }, \\n                        \"required\": [\"expression\"] \\n                    } \\n                } \\n            }, \\n            { \\n                \"type\": \"function\",  \\n                \"function\": { \\n                    \"name\": \"search\", \\n                    \"description\": \"Search for information\", \\n                    \"parameters\": { \\n                        \"type\": \"object\", \\n                        \"properties\": { \\n                            \"query\": {\"type\": \"string\", \"description\": \"Search query\"} \\n                        }, \\n                        \"required\": [\"query\"] \\n                    } \\n                } \\n            } \\n        ] \\n     \\n    def calculate(self, expression): \\n        try: \\n            result = eval(expression)  # Use safely in production! \\n            return f\"Result: {result}\" \\n        except: \\n            return \"Error in calculation\" \\n     \\n    def search(self, query): \\n        return f\"Search results for \\'{query}\\': [Simulated results]\" \\n     \\n    def save_note(self, content): \\n        # Simulate saving \\n        return f\"Note saved: {content[:50]}...\" \\n     \\n    def run(self, user_input): \\n        messages = [{\"role\": \"user\", \"content\": user_input}] \\n         \\n        response = self.client.chat.completions.create( \\n            model=\"gpt-4\", \\n            messages=messages, \\n            tools=self.tools, \\n            tool_choice=\"auto\"'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:04+00:00', 'source': '../data/pdf_files/agentic_ai_tutorial.pdf', 'file_path': '../data/pdf_files/agentic_ai_tutorial.pdf', 'total_pages': 74, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:22:04+00:00', 'trapped': '', 'modDate': 'D:20250918142204Z', 'creationDate': \"D:20250918142204+00'00'\", 'page': 61}, page_content=') \\n         \\n        # Handle tool calls \\n        if response.choices[0].message.tool_calls: \\n            messages.append(response.choices[0].message) \\n             \\n            for tool_call in response.choices[0].message.tool_calls: \\n                function_name = tool_call.function.name \\n                function_args = json.loads(tool_call.function.arguments) \\n                 \\n                if function_name in self.functions: \\n                    result = self.functions[function_name](**function_args) \\n                    messages.append({ \\n                        \"role\": \"tool\", \\n                        \"content\": result, \\n                        \"tool_call_id\": tool_call.id \\n                    }) \\n             \\n            # Get final response \\n            final_response = self.client.chat.completions.create( \\n                model=\"gpt-4\", \\n                messages=messages \\n            ) \\n            return final_response.choices[0].message.content \\n         \\n        return response.choices[0].message.content \\n# Usage \\nagent = FunctionAgent() \\nresult = agent.run(\"What\\'s 25 * 47 + 123?\") \\nEmbeddings\\nBasic Embeddings\\ndef get_embeddings(texts): \\n    if isinstance(texts, str): \\n        texts = [texts] \\n     \\n    response = client.embeddings.create( \\n        model=\"text-embedding-3-large\",  # or text-embedding-3-small \\n        input=texts \\n    )\\n     \\n    return [embedding.embedding for embedding in response.data] \\n# Usage \\ntext = \"This is a sample sentence for embedding.\" \\nembeddings = get_embeddings(text) \\nprint(f\"Embedding dimension: {len(embeddings[0])}\") \\nSemantic Search System'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:04+00:00', 'source': '../data/pdf_files/agentic_ai_tutorial.pdf', 'file_path': '../data/pdf_files/agentic_ai_tutorial.pdf', 'total_pages': 74, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:22:04+00:00', 'trapped': '', 'modDate': 'D:20250918142204Z', 'creationDate': \"D:20250918142204+00'00'\", 'page': 62}, page_content='import numpy as np \\nfrom sklearn.metrics.pairwise import cosine_similarity \\nclass SemanticSearch: \\n    def __init__(self): \\n        self.client = OpenAI() \\n        self.documents = [] \\n        self.embeddings = [] \\n     \\n    def add_documents(self, docs): \\n        \"\"\"Add documents to search index\"\"\" \\n        self.documents.extend(docs) \\n         \\n        # Get embeddings for new documents \\n        response = self.client.embeddings.create( \\n            model=\"text-embedding-3-large\", \\n            input=docs \\n        ) \\n         \\n        new_embeddings = [emb.embedding for emb in response.data] \\n        self.embeddings.extend(new_embeddings) \\n     \\n    def search(self, query, top_k=5): \\n        \"\"\"Search for similar documents\"\"\" \\n        if not self.embeddings: \\n            return [] \\n         \\n        # Get query embedding \\n        query_response = self.client.embeddings.create( \\n            model=\"text-embedding-3-large\", \\n            input=[query] \\n        ) \\n        query_embedding = query_response.data[0].embedding \\n         \\n        # Calculate similarities \\n        similarities = cosine_similarity( \\n            [query_embedding],  \\n            self.embeddings \\n        )[0] \\n         \\n        # Get top results \\n        top_indices = np.argsort(similarities)[::-1][:top_k] \\n         \\n        results = [] \\n        for idx in top_indices: \\n            results.append({ \\n                \"document\": self.documents[idx], \\n                \"similarity\": similarities[idx] \\n            }) \\n         \\n        return results \\n# Usage \\nsearch_engine = SemanticSearch() \\nsearch_engine.add_documents([ \\n    \"Python is a programming language\", \\n    \"Machine learning is a subset of AI\", \\n    \"Deep learning uses neural networks\", \\n    \"Natural language processing handles text\" \\n]) \\nresults = search_engine.search(\"What is AI?\", top_k=2)'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:04+00:00', 'source': '../data/pdf_files/agentic_ai_tutorial.pdf', 'file_path': '../data/pdf_files/agentic_ai_tutorial.pdf', 'total_pages': 74, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:22:04+00:00', 'trapped': '', 'modDate': 'D:20250918142204Z', 'creationDate': \"D:20250918142204+00'00'\", 'page': 63}, page_content='for result in results: \\n    print(f\"Similarity: {result[\\'similarity\\']:.3f}\") \\n    print(f\"Document: {result[\\'document\\']}\\\\n\") \\nVision Models\\nImage Analysis\\nimport base64 \\nimport requests \\ndef encode_image(image_path): \\n    \"\"\"Encode image to base64\"\"\" \\n    with open(image_path, \"rb\") as image_file: \\n        return base64.b64encode(image_file.read()).decode(\\'utf-8\\') \\ndef analyze_image(image_path, prompt=\"What\\'s in this image?\"): \\n    base64_image = encode_image(image_path) \\n     \\n    response = client.chat.completions.create( \\n        model=\"gpt-4-vision-preview\", \\n        messages=[ \\n            { \\n                \"role\": \"user\", \\n                \"content\": [ \\n                    {\"type\": \"text\", \"text\": prompt}, \\n                    { \\n                        \"type\": \"image_url\", \\n                        \"image_url\": { \\n                            \"url\": f\"data:image/jpeg;base64,{base64_image}\", \\n                            \"detail\": \"high\"  # or \"low\" for faster processing \\n                        } \\n                    } \\n                ] \\n            } \\n        ],\\n        max_tokens=300 \\n    )\\n     \\n    return response.choices[0].message.content \\n# Usage with URL \\ndef analyze_image_url(image_url, prompt=\"Describe this image\"): \\n    response = client.chat.completions.create( \\n        model=\"gpt-4-vision-preview\", \\n        messages=[ \\n            { \\n                \"role\": \"user\", \\n                \"content\": [ \\n                    {\"type\": \"text\", \"text\": prompt}, \\n                    {\"type\": \"image_url\", \"image_url\": {\"url\": image_url}} \\n                ] \\n            } \\n        ],\\n        max_tokens=300 \\n    )\\n     \\n    return response.choices[0].message.content \\nDocument OCR and Analysis'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:04+00:00', 'source': '../data/pdf_files/agentic_ai_tutorial.pdf', 'file_path': '../data/pdf_files/agentic_ai_tutorial.pdf', 'total_pages': 74, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:22:04+00:00', 'trapped': '', 'modDate': 'D:20250918142204Z', 'creationDate': \"D:20250918142204+00'00'\", 'page': 64}, page_content='def document_analyzer(image_path): \\n    \"\"\"Extract and analyze text from documents\"\"\" \\n    base64_image = encode_image(image_path) \\n     \\n    response = client.chat.completions.create( \\n        model=\"gpt-4-vision-preview\", \\n        messages=[ \\n            { \\n                \"role\": \"user\", \\n                \"content\": [ \\n                    { \\n                        \"type\": \"text\",  \\n                        \"text\": \"Extract all text from this document and provide a summary of its key points.\" \\n                    }, \\n                    { \\n                        \"type\": \"image_url\", \\n                        \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"} \\n                    } \\n                ] \\n            } \\n        ],\\n        max_tokens=1000 \\n    )\\n     \\n    return response.choices[0].message.content \\nAudio and Speech\\nSpeech-to-Text (Whisper)'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:04+00:00', 'source': '../data/pdf_files/agentic_ai_tutorial.pdf', 'file_path': '../data/pdf_files/agentic_ai_tutorial.pdf', 'total_pages': 74, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:22:04+00:00', 'trapped': '', 'modDate': 'D:20250918142204Z', 'creationDate': \"D:20250918142204+00'00'\", 'page': 65}, page_content='def transcribe_audio(audio_file_path): \\n    \"\"\"Transcribe audio file using Whisper\"\"\" \\n    with open(audio_file_path, \"rb\") as audio_file: \\n        transcription = client.audio.transcriptions.create( \\n            model=\"whisper-1\", \\n            file=audio_file, \\n            response_format=\"text\" \\n        ) \\n     \\n    return transcription \\ndef transcribe_with_timestamps(audio_file_path): \\n    \"\"\"Get transcription with timestamps\"\"\" \\n    with open(audio_file_path, \"rb\") as audio_file: \\n        transcription = client.audio.transcriptions.create( \\n            model=\"whisper-1\", \\n            file=audio_file, \\n            response_format=\"verbose_json\", \\n            timestamp_granularities=[\"word\"] \\n        ) \\n     \\n    return transcription \\n# Translation \\ndef translate_audio(audio_file_path): \\n    \"\"\"Translate foreign language audio to English\"\"\" \\n    with open(audio_file_path, \"rb\") as audio_file: \\n        translation = client.audio.translations.create( \\n            model=\"whisper-1\", \\n            file=audio_file \\n        ) \\n     \\n    return translation.text \\nText-to-Speech'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:04+00:00', 'source': '../data/pdf_files/agentic_ai_tutorial.pdf', 'file_path': '../data/pdf_files/agentic_ai_tutorial.pdf', 'total_pages': 74, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:22:04+00:00', 'trapped': '', 'modDate': 'D:20250918142204Z', 'creationDate': \"D:20250918142204+00'00'\", 'page': 66}, page_content='def text_to_speech(text, voice=\"alloy\", output_file=\"speech.mp3\"): \\n    \"\"\"Convert text to speech\"\"\" \\n    response = client.audio.speech.create( \\n        model=\"tts-1\",  # or \"tts-1-hd\" for higher quality \\n        voice=voice,    # alloy, echo, fable, onyx, nova, shimmer \\n        input=text, \\n        speed=1.0       # 0.25 to 4.0 \\n    )\\n     \\n    with open(output_file, \"wb\") as f: \\n        for chunk in response.iter_bytes(): \\n            f.write(chunk) \\n     \\n    return output_file \\n# Real-time streaming \\ndef streaming_text_to_speech(text, voice=\"alloy\"): \\n    \"\"\"Stream audio in real-time\"\"\" \\n    response = client.audio.speech.create( \\n        model=\"tts-1\", \\n        voice=voice, \\n        input=text, \\n        response_format=\"opus\"  # Better for streaming \\n    )\\n     \\n    # Play audio chunks as they arrive \\n    for chunk in response.iter_bytes(chunk_size=1024): \\n        # Send to audio player \\n        yield chunk \\nFine-tuning\\nPrepare Training Data'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:04+00:00', 'source': '../data/pdf_files/agentic_ai_tutorial.pdf', 'file_path': '../data/pdf_files/agentic_ai_tutorial.pdf', 'total_pages': 74, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:22:04+00:00', 'trapped': '', 'modDate': 'D:20250918142204Z', 'creationDate': \"D:20250918142204+00'00'\", 'page': 67}, page_content='import json \\ndef prepare_training_data(examples): \\n    \"\"\"Prepare data for fine-tuning\"\"\" \\n    training_data = [] \\n     \\n    for example in examples: \\n        training_data.append({ \\n            \"messages\": [ \\n                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, \\n                {\"role\": \"user\", \"content\": example[\"input\"]}, \\n                {\"role\": \"assistant\", \"content\": example[\"output\"]} \\n            ] \\n        })\\n     \\n    # Save to JSONL file \\n    with open(\"training_data.jsonl\", \"w\") as f: \\n        for item in training_data: \\n            f.write(json.dumps(item) + \"\\\\n\") \\n     \\n    return \"training_data.jsonl\" \\n# Example data \\nexamples = [ \\n    {\"input\": \"What is Python?\", \"output\": \"Python is a programming language...\"}, \\n    {\"input\": \"How do lists work?\", \"output\": \"Lists in Python are ordered collections...\"} \\n] \\ntraining_file = prepare_training_data(examples) \\nFine-tuning Process'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:04+00:00', 'source': '../data/pdf_files/agentic_ai_tutorial.pdf', 'file_path': '../data/pdf_files/agentic_ai_tutorial.pdf', 'total_pages': 74, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:22:04+00:00', 'trapped': '', 'modDate': 'D:20250918142204Z', 'creationDate': \"D:20250918142204+00'00'\", 'page': 68}, page_content='def create_fine_tuning_job(training_file, model=\"gpt-3.5-turbo\"): \\n    \"\"\"Create a fine-tuning job\"\"\" \\n    # Upload training file \\n    with open(training_file, \"rb\") as f: \\n        file_response = client.files.create( \\n            file=f, \\n            purpose=\"fine-tune\" \\n        ) \\n     \\n    # Create fine-tuning job \\n    job = client.fine_tuning.jobs.create( \\n        training_file=file_response.id, \\n        model=model, \\n        hyperparameters={ \\n            \"n_epochs\": 3, \\n            \"batch_size\": 1, \\n            \"learning_rate_multiplier\": 2 \\n        } \\n    )\\n     \\n    return job \\ndef monitor_fine_tuning(job_id): \\n    \"\"\"Monitor fine-tuning progress\"\"\" \\n    job = client.fine_tuning.jobs.retrieve(job_id) \\n     \\n    print(f\"Job ID: {job.id}\") \\n    print(f\"Status: {job.status}\") \\n    print(f\"Model: {job.fine_tuned_model}\") \\n     \\n    # Get events \\n    events = client.fine_tuning.jobs.list_events(job_id) \\n    for event in events.data[:5]:  # Show last 5 events \\n        print(f\"{event.created_at}: {event.message}\") \\n     \\n    return job \\ndef use_fine_tuned_model(model_id, prompt): \\n    \"\"\"Use your fine-tuned model\"\"\" \\n    response = client.chat.completions.create( \\n        model=model_id, \\n        messages=[ \\n            {\"role\": \"user\", \"content\": prompt} \\n        ] \\n    )\\n     \\n    return response.choices[0].message.content \\nBest Practices\\nError Handling'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:04+00:00', 'source': '../data/pdf_files/agentic_ai_tutorial.pdf', 'file_path': '../data/pdf_files/agentic_ai_tutorial.pdf', 'total_pages': 74, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:22:04+00:00', 'trapped': '', 'modDate': 'D:20250918142204Z', 'creationDate': \"D:20250918142204+00'00'\", 'page': 69}, page_content='from openai import RateLimitError, APIError \\nimport time \\ndef robust_api_call(func, max_retries=3, backoff_factor=2): \\n    \"\"\"Robust API call with retry logic\"\"\" \\n    for attempt in range(max_retries): \\n        try: \\n            return func() \\n        except RateLimitError: \\n            if attempt == max_retries - 1: \\n                raise \\n            wait_time = backoff_factor ** attempt \\n            print(f\"Rate limit hit, waiting {wait_time} seconds...\") \\n            time.sleep(wait_time) \\n        except APIError as e: \\n            print(f\"API Error: {e}\") \\n            if attempt == max_retries - 1: \\n                raise \\n            time.sleep(backoff_factor ** attempt) \\n# Usage \\ndef safe_chat_completion(message): \\n    return robust_api_call( \\n        lambda: client.chat.completions.create( \\n            model=\"gpt-4\", \\n            messages=[{\"role\": \"user\", \"content\": message}] \\n        ) \\n    )\\nToken Management'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:04+00:00', 'source': '../data/pdf_files/agentic_ai_tutorial.pdf', 'file_path': '../data/pdf_files/agentic_ai_tutorial.pdf', 'total_pages': 74, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:22:04+00:00', 'trapped': '', 'modDate': 'D:20250918142204Z', 'creationDate': \"D:20250918142204+00'00'\", 'page': 70}, page_content='import tiktoken \\ndef count_tokens(text, model=\"gpt-4\"): \\n    \"\"\"Count tokens in text\"\"\" \\n    encoding = tiktoken.encoding_for_model(model) \\n    return len(encoding.encode(text)) \\ndef truncate_text(text, max_tokens, model=\"gpt-4\"): \\n    \"\"\"Truncate text to fit within token limit\"\"\" \\n    encoding = tiktoken.encoding_for_model(model) \\n    tokens = encoding.encode(text) \\n     \\n    if len(tokens) <= max_tokens: \\n        return text \\n     \\n    truncated_tokens = tokens[:max_tokens] \\n    return encoding.decode(truncated_tokens) \\ndef smart_chunking(text, chunk_size=1000, model=\"gpt-4\"): \\n    \"\"\"Split text into chunks based on token count\"\"\" \\n    encoding = tiktoken.encoding_for_model(model) \\n    tokens = encoding.encode(text) \\n     \\n    chunks = [] \\n    for i in range(0, len(tokens), chunk_size): \\n        chunk_tokens = tokens[i:i + chunk_size] \\n        chunk_text = encoding.decode(chunk_tokens) \\n        chunks.append(chunk_text) \\n     \\n    return chunks \\nCost Optimization'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:04+00:00', 'source': '../data/pdf_files/agentic_ai_tutorial.pdf', 'file_path': '../data/pdf_files/agentic_ai_tutorial.pdf', 'total_pages': 74, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:22:04+00:00', 'trapped': '', 'modDate': 'D:20250918142204Z', 'creationDate': \"D:20250918142204+00'00'\", 'page': 71}, page_content='class CostTracker: \\n    def __init__(self): \\n        self.costs = { \\n            \"gpt-4\": {\"input\": 0.03, \"output\": 0.06},  # per 1K tokens \\n            \"gpt-3.5-turbo\": {\"input\": 0.001, \"output\": 0.002}, \\n            \"text-embedding-3-large\": {\"input\": 0.00013, \"output\": 0} \\n        } \\n        self.total_cost = 0 \\n     \\n    def calculate_cost(self, model, input_tokens, output_tokens): \\n        if model in self.costs: \\n            cost = ( \\n                (input_tokens / 1000) * self.costs[model][\"input\"] + \\n                (output_tokens / 1000) * self.costs[model][\"output\"] \\n            ) \\n            self.total_cost += cost \\n            return cost \\n        return 0 \\n     \\n    def tracked_completion(self, **kwargs): \\n        response = client.chat.completions.create(**kwargs) \\n         \\n        usage = response.usage \\n        cost = self.calculate_cost( \\n            kwargs[\"model\"], \\n            usage.prompt_tokens, \\n            usage.completion_tokens \\n        ) \\n         \\n        print(f\"Cost: ${cost:.4f} | Total: ${self.total_cost:.4f}\") \\n        return response \\n# Usage \\ntracker = CostTracker() \\nresponse = tracker.tracked_completion( \\n    model=\"gpt-4\", \\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}] \\n) \\nAsync Operations'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:04+00:00', 'source': '../data/pdf_files/agentic_ai_tutorial.pdf', 'file_path': '../data/pdf_files/agentic_ai_tutorial.pdf', 'total_pages': 74, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:22:04+00:00', 'trapped': '', 'modDate': 'D:20250918142204Z', 'creationDate': \"D:20250918142204+00'00'\", 'page': 72}, page_content='import asyncio \\nfrom openai import AsyncOpenAI \\nasync_client = AsyncOpenAI() \\nasync def async_chat_completion(message): \\n    \"\"\"Async chat completion\"\"\" \\n    response = await async_client.chat.completions.create( \\n        model=\"gpt-4\", \\n        messages=[{\"role\": \"user\", \"content\": message}] \\n    )\\n    return response.choices[0].message.content \\nasync def batch_completions(messages): \\n    \"\"\"Process multiple completions concurrently\"\"\" \\n    tasks = [async_chat_completion(msg) for msg in messages] \\n    results = await asyncio.gather(*tasks) \\n    return results \\n# Usage \\nasync def main(): \\n    messages = [ \\n        \"What is Python?\", \\n        \"What is JavaScript?\", \\n        \"What is Rust?\" \\n    ]\\n     \\n    results = await batch_completions(messages) \\n    for i, result in enumerate(results): \\n        print(f\"Question {i+1}: {result[:100]}...\") \\n# Run\\n# asyncio.run(main()) \\nProduction Configuration'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:04+00:00', 'source': '../data/pdf_files/agentic_ai_tutorial.pdf', 'file_path': '../data/pdf_files/agentic_ai_tutorial.pdf', 'total_pages': 74, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:22:04+00:00', 'trapped': '', 'modDate': 'D:20250918142204Z', 'creationDate': \"D:20250918142204+00'00'\", 'page': 73}, page_content='class ProductionOpenAI: \\n    def __init__(self, api_key=None): \\n        self.client = OpenAI( \\n            api_key=api_key or os.getenv(\"OPENAI_API_KEY\"), \\n            timeout=30, \\n            max_retries=3 \\n        ) \\n        self.default_params = { \\n            \"temperature\": 0.7, \\n            \"max_tokens\": 1000, \\n            \"top_p\": 0.9 \\n        } \\n     \\n    def chat(self, messages, **kwargs): \\n        params = {**self.default_params, **kwargs} \\n         \\n        try: \\n            response = self.client.chat.completions.create( \\n                messages=messages, \\n                **params \\n            ) \\n            return { \\n                \"success\": True, \\n                \"content\": response.choices[0].message.content, \\n                \"usage\": response.usage, \\n                \"model\": response.model \\n            } \\n        except Exception as e: \\n            return { \\n                \"success\": False, \\n                \"error\": str(e), \\n                \"content\": None \\n            } \\nConclusion\\nThe OpenAI API provides powerful capabilities for building AI-powered applications. This tutorial covered the essential patterns and best practices for:\\nChat completions and conversation management\\nFunction calling for tool integration\\nEmbeddings for semantic search\\nVision capabilities for image analysis\\nAudio processing with Whisper and TTS\\nFine-tuning for specialized models\\nProduction-ready error handling and optimization\\nNext Steps:\\n1. Experiment with different models and parameters\\n2. Build a complete application using multiple API features\\n3. Implement proper monitoring and cost tracking\\n4. Explore advanced techniques like RAG and agent frameworks\\nAdditional Resources:\\nOpenAI API Documentation (https://platform.openai.com/docs)\\nOpenAI Cookbook (https://github.com/openai/openai-cookbook)\\nBest Practices Guide (https://platform.openai.com/docs/guides/production-best-practices)'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:23:47+00:00', 'source': '../data/pdf_files/langgraph_tutorial.pdf', 'file_path': '../data/pdf_files/langgraph_tutorial.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:23:48+00:00', 'trapped': '', 'modDate': 'D:20250918142348Z', 'creationDate': \"D:20250918142347+00'00'\", 'page': 0}, page_content='LangGraph Tutorial: Building\\nComplex Agent Workflows\\nTable of Contents\\n1. Introduction to LangGraph\\n2. Core Concepts\\n3. Installation and Setup\\n4. Basic Graph Construction\\n5. Advanced Patterns\\n6. Real-World Examples\\n7. Best Practices\\nIntroduction to LangGraph\\nLangGraph is a library for building stateful, multi-actor applications with LLMs, built on top of LangChain. It extends the LangChain\\nExpression Language with the ability to coordinate multiple chains (or actors) across multiple steps of computation in a cyclic manner.\\nKey Features:\\nStateful: Maintains state across multiple turns of conversation\\nMulti-actor: Coordinate multiple LLM chains or agents\\nCyclic: Support for loops and conditional branching\\nHuman-in-the-loop: Easy integration of human feedback\\nStreaming: Real-time streaming of intermediate results\\nCore Concepts\\n1. Nodes\\nNodes represent individual processing units in your graph. Each node is a function that takes the current state and returns an updated\\nstate.\\ndef my_node(state): \\n    # Process the state \\n    return {\"messages\": state[\"messages\"] + [new_message]} \\n2. Edges\\nEdges define the flow between nodes. LangGraph supports:\\nNormal edges: Direct connections between nodes\\nConditional edges: Branching based on state evaluation'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:23:47+00:00', 'source': '../data/pdf_files/langgraph_tutorial.pdf', 'file_path': '../data/pdf_files/langgraph_tutorial.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:23:48+00:00', 'trapped': '', 'modDate': 'D:20250918142348Z', 'creationDate': \"D:20250918142347+00'00'\", 'page': 1}, page_content='Start/End edges: Entry and exit points\\n3. State\\nState is the shared data structure that flows through your graph. It\\'s typically a dictionary that gets updated by each node.\\n4. Checkpoints\\nCheckpoints allow you to save and restore the state at any point in the execution.\\nInstallation and Setup\\n# Install LangGraph \\npip install langgraph \\n# For development \\npip install langgraph[dev] \\n# With additional integrations \\npip install \"langgraph[anthropic,openai]\" \\nEnvironment Setup\\nimport os \\nfrom langgraph.graph import StateGraph, END \\nfrom langgraph.prebuilt import ToolExecutor \\nfrom langchain_openai import ChatOpenAI \\nfrom langchain_core.messages import HumanMessage, AIMessage \\n# Set your API keys \\nos.environ[\"OPENAI_API_KEY\"] = \"your-api-key-here\" \\nBasic Graph Construction\\nSimple Linear Flow'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:23:47+00:00', 'source': '../data/pdf_files/langgraph_tutorial.pdf', 'file_path': '../data/pdf_files/langgraph_tutorial.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:23:48+00:00', 'trapped': '', 'modDate': 'D:20250918142348Z', 'creationDate': \"D:20250918142347+00'00'\", 'page': 2}, page_content='from langgraph.graph import StateGraph, END \\nfrom typing import TypedDict, List \\nfrom langchain_core.messages import BaseMessage \\nclass GraphState(TypedDict): \\n    messages: List[BaseMessage] \\ndef chatbot(state: GraphState): \\n    messages = state[\"messages\"] \\n    llm = ChatOpenAI() \\n    response = llm.invoke(messages) \\n    return {\"messages\": [response]} \\ndef create_simple_graph(): \\n    workflow = StateGraph(GraphState) \\n     \\n    # Add nodes \\n    workflow.add_node(\"chatbot\", chatbot) \\n     \\n    # Add edges \\n    workflow.set_entry_point(\"chatbot\") \\n    workflow.add_edge(\"chatbot\", END) \\n     \\n    return workflow.compile() \\n# Usage \\napp = create_simple_graph() \\nresult = app.invoke({\"messages\": [HumanMessage(content=\"Hello!\")]}) \\nprint(result[\"messages\"][-1].content) \\nConditional Branching'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:23:47+00:00', 'source': '../data/pdf_files/langgraph_tutorial.pdf', 'file_path': '../data/pdf_files/langgraph_tutorial.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:23:48+00:00', 'trapped': '', 'modDate': 'D:20250918142348Z', 'creationDate': \"D:20250918142347+00'00'\", 'page': 3}, page_content='def should_continue(state: GraphState) -> str: \\n    messages = state[\"messages\"] \\n    last_message = messages[-1] \\n     \\n    if \"FINAL ANSWER\" in last_message.content: \\n        return \"end\" \\n    else: \\n        return \"continue\" \\ndef researcher(state: GraphState): \\n    # Research logic here \\n    return {\"messages\": state[\"messages\"] + [AIMessage(content=\"Research complete\")]} \\ndef writer(state: GraphState): \\n    # Writing logic here   \\n    return {\"messages\": state[\"messages\"] + [AIMessage(content=\"FINAL ANSWER: Here\\'s the result\")]} \\ndef create_conditional_graph(): \\n    workflow = StateGraph(GraphState) \\n     \\n    workflow.add_node(\"researcher\", researcher) \\n    workflow.add_node(\"writer\", writer) \\n     \\n    workflow.set_entry_point(\"researcher\") \\n    workflow.add_conditional_edges( \\n        \"researcher\", \\n        should_continue, \\n        { \\n            \"continue\": \"writer\",  \\n            \"end\": END \\n        } \\n    )\\n    workflow.add_edge(\"writer\", END) \\n     \\n    return workflow.compile() \\nAdvanced Patterns\\nMulti-Agent Collaboration'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:23:47+00:00', 'source': '../data/pdf_files/langgraph_tutorial.pdf', 'file_path': '../data/pdf_files/langgraph_tutorial.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:23:48+00:00', 'trapped': '', 'modDate': 'D:20250918142348Z', 'creationDate': \"D:20250918142347+00'00'\", 'page': 4}, page_content='from langgraph.prebuilt import create_react_agent \\nfrom langchain_core.tools import Tool \\nclass MultiAgentState(TypedDict): \\n    messages: List[BaseMessage] \\n    next_agent: str \\n     \\ndef create_multi_agent_system(): \\n    # Create specialized agents \\n    researcher = create_react_agent( \\n        ChatOpenAI(),  \\n        [search_tool, calculator_tool] \\n    )\\n     \\n    writer = create_react_agent( \\n        ChatOpenAI(), \\n        [writing_tool, fact_checker_tool] \\n    )\\n     \\n    def agent_node(state, agent, name): \\n        result = agent.invoke(state) \\n        return { \\n            \"messages\": [AIMessage(content=result[\"output\"])], \\n            \"next_agent\": determine_next_agent(result) \\n        } \\n     \\n    workflow = StateGraph(MultiAgentState) \\n    workflow.add_node(\"researcher\", lambda x: agent_node(x, researcher, \"researcher\")) \\n    workflow.add_node(\"writer\", lambda x: agent_node(x, writer, \"writer\")) \\n     \\n    # Routing logic \\n    def route_next(state): \\n        return state.get(\"next_agent\", \"writer\") \\n     \\n    workflow.add_conditional_edges(\"researcher\", route_next) \\n    workflow.add_conditional_edges(\"writer\", route_next) \\n     \\n    return workflow.compile() \\nHuman-in-the-Loop'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:23:47+00:00', 'source': '../data/pdf_files/langgraph_tutorial.pdf', 'file_path': '../data/pdf_files/langgraph_tutorial.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:23:48+00:00', 'trapped': '', 'modDate': 'D:20250918142348Z', 'creationDate': \"D:20250918142347+00'00'\", 'page': 5}, page_content='from langgraph.checkpoint.sqlite import SqliteSaver \\ndef create_human_in_loop_graph(): \\n    memory = SqliteSaver.from_conn_string(\":memory:\") \\n     \\n    def human_feedback(state): \\n        # This will pause execution and wait for human input \\n        pass \\n     \\n    workflow = StateGraph(GraphState) \\n    workflow.add_node(\"agent\", chatbot) \\n    workflow.add_node(\"human\", human_feedback) \\n     \\n    workflow.set_entry_point(\"agent\") \\n    workflow.add_edge(\"agent\", \"human\") \\n    workflow.add_edge(\"human\", END) \\n     \\n    return workflow.compile(checkpointer=memory, interrupt_before=[\"human\"]) \\n# Usage with interruption \\napp = create_human_in_loop_graph() \\nconfig = {\"configurable\": {\"thread_id\": \"1\"}} \\n# Initial run - will stop at human node \\nresult = app.invoke({\"messages\": [HumanMessage(\"Analyze this data\")]}, config) \\n# Continue after human feedback \\napp.update_state(config, {\"messages\": [HumanMessage(\"User feedback here\")]}) \\nresult = app.invoke(None, config)  # Resume from checkpoint \\nParallel Processing'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:23:47+00:00', 'source': '../data/pdf_files/langgraph_tutorial.pdf', 'file_path': '../data/pdf_files/langgraph_tutorial.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:23:48+00:00', 'trapped': '', 'modDate': 'D:20250918142348Z', 'creationDate': \"D:20250918142347+00'00'\", 'page': 6}, page_content='def create_parallel_processing_graph(): \\n    def parallel_task_1(state): \\n        return {\"task1_result\": \"Result from task 1\"} \\n     \\n    def parallel_task_2(state): \\n        return {\"task2_result\": \"Result from task 2\"} \\n     \\n    def combine_results(state): \\n        combined = f\"{state.get(\\'task1_result\\', \\'\\')} + {state.get(\\'task2_result\\', \\'\\')}\" \\n        return {\"final_result\": combined} \\n     \\n    workflow = StateGraph(dict) \\n    workflow.add_node(\"task1\", parallel_task_1) \\n    workflow.add_node(\"task2\", parallel_task_2) \\n    workflow.add_node(\"combine\", combine_results) \\n     \\n    # Both tasks run in parallel \\n    workflow.set_entry_point(\"task1\") \\n    workflow.set_entry_point(\"task2\") \\n     \\n    # Both feed into combine \\n    workflow.add_edge(\"task1\", \"combine\") \\n    workflow.add_edge(\"task2\", \"combine\") \\n    workflow.add_edge(\"combine\", END) \\n     \\n    return workflow.compile() \\nReal-World Examples\\nResearch Assistant'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:23:47+00:00', 'source': '../data/pdf_files/langgraph_tutorial.pdf', 'file_path': '../data/pdf_files/langgraph_tutorial.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:23:48+00:00', 'trapped': '', 'modDate': 'D:20250918142348Z', 'creationDate': \"D:20250918142347+00'00'\", 'page': 7}, page_content='def create_research_assistant(): \\n    class ResearchState(TypedDict): \\n        query: str \\n        research_results: List[str] \\n        summary: str \\n        citations: List[str] \\n     \\n    def search_step(state: ResearchState): \\n        # Implement web search \\n        results = web_search(state[\"query\"]) \\n        return {\"research_results\": results} \\n     \\n    def analyze_step(state: ResearchState): \\n        # Analyze search results \\n        analysis = llm_analyze(state[\"research_results\"]) \\n        return {\"summary\": analysis} \\n     \\n    def cite_step(state: ResearchState): \\n        # Generate citations \\n        citations = extract_citations(state[\"research_results\"]) \\n        return {\"citations\": citations} \\n     \\n    workflow = StateGraph(ResearchState) \\n    workflow.add_node(\"search\", search_step) \\n    workflow.add_node(\"analyze\", analyze_step)  \\n    workflow.add_node(\"cite\", cite_step) \\n     \\n    workflow.set_entry_point(\"search\") \\n    workflow.add_edge(\"search\", \"analyze\") \\n    workflow.add_edge(\"analyze\", \"cite\") \\n    workflow.add_edge(\"cite\", END) \\n     \\n    return workflow.compile() \\nCustomer Service Agent'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:23:47+00:00', 'source': '../data/pdf_files/langgraph_tutorial.pdf', 'file_path': '../data/pdf_files/langgraph_tutorial.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:23:48+00:00', 'trapped': '', 'modDate': 'D:20250918142348Z', 'creationDate': \"D:20250918142347+00'00'\", 'page': 8}, page_content='def create_customer_service_agent(): \\n    class ServiceState(TypedDict): \\n        customer_input: str \\n        intent: str \\n        customer_data: dict \\n        response: str \\n        escalate: bool \\n     \\n    def classify_intent(state: ServiceState): \\n        intent = classify_customer_intent(state[\"customer_input\"]) \\n        return {\"intent\": intent} \\n     \\n    def fetch_customer_data(state: ServiceState): \\n        data = get_customer_info(state.get(\"customer_id\")) \\n        return {\"customer_data\": data} \\n     \\n    def handle_request(state: ServiceState): \\n        response = generate_response( \\n            state[\"intent\"],  \\n            state[\"customer_data\"],  \\n            state[\"customer_input\"] \\n        ) \\n        return {\"response\": response, \"escalate\": should_escalate(response)} \\n     \\n    def escalate_to_human(state: ServiceState): \\n        # Escalation logic \\n        return {\"response\": \"Transferring to human agent...\"} \\n     \\n    def should_escalate_decision(state: ServiceState): \\n        return \"escalate\" if state.get(\"escalate\") else \"respond\" \\n     \\n    workflow = StateGraph(ServiceState) \\n    workflow.add_node(\"classify\", classify_intent) \\n    workflow.add_node(\"fetch_data\", fetch_customer_data) \\n    workflow.add_node(\"handle\", handle_request) \\n    workflow.add_node(\"escalate\", escalate_to_human) \\n     \\n    workflow.set_entry_point(\"classify\") \\n    workflow.add_edge(\"classify\", \"fetch_data\") \\n    workflow.add_edge(\"fetch_data\", \"handle\") \\n    workflow.add_conditional_edges( \\n        \"handle\", \\n        should_escalate_decision, \\n        {\"escalate\": \"escalate\", \"respond\": END} \\n    )\\n    workflow.add_edge(\"escalate\", END) \\n     \\n    return workflow.compile()'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:23:47+00:00', 'source': '../data/pdf_files/langgraph_tutorial.pdf', 'file_path': '../data/pdf_files/langgraph_tutorial.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:23:48+00:00', 'trapped': '', 'modDate': 'D:20250918142348Z', 'creationDate': \"D:20250918142347+00'00'\", 'page': 9}, page_content='Best Practices\\n1. State Design\\nKeep state minimal and focused\\nUse TypedDict for type safety\\nAvoid deeply nested structures\\nMake state serializable for checkpoints\\n2. Error Handling\\ndef robust_node(state): \\n    try: \\n        # Your logic here \\n        result = process_data(state) \\n        return {\"result\": result, \"error\": None} \\n    except Exception as e: \\n        return {\"result\": None, \"error\": str(e)} \\ndef error_recovery(state): \\n    if state.get(\"error\"): \\n        # Implement recovery logic \\n        return {\"error\": None, \"retry_count\": state.get(\"retry_count\", 0) + 1} \\n    return state \\n3. Testing Strategies\\ndef test_graph(): \\n    app = create_your_graph() \\n     \\n    # Test individual nodes \\n    test_state = {\"messages\": [HumanMessage(\"test\")]} \\n    result = app.get_node(\"your_node\").invoke(test_state) \\n    assert \"expected_key\" in result \\n     \\n    # Test full flow \\n    final_result = app.invoke(test_state) \\n    assert final_result[\"messages\"][-1].content is not None \\n4. Performance Optimization\\nUse streaming for long-running operations\\nImplement proper caching strategies\\nConsider parallel execution where possible\\nMonitor state size and complexity'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:23:47+00:00', 'source': '../data/pdf_files/langgraph_tutorial.pdf', 'file_path': '../data/pdf_files/langgraph_tutorial.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:23:48+00:00', 'trapped': '', 'modDate': 'D:20250918142348Z', 'creationDate': \"D:20250918142347+00'00'\", 'page': 10}, page_content='5. Monitoring and Debugging\\nfrom langgraph.prebuilt import ToolExecutor \\nimport logging \\nlogging.basicConfig(level=logging.INFO) \\ndef debug_node(state): \\n    logging.info(f\"Node input: {state}\") \\n    result = your_processing_function(state) \\n    logging.info(f\"Node output: {result}\") \\n    return result \\nConclusion\\nLangGraph provides a powerful framework for building complex, stateful AI applications. By understanding its core concepts and\\npatterns, you can create sophisticated multi-agent systems that handle real-world complexity.\\nNext Steps:\\n1. Experiment with the examples provided\\n2. Build your own custom nodes and edges\\n3. Explore the LangGraph documentation for advanced features\\n4. Join the LangChain community for support and updates\\nAdditional Resources:\\nLangGraph Documentation (https://python.langchain.com/docs/langgraph)\\nLangGraph Examples Repository (https://github.com/langchain-ai/langgraph/tree/main/examples)\\nLangChain Community (https://discord.gg/cU2adEyC7w)'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:59+00:00', 'source': '../data/pdf_files/huggingface_tutorial.pdf', 'file_path': '../data/pdf_files/huggingface_tutorial.pdf', 'total_pages': 58, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:23:00+00:00', 'trapped': '', 'modDate': 'D:20250918142300Z', 'creationDate': \"D:20250918142259+00'00'\", 'page': 0}, page_content='Hugging Face Tutorial: Complete Guide\\nto Transformers and Model Hub\\nTable of Contents\\n1. Introduction to Hugging Face\\n2. Installation and Setup\\n3. Using Pre-trained Models\\n4. Transformers Library\\n5. Datasets Library\\n6. Tokenizers\\n7. Fine-tuning Models\\n8. Model Hub and Sharing\\n9. Inference Endpoints\\n10. Best Practices\\nIntroduction to Hugging Face\\nHugging Face is the leading platform for machine learning models, providing:\\n\\x00 Transformers: State-of-the-art ML models for PyTorch, TensorFlow, and JAX\\n\\x00 Datasets: The largest collection of ready-to-use datasets\\n\\x00 Model Hub: Over 300,000+ models shared by the community\\n\\x00 Spaces: Collaborative platform for ML demos and applications\\nKey Ecosystems:\\nNLP: BERT, GPT, RoBERTa, T5, and more\\nComputer Vision: Vision Transformer, CLIP, DETR\\nAudio: Wav2Vec2, Whisper, SpeechT5\\nMultimodal: CLIP, DALL-E, Flamingo\\nReinforcement Learning: Decision Transformers\\nInstallation and Setup\\nCore Installation'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:59+00:00', 'source': '../data/pdf_files/huggingface_tutorial.pdf', 'file_path': '../data/pdf_files/huggingface_tutorial.pdf', 'total_pages': 58, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:23:00+00:00', 'trapped': '', 'modDate': 'D:20250918142300Z', 'creationDate': \"D:20250918142259+00'00'\", 'page': 1}, page_content='# Basic installation \\npip install transformers \\n# With PyTorch \\npip install transformers[torch] \\n# With TensorFlow \\npip install transformers[tf] \\n# Full installation with all dependencies \\npip install transformers[all] \\n# Additional libraries \\npip install datasets \\npip install tokenizers \\npip install accelerate \\npip install peft  # For parameter-efficient fine-tuning \\npip install bitsandbytes  # For quantization \\nEnvironment Setup\\nimport torch \\nfrom transformers import ( \\n    AutoTokenizer,  \\n    AutoModel,  \\n    AutoModelForSequenceClassification, \\n    pipeline, \\n    TrainingArguments, \\n    Trainer \\n) \\nfrom datasets import Dataset, load_dataset \\nimport numpy as np \\n# Check if CUDA is available \\nprint(f\"CUDA available: {torch.cuda.is_available()}\") \\nprint(f\"Device: {torch.cuda.get_device_name() if torch.cuda.is_available() else \\'CPU\\'}\") \\n# Set device \\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \\nHugging Face Hub Authentication'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:59+00:00', 'source': '../data/pdf_files/huggingface_tutorial.pdf', 'file_path': '../data/pdf_files/huggingface_tutorial.pdf', 'total_pages': 58, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:23:00+00:00', 'trapped': '', 'modDate': 'D:20250918142300Z', 'creationDate': \"D:20250918142259+00'00'\", 'page': 2}, page_content='from huggingface_hub import login, HfApi \\nimport os \\n# Method 1: Login interactively \\nlogin() \\n# Method 2: Use token from environment \\nos.environ[\"HUGGINGFACE_HUB_TOKEN\"] = \"your_token_here\" \\n# Method 3: Programmatic login \\nlogin(token=\"your_token_here\") \\n# Check login status \\napi = HfApi() \\nuser = api.whoami() \\nprint(f\"Logged in as: {user[\\'name\\']}\") \\nUsing Pre-trained Models\\nQuick Start with Pipelines\\n# Sentiment Analysis \\nsentiment_pipeline = pipeline(\"sentiment-analysis\") \\nresult = sentiment_pipeline(\"I love Hugging Face!\") \\nprint(result)  # [{\\'label\\': \\'POSITIVE\\', \\'score\\': 0.9998}] \\n# Text Generation \\ngenerator = pipeline(\"text-generation\", model=\"gpt2\") \\noutput = generator(\"The future of AI is\", max_length=50, num_return_sequences=1) \\nprint(output[0][\\'generated_text\\']) \\n# Question Answering \\nqa_pipeline = pipeline(\"question-answering\") \\ncontext = \"Hugging Face is a company that democratizes AI through open-source and open science.\" \\nquestion = \"What does Hugging Face do?\" \\nanswer = qa_pipeline(question=question, context=context) \\nprint(answer) \\n# Named Entity Recognition \\nner_pipeline = pipeline(\"ner\", aggregation_strategy=\"simple\") \\ntext = \"My name is Sarah and I work at Google in California.\" \\nentities = ner_pipeline(text) \\nprint(entities) \\n# Translation \\ntranslator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-fr\") \\nfrench_text = translator(\"Hello, how are you?\") \\nprint(french_text) \\nAvailable Pipeline Tasks'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:59+00:00', 'source': '../data/pdf_files/huggingface_tutorial.pdf', 'file_path': '../data/pdf_files/huggingface_tutorial.pdf', 'total_pages': 58, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:23:00+00:00', 'trapped': '', 'modDate': 'D:20250918142300Z', 'creationDate': \"D:20250918142259+00'00'\", 'page': 3}, page_content='# Get all available tasks \\nfrom transformers import PIPELINE_REGISTRY \\nprint(\"Available tasks:\", list(PIPELINE_REGISTRY.supported_tasks.keys())) \\n# Specific pipeline examples \\npipelines_examples = { \\n    \"text-classification\": \"distilbert-base-uncased-finetuned-sst-2-english\", \\n    \"token-classification\": \"dbmdz/bert-large-cased-finetuned-conll03-english\", \\n    \"question-answering\": \"distilbert-base-cased-distilled-squad\", \\n    \"fill-mask\": \"bert-base-uncased\", \\n    \"summarization\": \"facebook/bart-large-cnn\", \\n    \"translation\": \"t5-base\", \\n    \"text-generation\": \"gpt2\", \\n    \"text2text-generation\": \"t5-small\", \\n    \"zero-shot-classification\": \"facebook/bart-large-mnli\", \\n    \"image-classification\": \"google/vit-base-patch16-224\", \\n    \"object-detection\": \"facebook/detr-resnet-50\", \\n    \"image-segmentation\": \"facebook/detr-resnet-50-panoptic\", \\n    \"automatic-speech-recognition\": \"facebook/wav2vec2-base-960h\", \\n    \"text-to-speech\": \"microsoft/speecht5_tts\" \\n} \\n# Use any pipeline \\nfor task, model_name in pipelines_examples.items(): \\n    try: \\n        pipe = pipeline(task, model=model_name) \\n        print(f\"✓ {task}: {model_name}\") \\n    except Exception as e: \\n        print(f\"✗ {task}: {e}\") \\nTransformers Library\\nLoading Models and Tokenizers\\n# Method 1: Auto classes (recommended) \\nmodel_name = \"bert-base-uncased\" \\ntokenizer = AutoTokenizer.from_pretrained(model_name) \\nmodel = AutoModel.from_pretrained(model_name) \\n# Method 2: Specific model classes \\nfrom transformers import BertTokenizer, BertModel \\ntokenizer = BertTokenizer.from_pretrained(model_name) \\nmodel = BertModel.from_pretrained(model_name) \\n# Load model for specific tasks \\nmodel = AutoModelForSequenceClassification.from_pretrained( \\n    \"cardiffnlp/twitter-roberta-base-sentiment-latest\" \\n) \\n# Load with specific configurations \\nfrom transformers import AutoConfig \\nconfig = AutoConfig.from_pretrained(model_name) \\nconfig.output_hidden_states = True \\nmodel = AutoModel.from_pretrained(model_name, config=config) \\nText Processing'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:59+00:00', 'source': '../data/pdf_files/huggingface_tutorial.pdf', 'file_path': '../data/pdf_files/huggingface_tutorial.pdf', 'total_pages': 58, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:23:00+00:00', 'trapped': '', 'modDate': 'D:20250918142300Z', 'creationDate': \"D:20250918142259+00'00'\", 'page': 4}, page_content='def process_text_with_bert(text, model_name=\"bert-base-uncased\"): \\n    tokenizer = AutoTokenizer.from_pretrained(model_name) \\n    model = AutoModel.from_pretrained(model_name) \\n     \\n    # Tokenize \\n    inputs = tokenizer( \\n        text, \\n        padding=True, \\n        truncation=True, \\n        max_length=512, \\n        return_tensors=\"pt\" \\n    )\\n     \\n    # Get model outputs \\n    with torch.no_grad(): \\n        outputs = model(**inputs) \\n     \\n    # Extract embeddings \\n    last_hidden_states = outputs.last_hidden_state \\n    pooled_output = outputs.pooler_output if hasattr(outputs, \\'pooler_output\\') else None \\n     \\n    return { \\n        \"input_ids\": inputs[\"input_ids\"], \\n        \"attention_mask\": inputs[\"attention_mask\"], \\n        \"last_hidden_states\": last_hidden_states, \\n        \"pooled_output\": pooled_output, \\n        \"tokens\": tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0]) \\n    }\\n# Usage \\ntext = \"Hugging Face transformers are amazing!\" \\nresult = process_text_with_bert(text) \\nprint(f\"Sequence length: {result[\\'last_hidden_states\\'].shape[1]}\") \\nprint(f\"Hidden size: {result[\\'last_hidden_states\\'].shape[2]}\") \\nBatch Processing'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:59+00:00', 'source': '../data/pdf_files/huggingface_tutorial.pdf', 'file_path': '../data/pdf_files/huggingface_tutorial.pdf', 'total_pages': 58, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:23:00+00:00', 'trapped': '', 'modDate': 'D:20250918142300Z', 'creationDate': \"D:20250918142259+00'00'\", 'page': 5}, page_content='def batch_encode_texts(texts, model_name=\"bert-base-uncased\", batch_size=16): \\n    tokenizer = AutoTokenizer.from_pretrained(model_name) \\n    model = AutoModel.from_pretrained(model_name) \\n     \\n    all_embeddings = [] \\n     \\n    for i in range(0, len(texts), batch_size): \\n        batch_texts = texts[i:i+batch_size] \\n         \\n        # Tokenize batch \\n        inputs = tokenizer( \\n            batch_texts, \\n            padding=True, \\n            truncation=True, \\n            max_length=512, \\n            return_tensors=\"pt\" \\n        ) \\n         \\n        # Get embeddings \\n        with torch.no_grad(): \\n            outputs = model(**inputs) \\n            # Use mean pooling for sentence embeddings \\n            embeddings = outputs.last_hidden_state.mean(dim=1) \\n            all_embeddings.append(embeddings) \\n     \\n    return torch.cat(all_embeddings, dim=0) \\n# Usage \\ntexts = [ \\n    \"I love machine learning\", \\n    \"Natural language processing is fascinating\", \\n    \"Transformers revolutionized NLP\", \\n    \"BERT is a powerful model\" \\n] \\nembeddings = batch_encode_texts(texts) \\nprint(f\"Embeddings shape: {embeddings.shape}\") \\nModel Comparison'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:59+00:00', 'source': '../data/pdf_files/huggingface_tutorial.pdf', 'file_path': '../data/pdf_files/huggingface_tutorial.pdf', 'total_pages': 58, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:23:00+00:00', 'trapped': '', 'modDate': 'D:20250918142300Z', 'creationDate': \"D:20250918142259+00'00'\", 'page': 6}, page_content='def compare_models(text, models): \\n    results = {} \\n     \\n    for model_name in models: \\n        try: \\n            # Load model and tokenizer \\n            tokenizer = AutoTokenizer.from_pretrained(model_name) \\n            model = AutoModel.from_pretrained(model_name) \\n             \\n            # Process text \\n            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512) \\n             \\n            with torch.no_grad(): \\n                outputs = model(**inputs) \\n                embedding = outputs.last_hidden_state.mean(dim=1) \\n             \\n            results[model_name] = { \\n                \"embedding_dim\": embedding.shape[1], \\n                \"vocab_size\": len(tokenizer), \\n                \"max_position\": tokenizer.model_max_length, \\n                \"embedding_norm\": torch.norm(embedding).item() \\n            } \\n        except Exception as e: \\n            results[model_name] = {\"error\": str(e)} \\n     \\n    return results \\n# Compare different models \\nmodels_to_compare = [ \\n    \"bert-base-uncased\", \\n    \"roberta-base\",  \\n    \"distilbert-base-uncased\", \\n    \"albert-base-v2\" \\n] \\ncomparison = compare_models(\"This is a test sentence.\", models_to_compare) \\nfor model, stats in comparison.items(): \\n    print(f\"{model}:\") \\n    for key, value in stats.items(): \\n        print(f\"  {key}: {value}\") \\n    print() \\nDatasets Library\\nLoading Datasets'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:59+00:00', 'source': '../data/pdf_files/huggingface_tutorial.pdf', 'file_path': '../data/pdf_files/huggingface_tutorial.pdf', 'total_pages': 58, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:23:00+00:00', 'trapped': '', 'modDate': 'D:20250918142300Z', 'creationDate': \"D:20250918142259+00'00'\", 'page': 7}, page_content='from datasets import load_dataset, Dataset, DatasetDict \\n# Load popular datasets \\nimdb = load_dataset(\"imdb\") \\nsquad = load_dataset(\"squad\") \\nglue_sst2 = load_dataset(\"glue\", \"sst2\") \\n# Load specific splits \\ntrain_data = load_dataset(\"imdb\", split=\"train\") \\ntest_data = load_dataset(\"imdb\", split=\"test[:1000]\")  # First 1000 examples \\n# Load from local files \\nlocal_dataset = load_dataset(\"csv\", data_files=\"my_data.csv\") \\njson_dataset = load_dataset(\"json\", data_files=\"my_data.jsonl\") \\nprint(f\"IMDB dataset: {imdb}\") \\nprint(f\"Features: {imdb[\\'train\\'].features}\") \\nprint(f\"Number of examples: {len(imdb[\\'train\\'])}\") \\nDataset Exploration\\ndef explore_dataset(dataset): \\n    \"\"\"Explore dataset characteristics\"\"\" \\n    print(f\"Dataset: {dataset}\") \\n    print(f\"Splits: {list(dataset.keys())}\") \\n     \\n    for split_name, split_data in dataset.items(): \\n        print(f\"\\\\n{split_name.upper()} SPLIT:\") \\n        print(f\"  Size: {len(split_data)}\") \\n        print(f\"  Features: {split_data.features}\") \\n         \\n        # Show first few examples \\n        print(f\"  First example: {split_data[0]}\") \\n         \\n        # Show data types and statistics \\n        for feature_name, feature_type in split_data.features.items(): \\n            print(f\"  {feature_name}: {feature_type}\") \\n# Explore IMDB dataset \\nexplore_dataset(imdb) \\nData Preprocessing'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:59+00:00', 'source': '../data/pdf_files/huggingface_tutorial.pdf', 'file_path': '../data/pdf_files/huggingface_tutorial.pdf', 'total_pages': 58, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:23:00+00:00', 'trapped': '', 'modDate': 'D:20250918142300Z', 'creationDate': \"D:20250918142259+00'00'\", 'page': 8}, page_content='def preprocess_imdb_data(examples, tokenizer, max_length=512): \\n    \"\"\"Preprocess IMDB dataset for BERT\"\"\" \\n    return tokenizer( \\n        examples[\"text\"], \\n        truncation=True, \\n        padding=\"max_length\", \\n        max_length=max_length, \\n        return_tensors=\"pt\" \\n    )\\n# Load tokenizer \\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\") \\n# Preprocess dataset \\ndef tokenize_function(examples): \\n    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=256) \\n# Apply preprocessing \\ntokenized_imdb = imdb.map(tokenize_function, batched=True) \\n# Remove original text column and rename label \\ntokenized_imdb = tokenized_imdb.remove_columns([\"text\"]) \\ntokenized_imdb = tokenized_imdb.rename_column(\"label\", \"labels\") \\n# Set format for PyTorch \\ntokenized_imdb.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"]) \\nprint(\"Preprocessed dataset:\") \\nprint(tokenized_imdb[\"train\"][0]) \\nCustom Dataset Creation'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:59+00:00', 'source': '../data/pdf_files/huggingface_tutorial.pdf', 'file_path': '../data/pdf_files/huggingface_tutorial.pdf', 'total_pages': 58, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:23:00+00:00', 'trapped': '', 'modDate': 'D:20250918142300Z', 'creationDate': \"D:20250918142259+00'00'\", 'page': 9}, page_content='def create_custom_dataset(): \\n    \"\"\"Create a custom dataset\"\"\" \\n    # Sample data \\n    data = { \\n        \"text\": [ \\n            \"I love this movie!\", \\n            \"This film is terrible.\", \\n            \"Great acting and storyline.\", \\n            \"Boring and predictable.\", \\n            \"Amazing cinematography!\" \\n        ],\\n        \"label\": [1, 0, 1, 0, 1]  # 1=positive, 0=negative \\n    }\\n     \\n    # Create dataset \\n    dataset = Dataset.from_dict(data) \\n     \\n    # Split into train/test \\n    train_test = dataset.train_test_split(test_size=0.2) \\n     \\n    return train_test \\n# Create and use custom dataset \\ncustom_data = create_custom_dataset() \\nprint(custom_data) \\n# Add preprocessing \\ndef preprocess_custom(examples): \\n    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\") \\n    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=128) \\ncustom_data = custom_data.map(preprocess_custom, batched=True) \\ncustom_data.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"]) \\nData Augmentation'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:59+00:00', 'source': '../data/pdf_files/huggingface_tutorial.pdf', 'file_path': '../data/pdf_files/huggingface_tutorial.pdf', 'total_pages': 58, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:23:00+00:00', 'trapped': '', 'modDate': 'D:20250918142300Z', 'creationDate': \"D:20250918142259+00'00'\", 'page': 10}, page_content='def augment_text_data(dataset, augmentation_factor=2): \\n    \"\"\"Simple data augmentation for text\"\"\" \\n    import random \\n     \\n    def augment_example(example): \\n        text = example[\"text\"] \\n         \\n        # Simple augmentations \\n        augmented_texts = [text]  # Original \\n         \\n        # Synonym replacement (simplified) \\n        synonyms = { \\n            \"good\": [\"great\", \"excellent\", \"wonderful\"], \\n            \"bad\": [\"terrible\", \"awful\", \"horrible\"], \\n            \"nice\": [\"pleasant\", \"lovely\", \"delightful\"] \\n        } \\n         \\n        for word, syns in synonyms.items(): \\n            if word in text.lower(): \\n                for syn in syns: \\n                    augmented_texts.append(text.lower().replace(word, syn)) \\n         \\n        # Return multiple examples \\n        return { \\n            \"text\": augmented_texts[:augmentation_factor], \\n            \"label\": [example[\"label\"]] * min(len(augmented_texts), augmentation_factor) \\n        } \\n     \\n    # Apply augmentation \\n    augmented = dataset.map(augment_example, remove_columns=dataset.column_names, batched=False) \\n     \\n    return augmented \\nTokenizers\\nUnderstanding Tokenization'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:59+00:00', 'source': '../data/pdf_files/huggingface_tutorial.pdf', 'file_path': '../data/pdf_files/huggingface_tutorial.pdf', 'total_pages': 58, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:23:00+00:00', 'trapped': '', 'modDate': 'D:20250918142300Z', 'creationDate': \"D:20250918142259+00'00'\", 'page': 11}, page_content='from transformers import AutoTokenizer \\ndef analyze_tokenization(text, model_names): \\n    \"\"\"Analyze how different tokenizers handle the same text\"\"\" \\n    print(f\"Original text: \\'{text}\\'\\\\n\") \\n     \\n    for model_name in model_names: \\n        tokenizer = AutoTokenizer.from_pretrained(model_name) \\n         \\n        # Tokenize \\n        tokens = tokenizer.tokenize(text) \\n        token_ids = tokenizer.encode(text) \\n        decoded = tokenizer.decode(token_ids) \\n         \\n        print(f\"Model: {model_name}\") \\n        print(f\"  Tokens: {tokens}\") \\n        print(f\"  Token IDs: {token_ids}\") \\n        print(f\"  Decoded: \\'{decoded}\\'\") \\n        print(f\"  Vocab size: {tokenizer.vocab_size}\") \\n        print(f\"  Number of tokens: {len(tokens)}\") \\n        print() \\n# Compare different tokenizers \\ntext = \"Hello, I\\'m using Hugging Face transformers!\" \\nmodels = [\\n    \"bert-base-uncased\", \\n    \"gpt2\", \\n    \"roberta-base\", \\n    \"t5-base\" \\n] \\nanalyze_tokenization(text, models) \\nCustom Tokenization'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:59+00:00', 'source': '../data/pdf_files/huggingface_tutorial.pdf', 'file_path': '../data/pdf_files/huggingface_tutorial.pdf', 'total_pages': 58, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:23:00+00:00', 'trapped': '', 'modDate': 'D:20250918142300Z', 'creationDate': \"D:20250918142259+00'00'\", 'page': 12}, page_content='def custom_tokenization_pipeline(texts, model_name=\"bert-base-uncased\"): \\n    \"\"\"Custom tokenization with special handling\"\"\" \\n    tokenizer = AutoTokenizer.from_pretrained(model_name) \\n     \\n    results = [] \\n    for text in texts: \\n        # Basic tokenization \\n        basic_tokens = tokenizer(text) \\n         \\n        # Add special tokens handling \\n        encoded = tokenizer( \\n            text, \\n            add_special_tokens=True, \\n            padding=\"max_length\", \\n            truncation=True, \\n            max_length=128, \\n            return_tensors=\"pt\", \\n            return_attention_mask=True, \\n            return_token_type_ids=True if \"bert\" in model_name else False \\n        ) \\n         \\n        # Token analysis \\n        tokens = tokenizer.convert_ids_to_tokens(encoded[\"input_ids\"][0]) \\n         \\n        results.append({ \\n            \"original_text\": text, \\n            \"tokens\": tokens, \\n            \"input_ids\": encoded[\"input_ids\"], \\n            \"attention_mask\": encoded[\"attention_mask\"], \\n            \"special_tokens\": { \\n                \"CLS\": tokenizer.cls_token, \\n                \"SEP\": tokenizer.sep_token, \\n                \"PAD\": tokenizer.pad_token, \\n                \"UNK\": tokenizer.unk_token \\n            } \\n        })\\n     \\n    return results \\n# Usage \\ntexts = [ \\n    \"Short text\", \\n    \"This is a much longer text that might need truncation depending on the model\\'s maximum sequence length\", \\n    \"Text with special characters: @#$%!\" \\n] \\ntokenization_results = custom_tokenization_pipeline(texts) \\nfor i, result in enumerate(tokenization_results): \\n    print(f\"Example {i+1}:\") \\n    print(f\"  Original: {result[\\'original_text\\']}\") \\n    print(f\"  Tokens: {result[\\'tokens\\'][:10]}...\")  # First 10 tokens \\n    print(f\"  Length: {len(result[\\'tokens\\'])}\") \\n    print() \\nFast Tokenizers'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:59+00:00', 'source': '../data/pdf_files/huggingface_tutorial.pdf', 'file_path': '../data/pdf_files/huggingface_tutorial.pdf', 'total_pages': 58, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:23:00+00:00', 'trapped': '', 'modDate': 'D:20250918142300Z', 'creationDate': \"D:20250918142259+00'00'\", 'page': 13}, page_content='from transformers import AutoTokenizer \\nimport time \\ndef compare_tokenizer_speed(texts, model_name=\"bert-base-uncased\"): \\n    \"\"\"Compare fast vs slow tokenizer performance\"\"\" \\n     \\n    # Load both versions \\n    fast_tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True) \\n    slow_tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False) \\n     \\n    # Benchmark fast tokenizer \\n    start_time = time.time() \\n    fast_results = fast_tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\") \\n    fast_time = time.time() - start_time \\n     \\n    # Benchmark slow tokenizer   \\n    start_time = time.time() \\n    slow_results = slow_tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\") \\n    slow_time = time.time() - start_time \\n     \\n    print(f\"Fast tokenizer: {fast_time:.4f} seconds\") \\n    print(f\"Slow tokenizer: {slow_time:.4f} seconds\") \\n    print(f\"Speedup: {slow_time/fast_time:.2f}x\") \\n     \\n    return fast_results, slow_results \\n# Test with many texts \\nlarge_texts = [\"This is a test sentence.\"] * 1000 \\nfast_result, slow_result = compare_tokenizer_speed(large_texts) \\nFine-tuning Models\\nBasic Fine-tuning Setup'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:59+00:00', 'source': '../data/pdf_files/huggingface_tutorial.pdf', 'file_path': '../data/pdf_files/huggingface_tutorial.pdf', 'total_pages': 58, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:23:00+00:00', 'trapped': '', 'modDate': 'D:20250918142300Z', 'creationDate': \"D:20250918142259+00'00'\", 'page': 14}, page_content='from transformers import TrainingArguments, Trainer \\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support \\ndef setup_fine_tuning(model_name, num_labels): \\n    \"\"\"Setup model and tokenizer for fine-tuning\"\"\" \\n    tokenizer = AutoTokenizer.from_pretrained(model_name) \\n    model = AutoModelForSequenceClassification.from_pretrained( \\n        model_name, \\n        num_labels=num_labels \\n    )\\n     \\n    # Add padding token if needed \\n    if tokenizer.pad_token is None: \\n        tokenizer.pad_token = tokenizer.eos_token \\n        model.config.pad_token_id = tokenizer.eos_token_id \\n     \\n    return model, tokenizer \\n# Define compute metrics function \\ndef compute_metrics(eval_pred): \\n    predictions, labels = eval_pred \\n    predictions = predictions.argmax(axis=-1) \\n     \\n    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average=\\'weighted\\') \\n    accuracy = accuracy_score(labels, predictions) \\n     \\n    return { \\n        \\'accuracy\\': accuracy, \\n        \\'f1\\': f1, \\n        \\'precision\\': precision, \\n        \\'recall\\': recall \\n    }\\nText Classification Fine-tuning'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:59+00:00', 'source': '../data/pdf_files/huggingface_tutorial.pdf', 'file_path': '../data/pdf_files/huggingface_tutorial.pdf', 'total_pages': 58, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:23:00+00:00', 'trapped': '', 'modDate': 'D:20250918142300Z', 'creationDate': \"D:20250918142259+00'00'\", 'page': 15}, page_content='def fine_tune_text_classifier(): \\n    \"\"\"Fine-tune BERT for text classification\"\"\" \\n     \\n    # Load and preprocess data \\n    dataset = load_dataset(\"imdb\") \\n    model, tokenizer = setup_fine_tuning(\"bert-base-uncased\", num_labels=2) \\n     \\n    def preprocess_function(examples): \\n        return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=256) \\n     \\n    # Preprocess datasets \\n    encoded_dataset = dataset.map(preprocess_function, batched=True) \\n    encoded_dataset = encoded_dataset.remove_columns([\"text\"]) \\n    encoded_dataset = encoded_dataset.rename_column(\"label\", \"labels\") \\n    encoded_dataset.set_format(\"torch\") \\n     \\n    # Use smaller subset for demo \\n    train_dataset = encoded_dataset[\"train\"].shuffle().select(range(1000)) \\n    eval_dataset = encoded_dataset[\"test\"].shuffle().select(range(200)) \\n     \\n    # Training arguments \\n    training_args = TrainingArguments( \\n        output_dir=\"./results\", \\n        num_train_epochs=3, \\n        per_device_train_batch_size=16, \\n        per_device_eval_batch_size=16, \\n        warmup_steps=500, \\n        weight_decay=0.01, \\n        logging_dir=\"./logs\", \\n        logging_steps=10, \\n        evaluation_strategy=\"epoch\", \\n        save_strategy=\"epoch\", \\n        load_best_model_at_end=True, \\n        metric_for_best_model=\"accuracy\", \\n        greater_is_better=True, \\n    )\\n     \\n    # Initialize trainer \\n    trainer = Trainer( \\n        model=model, \\n        args=training_args, \\n        train_dataset=train_dataset, \\n        eval_dataset=eval_dataset, \\n        tokenizer=tokenizer, \\n        compute_metrics=compute_metrics, \\n    )\\n     \\n    # Train \\n    trainer.train() \\n     \\n    # Evaluate \\n    eval_results = trainer.evaluate() \\n    print(f\"Evaluation results: {eval_results}\") \\n     \\n    return model, tokenizer, trainer \\n# Run fine-tuning (commented out for demo) \\n# model, tokenizer, trainer = fine_tune_text_classifier()'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:59+00:00', 'source': '../data/pdf_files/huggingface_tutorial.pdf', 'file_path': '../data/pdf_files/huggingface_tutorial.pdf', 'total_pages': 58, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:23:00+00:00', 'trapped': '', 'modDate': 'D:20250918142300Z', 'creationDate': \"D:20250918142259+00'00'\", 'page': 16}, page_content='Parameter-Efficient Fine-tuning (LoRA)\\nfrom peft import get_peft_model, LoraConfig, TaskType \\ndef setup_lora_fine_tuning(model_name, num_labels): \\n    \"\"\"Setup LoRA fine-tuning for efficient training\"\"\" \\n     \\n    # Load base model \\n    model = AutoModelForSequenceClassification.from_pretrained( \\n        model_name, \\n        num_labels=num_labels \\n    )\\n     \\n    # LoRA configuration \\n    peft_config = LoraConfig( \\n        task_type=TaskType.SEQ_CLS,  # Sequence classification \\n        inference_mode=False, \\n        r=8,                        # Rank \\n        lora_alpha=32,              # LoRA scaling parameter \\n        lora_dropout=0.1,           # LoRA dropout \\n        target_modules=[\"query\", \"value\"],  # Target attention modules \\n    )\\n     \\n    # Apply LoRA \\n    model = get_peft_model(model, peft_config) \\n    model.print_trainable_parameters() \\n     \\n    return model \\n# Usage \\nlora_model = setup_lora_fine_tuning(\"bert-base-uncased\", num_labels=2) \\nCustom Training Loop'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:59+00:00', 'source': '../data/pdf_files/huggingface_tutorial.pdf', 'file_path': '../data/pdf_files/huggingface_tutorial.pdf', 'total_pages': 58, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:23:00+00:00', 'trapped': '', 'modDate': 'D:20250918142300Z', 'creationDate': \"D:20250918142259+00'00'\", 'page': 17}, page_content='def custom_training_loop(model, tokenizer, train_dataloader, eval_dataloader, num_epochs=3): \\n    \"\"\"Custom training loop with more control\"\"\" \\n    from torch.optim import AdamW \\n    from transformers import get_linear_schedule_with_warmup \\n     \\n    # Optimizer and scheduler \\n    optimizer = AdamW(model.parameters(), lr=2e-5) \\n    total_steps = len(train_dataloader) * num_epochs \\n    scheduler = get_linear_schedule_with_warmup( \\n        optimizer, \\n        num_warmup_steps=0, \\n        num_training_steps=total_steps \\n    )\\n     \\n    # Training loop \\n    model.train() \\n    for epoch in range(num_epochs): \\n        total_loss = 0 \\n         \\n        for batch_idx, batch in enumerate(train_dataloader): \\n            # Forward pass \\n            outputs = model( \\n                input_ids=batch[\"input_ids\"], \\n                attention_mask=batch[\"attention_mask\"], \\n                labels=batch[\"labels\"] \\n            ) \\n             \\n            loss = outputs.loss \\n            total_loss += loss.item() \\n             \\n            # Backward pass \\n            loss.backward() \\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) \\n             \\n            optimizer.step() \\n            scheduler.step() \\n            optimizer.zero_grad() \\n             \\n            if batch_idx % 50 == 0: \\n                print(f\"Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item():.4f}\") \\n         \\n        avg_loss = total_loss / len(train_dataloader) \\n        print(f\"Epoch {epoch} completed. Average loss: {avg_loss:.4f}\") \\n         \\n        # Evaluation \\n        model.eval() \\n        eval_accuracy = evaluate_model(model, eval_dataloader) \\n        print(f\"Evaluation accuracy: {eval_accuracy:.4f}\") \\n        model.train() \\ndef evaluate_model(model, dataloader): \\n    \"\"\"Evaluate model accuracy\"\"\" \\n    correct = 0 \\n    total = 0 \\n     \\n    with torch.no_grad(): \\n        for batch in dataloader: \\n            outputs = model( \\n                input_ids=batch[\"input_ids\"], \\n                attention_mask=batch[\"attention_mask\"]'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:59+00:00', 'source': '../data/pdf_files/huggingface_tutorial.pdf', 'file_path': '../data/pdf_files/huggingface_tutorial.pdf', 'total_pages': 58, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:23:00+00:00', 'trapped': '', 'modDate': 'D:20250918142300Z', 'creationDate': \"D:20250918142259+00'00'\", 'page': 18}, page_content=') \\n             \\n            predictions = torch.argmax(outputs.logits, dim=-1) \\n            correct += (predictions == batch[\"labels\"]).sum().item() \\n            total += batch[\"labels\"].size(0) \\n     \\n    return correct / total \\nModel Hub and Sharing\\nExploring the Hub\\nfrom huggingface_hub import HfApi, list_models, model_info \\napi = HfApi() \\n# List models with filters \\nmodels = list_models( \\n    task=\"text-classification\", \\n    library=\"transformers\", \\n    language=\"en\", \\n    sort=\"downloads\", \\n    limit=10 \\n) \\nprint(\"Top 10 English text classification models:\") \\nfor model in models: \\n    print(f\"- {model.id} ({model.downloads} downloads)\") \\n# Get detailed model information \\nmodel_details = model_info(\"bert-base-uncased\") \\nprint(f\"\\\\nModel: {model_details.id}\") \\nprint(f\"Library: {model_details.library_name}\") \\nprint(f\"Downloads: {model_details.downloads}\") \\nprint(f\"Tags: {model_details.tags}\") \\nUploading Models'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:59+00:00', 'source': '../data/pdf_files/huggingface_tutorial.pdf', 'file_path': '../data/pdf_files/huggingface_tutorial.pdf', 'total_pages': 58, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:23:00+00:00', 'trapped': '', 'modDate': 'D:20250918142300Z', 'creationDate': \"D:20250918142259+00'00'\", 'page': 19}, page_content='from huggingface_hub import Repository, upload_folder \\ndef upload_model_to_hub(model, tokenizer, repo_name, commit_message=\"Upload model\"): \\n    \"\"\"Upload trained model to Hugging Face Hub\"\"\" \\n     \\n    # Save model locally first \\n    model.save_pretrained(f\"./{repo_name}\") \\n    tokenizer.save_pretrained(f\"./{repo_name}\") \\n     \\n    # Create model card \\n    model_card_content = f\"\"\" \\n--- \\nlanguage: en \\nlicense: apache-2.0 \\ntags:\\n- text-classification \\n- sentiment-analysis \\ndatasets: \\n- imdb \\nmetrics: \\n- accuracy\\n--- \\n# {repo_name} \\nThis model is a fine-tuned version of BERT for sentiment analysis on the IMDB dataset. \\n## Usage \\n```python \\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification \\ntokenizer = AutoTokenizer.from_pretrained(\"your-username/{repo_name}\") \\nmodel = AutoModelForSequenceClassification.from_pretrained(\"your-username/{repo_name}\") \\n# Use the model \\ninputs = tokenizer(\"I love this movie!\", return_tensors=\"pt\") \\noutputs = model(**inputs) \\npredictions = torch.nn.functional.softmax(outputs.logits, dim=-1) \\nTraining Data\\nThe model was trained on the IMDB movie reviews dataset.\\nTraining Procedure\\nLearning rate: 2e-5\\nBatch size: 16\\nNumber of epochs: 3\\nEvaluation Results\\nAccuracy: 92.5%\\nF1-score: 0.925 \"\"\"\\nwith open(f\".//README.md\", \"w\") as f: f.write(model_card_content)'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:59+00:00', 'source': '../data/pdf_files/huggingface_tutorial.pdf', 'file_path': '../data/pdf_files/huggingface_tutorial.pdf', 'total_pages': 58, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:23:00+00:00', 'trapped': '', 'modDate': 'D:20250918142300Z', 'creationDate': \"D:20250918142259+00'00'\", 'page': 20}, page_content='Upload to hub\\nupload_folder( folder_path=f\"./\", repo_id=f\"your-username/\", repo_type=\"model\", commit_message=commit_message )\\nUsage (example)\\nupload_model_to_hub(fine_tuned_model,\\ntokenizer, \"my-sentiment-model\")\\n### Model Versioning and Management \\n```python \\ndef manage_model_versions(repo_id): \\n    \"\"\"Manage different versions of a model\"\"\" \\n    api = HfApi() \\n     \\n    # List all commits (versions) \\n    commits = api.list_repo_commits(repo_id) \\n    print(f\"Model versions for {repo_id}:\") \\n    for commit in commits[:5]:  # Show last 5 versions \\n        print(f\"- {commit.commit_id[:8]}: {commit.title}\") \\n     \\n    # Load specific version \\n    specific_version_model = AutoModel.from_pretrained( \\n        repo_id, \\n        revision=commits[1].commit_id  # Load second-to-last version \\n    )\\n     \\n    return specific_version_model \\n# Usage \\n# model_versions = manage_model_versions(\"bert-base-uncased\") \\nInference Endpoints\\nLocal Inference Optimization'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:59+00:00', 'source': '../data/pdf_files/huggingface_tutorial.pdf', 'file_path': '../data/pdf_files/huggingface_tutorial.pdf', 'total_pages': 58, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:23:00+00:00', 'trapped': '', 'modDate': 'D:20250918142300Z', 'creationDate': \"D:20250918142259+00'00'\", 'page': 21}, page_content='import torch \\nfrom transformers import pipeline \\ndef optimize_for_inference(model_name): \\n    \"\"\"Optimize model for faster inference\"\"\" \\n     \\n    # Load with optimizations \\n    classifier = pipeline( \\n        \"text-classification\", \\n        model=model_name, \\n        torch_dtype=torch.float16,  # Use half precision \\n        device_map=\"auto\"           # Automatic device mapping \\n    )\\n     \\n    # Batch processing function \\n    def batch_predict(texts, batch_size=32): \\n        results = [] \\n        for i in range(0, len(texts), batch_size): \\n            batch = texts[i:i+batch_size] \\n            batch_results = classifier(batch) \\n            results.extend(batch_results) \\n        return results \\n     \\n    return classifier, batch_predict \\n# Usage \\nclassifier, batch_predict = optimize_for_inference(\"cardiffnlp/twitter-roberta-base-sentiment-latest\") \\n# Test batch prediction \\ntexts = [\"I love this!\", \"This is terrible\", \"Not bad\"] * 100 \\nresults = batch_predict(texts) \\nprint(f\"Processed {len(results)} texts\") \\nQuantization for Edge Deployment'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:59+00:00', 'source': '../data/pdf_files/huggingface_tutorial.pdf', 'file_path': '../data/pdf_files/huggingface_tutorial.pdf', 'total_pages': 58, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:23:00+00:00', 'trapped': '', 'modDate': 'D:20250918142300Z', 'creationDate': \"D:20250918142259+00'00'\", 'page': 22}, page_content='from transformers import AutoModelForSequenceClassification, BitsAndBytesConfig \\ndef quantize_model(model_name, quantization_type=\"8bit\"): \\n    \"\"\"Quantize model for reduced memory usage\"\"\" \\n     \\n    if quantization_type == \"8bit\": \\n        quantization_config = BitsAndBytesConfig(load_in_8bit=True) \\n    elif quantization_type == \"4bit\": \\n        quantization_config = BitsAndBytesConfig( \\n            load_in_4bit=True, \\n            bnb_4bit_compute_dtype=torch.float16, \\n            bnb_4bit_quant_type=\"nf4\", \\n            bnb_4bit_use_double_quant=True \\n        ) \\n    else: \\n        quantization_config = None \\n     \\n    model = AutoModelForSequenceClassification.from_pretrained( \\n        model_name, \\n        quantization_config=quantization_config, \\n        device_map=\"auto\" \\n    )\\n     \\n    return model \\n# Usage \\nquantized_model = quantize_model(\"bert-base-uncased\", \"8bit\") \\nprint(f\"Model memory footprint reduced with 8-bit quantization\") \\nONNX Export for Production\\ndef export_to_onnx(model_name, output_path=\"model.onnx\"): \\n    \"\"\"Export model to ONNX format for production deployment\"\"\" \\n    from transformers.onnx import export \\n    from transformers import AutoTokenizer, AutoModel \\n    from pathlib import Path \\n     \\n    # Load model and tokenizer \\n    tokenizer = AutoTokenizer.from_pretrained(model_name) \\n    model = AutoModel.from_pretrained(model_name) \\n     \\n    # Export to ONNX \\n    onnx_path = Path(output_path) \\n    export( \\n        preprocessor=tokenizer, \\n        model=model, \\n        config=model.config, \\n        opset=14, \\n        output=onnx_path \\n    )\\n     \\n    return onnx_path \\n# Usage \\n# onnx_path = export_to_onnx(\"distilbert-base-uncased\") \\n# print(f\"Model exported to {onnx_path}\")'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:59+00:00', 'source': '../data/pdf_files/huggingface_tutorial.pdf', 'file_path': '../data/pdf_files/huggingface_tutorial.pdf', 'total_pages': 58, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:23:00+00:00', 'trapped': '', 'modDate': 'D:20250918142300Z', 'creationDate': \"D:20250918142259+00'00'\", 'page': 23}, page_content='Best Practices\\nMemory Management\\nimport gc \\nimport torch \\ndef optimize_memory_usage(): \\n    \"\"\"Best practices for memory management\"\"\" \\n     \\n    # Clear cache \\n    torch.cuda.empty_cache() \\n    gc.collect() \\n     \\n    # Use gradient checkpointing for large models \\n    model.gradient_checkpointing_enable() \\n     \\n    # Use mixed precision training \\n    from torch.cuda.amp import autocast, GradScaler \\n     \\n    scaler = GradScaler() \\n     \\n    # Training with mixed precision \\n    with autocast(): \\n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels) \\n        loss = outputs.loss \\n     \\n    scaler.scale(loss).backward() \\n    scaler.step(optimizer) \\n    scaler.update() \\ndef monitor_gpu_memory(): \\n    \"\"\"Monitor GPU memory usage\"\"\" \\n    if torch.cuda.is_available(): \\n        print(f\"GPU memory allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\") \\n        print(f\"GPU memory cached: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\") \\n        print(f\"GPU memory free: {torch.cuda.mem_get_info()[0] / 1024**3:.2f} GB\") \\nError Handling and Robustness'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:59+00:00', 'source': '../data/pdf_files/huggingface_tutorial.pdf', 'file_path': '../data/pdf_files/huggingface_tutorial.pdf', 'total_pages': 58, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:23:00+00:00', 'trapped': '', 'modDate': 'D:20250918142300Z', 'creationDate': \"D:20250918142259+00'00'\", 'page': 24}, page_content='def robust_model_loading(model_name, fallback_model=\"distilbert-base-uncased\"): \\n    \"\"\"Robust model loading with fallback\"\"\" \\n    try: \\n        tokenizer = AutoTokenizer.from_pretrained(model_name) \\n        model = AutoModel.from_pretrained(model_name) \\n        print(f\"Successfully loaded {model_name}\") \\n        return model, tokenizer \\n     \\n    except Exception as e: \\n        print(f\"Failed to load {model_name}: {e}\") \\n        print(f\"Falling back to {fallback_model}\") \\n         \\n        try: \\n            tokenizer = AutoTokenizer.from_pretrained(fallback_model) \\n            model = AutoModel.from_pretrained(fallback_model) \\n            return model, tokenizer \\n        except Exception as e: \\n            print(f\"Failed to load fallback model: {e}\") \\n            raise \\ndef safe_inference(model, tokenizer, text, max_retries=3): \\n    \"\"\"Safe inference with retry logic\"\"\" \\n    for attempt in range(max_retries): \\n        try: \\n            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True) \\n             \\n            with torch.no_grad(): \\n                outputs = model(**inputs) \\n             \\n            return outputs \\n             \\n        except RuntimeError as e: \\n            if \"out of memory\" in str(e).lower(): \\n                print(f\"OOM error, attempt {attempt + 1}/{max_retries}\") \\n                torch.cuda.empty_cache() \\n                # Reduce batch size or sequence length \\n                if attempt < max_retries - 1: \\n                    continue \\n            raise \\n        except Exception as e: \\n            print(f\"Inference error: {e}\") \\n            if attempt == max_retries - 1: \\n                raise \\nPerformance Monitoring'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:59+00:00', 'source': '../data/pdf_files/huggingface_tutorial.pdf', 'file_path': '../data/pdf_files/huggingface_tutorial.pdf', 'total_pages': 58, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:23:00+00:00', 'trapped': '', 'modDate': 'D:20250918142300Z', 'creationDate': \"D:20250918142259+00'00'\", 'page': 25}, page_content='import time \\nfrom functools import wraps \\ndef benchmark_function(func): \\n    \"\"\"Decorator to benchmark function execution time\"\"\" \\n    @wraps(func) \\n    def wrapper(*args, **kwargs): \\n        start_time = time.time() \\n        result = func(*args, **kwargs) \\n        end_time = time.time() \\n        print(f\"{func.__name__} executed in {end_time - start_time:.4f} seconds\") \\n        return result \\n    return wrapper \\n@benchmark_function \\ndef benchmark_model_inference(model, tokenizer, texts): \\n    \"\"\"Benchmark model inference speed\"\"\" \\n    all_results = [] \\n     \\n    for text in texts: \\n        inputs = tokenizer(text, return_tensors=\"pt\") \\n        with torch.no_grad(): \\n            outputs = model(**inputs) \\n        all_results.append(outputs) \\n     \\n    return all_results \\n# Usage \\ntexts = [\"Sample text for benchmarking\"] * 100 \\n# results = benchmark_model_inference(model, tokenizer, texts) \\nModel Selection Guide'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:59+00:00', 'source': '../data/pdf_files/huggingface_tutorial.pdf', 'file_path': '../data/pdf_files/huggingface_tutorial.pdf', 'total_pages': 58, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:23:00+00:00', 'trapped': '', 'modDate': 'D:20250918142300Z', 'creationDate': \"D:20250918142259+00'00'\", 'page': 26}, page_content='def recommend_model(task, performance_priority=\"balanced\"): \\n    \"\"\"Recommend model based on task and performance requirements\"\"\" \\n     \\n    recommendations = { \\n        \"text-classification\": { \\n            \"speed\": \"distilbert-base-uncased\", \\n            \"balanced\": \"bert-base-uncased\",  \\n            \"accuracy\": \"roberta-large\" \\n        },\\n        \"question-answering\": { \\n            \"speed\": \"distilbert-base-cased-distilled-squad\", \\n            \"balanced\": \"bert-base-cased\", \\n            \"accuracy\": \"roberta-large-squad2\" \\n        },\\n        \"text-generation\": { \\n            \"speed\": \"gpt2\", \\n            \"balanced\": \"gpt2-medium\", \\n            \"accuracy\": \"gpt2-large\" \\n        },\\n        \"summarization\": { \\n            \"speed\": \"facebook/bart-base\", \\n            \"balanced\": \"facebook/bart-large-cnn\", \\n            \"accuracy\": \"google/pegasus-large\" \\n        } \\n    }\\n     \\n    if task in recommendations: \\n        return recommendations[task].get(performance_priority, recommendations[task][\"balanced\"]) \\n    else: \\n        return \"bert-base-uncased\"  # Default fallback \\n# Usage \\nmodel_name = recommend_model(\"text-classification\", \"speed\") \\nprint(f\"Recommended model: {model_name}\") \\nConclusion\\nHugging Face provides a comprehensive ecosystem for working with transformer models. This tutorial covered:\\nUsing pre-trained models with pipelines\\nUnderstanding the Transformers library architecture\\nWorking with datasets and tokenizers\\nFine-tuning models for custom tasks\\nSharing models on the Hub\\nOptimizing models for production\\nKey Takeaways:\\n1. Start with pipelines for quick prototyping\\n2. Use Auto classes for flexibility\\n3. Preprocess data carefully for best results\\n4. Consider parameter-efficient fine-tuning (LoRA/QLoRA)\\n5. Optimize models for production deployment\\n6. Monitor performance and memory usage\\nNext Steps:\\n1. Explore domain-specific models on the Hub\\n2. Experiment with multimodal models (vision + language)'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:59+00:00', 'source': '../data/pdf_files/huggingface_tutorial.pdf', 'file_path': '../data/pdf_files/huggingface_tutorial.pdf', 'total_pages': 58, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:23:00+00:00', 'trapped': '', 'modDate': 'D:20250918142300Z', 'creationDate': \"D:20250918142259+00'00'\", 'page': 27}, page_content='3. Try advanced fine-tuning techniques\\n4. Build end-to-end applications with Gradio/Streamlit\\n5. Contribute models back to the community\\nAdditional Resources:\\nHugging Face Documentation (https://huggingface.co/docs)\\nTransformers Course (https://huggingface.co/course)\\nCommunity Forums (https://discuss.huggingface.co)\\nModel Hub (https://huggingface.co/models)\\nLangGraph Tutorial: Building Complex\\nAgent Workflows\\nTable of Contents\\n1. Introduction to LangGraph\\n2. Core Concepts\\n3. Installation and Setup\\n4. Basic Graph Construction\\n5. Advanced Patterns\\n6. Real-World Examples\\n7. Best Practices\\nIntroduction to LangGraph\\nLangGraph is a library for building stateful, multi-actor applications with LLMs, built on top of LangChain. It extends the LangChain Expression Language with the\\nability to coordinate multiple chains (or actors) across multiple steps of computation in a cyclic manner.\\nKey Features:\\nStateful: Maintains state across multiple turns of conversation\\nMulti-actor: Coordinate multiple LLM chains or agents\\nCyclic: Support for loops and conditional branching\\nHuman-in-the-loop: Easy integration of human feedback\\nStreaming: Real-time streaming of intermediate results\\nCore Concepts\\n1. Nodes\\nNodes represent individual processing units in your graph. Each node is a function that takes the current state and returns an updated state.\\ndef my_node(state): \\n    # Process the state \\n    return {\"messages\": state[\"messages\"] + [new_message]} \\n2. Edges\\nEdges define the flow between nodes. LangGraph supports:\\nNormal edges: Direct connections between nodes\\nConditional edges: Branching based on state evaluation\\nStart/End edges: Entry and exit points'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:59+00:00', 'source': '../data/pdf_files/huggingface_tutorial.pdf', 'file_path': '../data/pdf_files/huggingface_tutorial.pdf', 'total_pages': 58, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:23:00+00:00', 'trapped': '', 'modDate': 'D:20250918142300Z', 'creationDate': \"D:20250918142259+00'00'\", 'page': 28}, page_content='3. State\\nState is the shared data structure that flows through your graph. It\\'s typically a dictionary that gets updated by each node.\\n4. Checkpoints\\nCheckpoints allow you to save and restore the state at any point in the execution.\\nInstallation and Setup\\n# Install LangGraph \\npip install langgraph \\n# For development \\npip install langgraph[dev] \\n# With additional integrations \\npip install \"langgraph[anthropic,openai]\" \\nEnvironment Setup\\nimport os \\nfrom langgraph.graph import StateGraph, END \\nfrom langgraph.prebuilt import ToolExecutor \\nfrom langchain_openai import ChatOpenAI \\nfrom langchain_core.messages import HumanMessage, AIMessage \\n# Set your API keys \\nos.environ[\"OPENAI_API_KEY\"] = \"your-api-key-here\" \\nBasic Graph Construction\\nSimple Linear Flow'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:59+00:00', 'source': '../data/pdf_files/huggingface_tutorial.pdf', 'file_path': '../data/pdf_files/huggingface_tutorial.pdf', 'total_pages': 58, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:23:00+00:00', 'trapped': '', 'modDate': 'D:20250918142300Z', 'creationDate': \"D:20250918142259+00'00'\", 'page': 29}, page_content='from langgraph.graph import StateGraph, END \\nfrom typing import TypedDict, List \\nfrom langchain_core.messages import BaseMessage \\nclass GraphState(TypedDict): \\n    messages: List[BaseMessage] \\ndef chatbot(state: GraphState): \\n    messages = state[\"messages\"] \\n    llm = ChatOpenAI() \\n    response = llm.invoke(messages) \\n    return {\"messages\": [response]} \\ndef create_simple_graph(): \\n    workflow = StateGraph(GraphState) \\n     \\n    # Add nodes \\n    workflow.add_node(\"chatbot\", chatbot) \\n     \\n    # Add edges \\n    workflow.set_entry_point(\"chatbot\") \\n    workflow.add_edge(\"chatbot\", END) \\n     \\n    return workflow.compile() \\n# Usage \\napp = create_simple_graph() \\nresult = app.invoke({\"messages\": [HumanMessage(content=\"Hello!\")]}) \\nprint(result[\"messages\"][-1].content) \\nConditional Branching'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:59+00:00', 'source': '../data/pdf_files/huggingface_tutorial.pdf', 'file_path': '../data/pdf_files/huggingface_tutorial.pdf', 'total_pages': 58, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:23:00+00:00', 'trapped': '', 'modDate': 'D:20250918142300Z', 'creationDate': \"D:20250918142259+00'00'\", 'page': 30}, page_content='def should_continue(state: GraphState) -> str: \\n    messages = state[\"messages\"] \\n    last_message = messages[-1] \\n     \\n    if \"FINAL ANSWER\" in last_message.content: \\n        return \"end\" \\n    else: \\n        return \"continue\" \\ndef researcher(state: GraphState): \\n    # Research logic here \\n    return {\"messages\": state[\"messages\"] + [AIMessage(content=\"Research complete\")]} \\ndef writer(state: GraphState): \\n    # Writing logic here   \\n    return {\"messages\": state[\"messages\"] + [AIMessage(content=\"FINAL ANSWER: Here\\'s the result\")]} \\ndef create_conditional_graph(): \\n    workflow = StateGraph(GraphState) \\n     \\n    workflow.add_node(\"researcher\", researcher) \\n    workflow.add_node(\"writer\", writer) \\n     \\n    workflow.set_entry_point(\"researcher\") \\n    workflow.add_conditional_edges( \\n        \"researcher\", \\n        should_continue, \\n        { \\n            \"continue\": \"writer\",  \\n            \"end\": END \\n        } \\n    )\\n    workflow.add_edge(\"writer\", END) \\n     \\n    return workflow.compile() \\nAdvanced Patterns\\nMulti-Agent Collaboration'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:59+00:00', 'source': '../data/pdf_files/huggingface_tutorial.pdf', 'file_path': '../data/pdf_files/huggingface_tutorial.pdf', 'total_pages': 58, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:23:00+00:00', 'trapped': '', 'modDate': 'D:20250918142300Z', 'creationDate': \"D:20250918142259+00'00'\", 'page': 31}, page_content='from langgraph.prebuilt import create_react_agent \\nfrom langchain_core.tools import Tool \\nclass MultiAgentState(TypedDict): \\n    messages: List[BaseMessage] \\n    next_agent: str \\n     \\ndef create_multi_agent_system(): \\n    # Create specialized agents \\n    researcher = create_react_agent( \\n        ChatOpenAI(),  \\n        [search_tool, calculator_tool] \\n    )\\n     \\n    writer = create_react_agent( \\n        ChatOpenAI(), \\n        [writing_tool, fact_checker_tool] \\n    )\\n     \\n    def agent_node(state, agent, name): \\n        result = agent.invoke(state) \\n        return { \\n            \"messages\": [AIMessage(content=result[\"output\"])], \\n            \"next_agent\": determine_next_agent(result) \\n        } \\n     \\n    workflow = StateGraph(MultiAgentState) \\n    workflow.add_node(\"researcher\", lambda x: agent_node(x, researcher, \"researcher\")) \\n    workflow.add_node(\"writer\", lambda x: agent_node(x, writer, \"writer\")) \\n     \\n    # Routing logic \\n    def route_next(state): \\n        return state.get(\"next_agent\", \"writer\") \\n     \\n    workflow.add_conditional_edges(\"researcher\", route_next) \\n    workflow.add_conditional_edges(\"writer\", route_next) \\n     \\n    return workflow.compile() \\nHuman-in-the-Loop'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:59+00:00', 'source': '../data/pdf_files/huggingface_tutorial.pdf', 'file_path': '../data/pdf_files/huggingface_tutorial.pdf', 'total_pages': 58, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:23:00+00:00', 'trapped': '', 'modDate': 'D:20250918142300Z', 'creationDate': \"D:20250918142259+00'00'\", 'page': 32}, page_content='from langgraph.checkpoint.sqlite import SqliteSaver \\ndef create_human_in_loop_graph(): \\n    memory = SqliteSaver.from_conn_string(\":memory:\") \\n     \\n    def human_feedback(state): \\n        # This will pause execution and wait for human input \\n        pass \\n     \\n    workflow = StateGraph(GraphState) \\n    workflow.add_node(\"agent\", chatbot) \\n    workflow.add_node(\"human\", human_feedback) \\n     \\n    workflow.set_entry_point(\"agent\") \\n    workflow.add_edge(\"agent\", \"human\") \\n    workflow.add_edge(\"human\", END) \\n     \\n    return workflow.compile(checkpointer=memory, interrupt_before=[\"human\"]) \\n# Usage with interruption \\napp = create_human_in_loop_graph() \\nconfig = {\"configurable\": {\"thread_id\": \"1\"}} \\n# Initial run - will stop at human node \\nresult = app.invoke({\"messages\": [HumanMessage(\"Analyze this data\")]}, config) \\n# Continue after human feedback \\napp.update_state(config, {\"messages\": [HumanMessage(\"User feedback here\")]}) \\nresult = app.invoke(None, config)  # Resume from checkpoint \\nParallel Processing\\ndef create_parallel_processing_graph(): \\n    def parallel_task_1(state): \\n        return {\"task1_result\": \"Result from task 1\"} \\n     \\n    def parallel_task_2(state): \\n        return {\"task2_result\": \"Result from task 2\"} \\n     \\n    def combine_results(state): \\n        combined = f\"{state.get(\\'task1_result\\', \\'\\')} + {state.get(\\'task2_result\\', \\'\\')}\" \\n        return {\"final_result\": combined} \\n     \\n    workflow = StateGraph(dict) \\n    workflow.add_node(\"task1\", parallel_task_1) \\n    workflow.add_node(\"task2\", parallel_task_2) \\n    workflow.add_node(\"combine\", combine_results) \\n     \\n    # Both tasks run in parallel \\n    workflow.set_entry_point(\"task1\") \\n    workflow.set_entry_point(\"task2\") \\n     \\n    # Both feed into combine \\n    workflow.add_edge(\"task1\", \"combine\") \\n    workflow.add_edge(\"task2\", \"combine\") \\n    workflow.add_edge(\"combine\", END) \\n     \\n    return workflow.compile()'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:59+00:00', 'source': '../data/pdf_files/huggingface_tutorial.pdf', 'file_path': '../data/pdf_files/huggingface_tutorial.pdf', 'total_pages': 58, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:23:00+00:00', 'trapped': '', 'modDate': 'D:20250918142300Z', 'creationDate': \"D:20250918142259+00'00'\", 'page': 33}, page_content='Real-World Examples\\nResearch Assistant\\ndef create_research_assistant(): \\n    class ResearchState(TypedDict): \\n        query: str \\n        research_results: List[str] \\n        summary: str \\n        citations: List[str] \\n     \\n    def search_step(state: ResearchState): \\n        # Implement web search \\n        results = web_search(state[\"query\"]) \\n        return {\"research_results\": results} \\n     \\n    def analyze_step(state: ResearchState): \\n        # Analyze search results \\n        analysis = llm_analyze(state[\"research_results\"]) \\n        return {\"summary\": analysis} \\n     \\n    def cite_step(state: ResearchState): \\n        # Generate citations \\n        citations = extract_citations(state[\"research_results\"]) \\n        return {\"citations\": citations} \\n     \\n    workflow = StateGraph(ResearchState) \\n    workflow.add_node(\"search\", search_step) \\n    workflow.add_node(\"analyze\", analyze_step)  \\n    workflow.add_node(\"cite\", cite_step) \\n     \\n    workflow.set_entry_point(\"search\") \\n    workflow.add_edge(\"search\", \"analyze\") \\n    workflow.add_edge(\"analyze\", \"cite\") \\n    workflow.add_edge(\"cite\", END) \\n     \\n    return workflow.compile() \\nCustomer Service Agent'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:59+00:00', 'source': '../data/pdf_files/huggingface_tutorial.pdf', 'file_path': '../data/pdf_files/huggingface_tutorial.pdf', 'total_pages': 58, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:23:00+00:00', 'trapped': '', 'modDate': 'D:20250918142300Z', 'creationDate': \"D:20250918142259+00'00'\", 'page': 34}, page_content='def create_customer_service_agent(): \\n    class ServiceState(TypedDict): \\n        customer_input: str \\n        intent: str \\n        customer_data: dict \\n        response: str \\n        escalate: bool \\n     \\n    def classify_intent(state: ServiceState): \\n        intent = classify_customer_intent(state[\"customer_input\"]) \\n        return {\"intent\": intent} \\n     \\n    def fetch_customer_data(state: ServiceState): \\n        data = get_customer_info(state.get(\"customer_id\")) \\n        return {\"customer_data\": data} \\n     \\n    def handle_request(state: ServiceState): \\n        response = generate_response( \\n            state[\"intent\"],  \\n            state[\"customer_data\"],  \\n            state[\"customer_input\"] \\n        ) \\n        return {\"response\": response, \"escalate\": should_escalate(response)} \\n     \\n    def escalate_to_human(state: ServiceState): \\n        # Escalation logic \\n        return {\"response\": \"Transferring to human agent...\"} \\n     \\n    def should_escalate_decision(state: ServiceState): \\n        return \"escalate\" if state.get(\"escalate\") else \"respond\" \\n     \\n    workflow = StateGraph(ServiceState) \\n    workflow.add_node(\"classify\", classify_intent) \\n    workflow.add_node(\"fetch_data\", fetch_customer_data) \\n    workflow.add_node(\"handle\", handle_request) \\n    workflow.add_node(\"escalate\", escalate_to_human) \\n     \\n    workflow.set_entry_point(\"classify\") \\n    workflow.add_edge(\"classify\", \"fetch_data\") \\n    workflow.add_edge(\"fetch_data\", \"handle\") \\n    workflow.add_conditional_edges( \\n        \"handle\", \\n        should_escalate_decision, \\n        {\"escalate\": \"escalate\", \"respond\": END} \\n    )\\n    workflow.add_edge(\"escalate\", END) \\n     \\n    return workflow.compile() \\nBest Practices\\n1. State Design\\nKeep state minimal and focused\\nUse TypedDict for type safety\\nAvoid deeply nested structures\\nMake state serializable for checkpoints'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:59+00:00', 'source': '../data/pdf_files/huggingface_tutorial.pdf', 'file_path': '../data/pdf_files/huggingface_tutorial.pdf', 'total_pages': 58, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:23:00+00:00', 'trapped': '', 'modDate': 'D:20250918142300Z', 'creationDate': \"D:20250918142259+00'00'\", 'page': 35}, page_content='2. Error Handling\\ndef robust_node(state): \\n    try: \\n        # Your logic here \\n        result = process_data(state) \\n        return {\"result\": result, \"error\": None} \\n    except Exception as e: \\n        return {\"result\": None, \"error\": str(e)} \\ndef error_recovery(state): \\n    if state.get(\"error\"): \\n        # Implement recovery logic \\n        return {\"error\": None, \"retry_count\": state.get(\"retry_count\", 0) + 1} \\n    return state \\n3. Testing Strategies\\ndef test_graph(): \\n    app = create_your_graph() \\n     \\n    # Test individual nodes \\n    test_state = {\"messages\": [HumanMessage(\"test\")]} \\n    result = app.get_node(\"your_node\").invoke(test_state) \\n    assert \"expected_key\" in result \\n     \\n    # Test full flow \\n    final_result = app.invoke(test_state) \\n    assert final_result[\"messages\"][-1].content is not None \\n4. Performance Optimization\\nUse streaming for long-running operations\\nImplement proper caching strategies\\nConsider parallel execution where possible\\nMonitor state size and complexity\\n5. Monitoring and Debugging\\nfrom langgraph.prebuilt import ToolExecutor \\nimport logging \\nlogging.basicConfig(level=logging.INFO) \\ndef debug_node(state): \\n    logging.info(f\"Node input: {state}\") \\n    result = your_processing_function(state) \\n    logging.info(f\"Node output: {result}\") \\n    return result \\nConclusion\\nLangGraph provides a powerful framework for building complex, stateful AI applications. By understanding its core concepts and patterns, you can create\\nsophisticated multi-agent systems that handle real-world complexity.\\nNext Steps:'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:59+00:00', 'source': '../data/pdf_files/huggingface_tutorial.pdf', 'file_path': '../data/pdf_files/huggingface_tutorial.pdf', 'total_pages': 58, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:23:00+00:00', 'trapped': '', 'modDate': 'D:20250918142300Z', 'creationDate': \"D:20250918142259+00'00'\", 'page': 36}, page_content=\"1. Experiment with the examples provided\\n2. Build your own custom nodes and edges\\n3. Explore the LangGraph documentation for advanced features\\n4. Join the LangChain community for support and updates\\nAdditional Resources:\\nLangGraph Documentation (https://python.langchain.com/docs/langgraph)\\nLangGraph Examples Repository (https://github.com/langchain-ai/langgraph/tree/main/examples)\\nLangChain Community (https://discord.gg/cU2adEyC7w)\\nOpenAI API Tutorial: Complete Guide to\\nGPT Models and Beyond\\nTable of Contents\\n1. Introduction to OpenAI API\\n2. Setup and Authentication\\n3. Chat Completions\\n4. Text Generation\\n5. Function Calling\\n6. Embeddings\\n7. Vision Models\\n8. Audio and Speech\\n9. Fine-tuning\\n10. Best Practices\\nIntroduction to OpenAI API\\nThe OpenAI API provides access to powerful AI models including GPT-4, GPT-3.5, DALL-E, Whisper, and more. This tutorial covers everything you need to know to\\nintegrate OpenAI's models into your applications.\\nAvailable Models:\\nGPT-4: Most capable model for complex reasoning\\nGPT-3.5: Fast and efficient for most tasks\\nGPT-4 Vision: Understands images and text\\nDALL-E 3: Generate images from text\\nWhisper: Speech-to-text transcription\\nTTS: Text-to-speech synthesis\\nSetup and Authentication\\nInstallation\\n# Install the OpenAI Python library \\npip install openai \\n# For async support \\npip install openai[async] \\n# Latest version with all features \\npip install openai>=1.0.0\"), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:59+00:00', 'source': '../data/pdf_files/huggingface_tutorial.pdf', 'file_path': '../data/pdf_files/huggingface_tutorial.pdf', 'total_pages': 58, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:23:00+00:00', 'trapped': '', 'modDate': 'D:20250918142300Z', 'creationDate': \"D:20250918142259+00'00'\", 'page': 37}, page_content='API Key Setup\\nimport openai \\nimport os \\nfrom openai import OpenAI \\n# Method 1: Environment variable (recommended) \\nos.environ[\"OPENAI_API_KEY\"] = \"your-api-key-here\" \\nclient = OpenAI() \\n# Method 2: Direct initialization \\nclient = OpenAI(api_key=\"your-api-key-here\") \\n# Method 3: Using Azure OpenAI \\nfrom openai import AzureOpenAI \\nazure_client = AzureOpenAI( \\n    api_key=\"your-azure-key\", \\n    api_version=\"2023-12-01-preview\", \\n    azure_endpoint=\"https://your-endpoint.openai.azure.com/\" \\n) \\nBasic Configuration\\n# Set default parameters \\nclient = OpenAI( \\n    api_key=\"your-key\", \\n    organization=\"your-org-id\",  # Optional \\n    project=\"your-project-id\",   # Optional \\n    base_url=\"https://api.openai.com/v1\",  # Custom endpoint if needed \\n    default_headers={\"Custom-Header\": \"value\"} \\n) \\nChat Completions\\nBasic Chat Completion\\ndef basic_chat_completion(): \\n    response = client.chat.completions.create( \\n        model=\"gpt-4\", \\n        messages=[ \\n            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, \\n            {\"role\": \"user\", \"content\": \"Hello! How can you help me today?\"} \\n        ],\\n        max_tokens=150, \\n        temperature=0.7 \\n    )\\n     \\n    return response.choices[0].message.content \\nprint(basic_chat_completion()) \\nAdvanced Parameters'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:59+00:00', 'source': '../data/pdf_files/huggingface_tutorial.pdf', 'file_path': '../data/pdf_files/huggingface_tutorial.pdf', 'total_pages': 58, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:23:00+00:00', 'trapped': '', 'modDate': 'D:20250918142300Z', 'creationDate': \"D:20250918142259+00'00'\", 'page': 38}, page_content='def advanced_chat_completion(): \\n    response = client.chat.completions.create( \\n        model=\"gpt-4\", \\n        messages=[ \\n            {\"role\": \"system\", \"content\": \"You are an expert Python developer.\"}, \\n            {\"role\": \"user\", \"content\": \"Explain list comprehensions with examples.\"} \\n        ],\\n        max_tokens=500, \\n        temperature=0.3,           # Lower = more focused \\n        top_p=0.9,                # Nucleus sampling \\n        frequency_penalty=0.0,     # Reduce repetition \\n        presence_penalty=0.0,      # Encourage new topics \\n        stop=[\"\\\\n\\\\n\", \"###\"],      # Stop sequences \\n        seed=42                    # For reproducible outputs \\n    )\\n     \\n    return response.choices[0].message.content \\nStreaming Responses\\ndef streaming_chat(): \\n    stream = client.chat.completions.create( \\n        model=\"gpt-4\", \\n        messages=[{\"role\": \"user\", \"content\": \"Tell me a long story about AI\"}], \\n        stream=True \\n    )\\n     \\n    full_response = \"\" \\n    for chunk in stream: \\n        if chunk.choices[0].delta.content is not None: \\n            content = chunk.choices[0].delta.content \\n            print(content, end=\"\", flush=True) \\n            full_response += content \\n     \\n    return full_response \\nConversation Management'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:59+00:00', 'source': '../data/pdf_files/huggingface_tutorial.pdf', 'file_path': '../data/pdf_files/huggingface_tutorial.pdf', 'total_pages': 58, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:23:00+00:00', 'trapped': '', 'modDate': 'D:20250918142300Z', 'creationDate': \"D:20250918142259+00'00'\", 'page': 39}, page_content='class ChatManager: \\n    def __init__(self, system_message=\"You are a helpful assistant.\"): \\n        self.messages = [{\"role\": \"system\", \"content\": system_message}] \\n        self.client = OpenAI() \\n     \\n    def add_user_message(self, content): \\n        self.messages.append({\"role\": \"user\", \"content\": content}) \\n     \\n    def add_assistant_message(self, content): \\n        self.messages.append({\"role\": \"assistant\", \"content\": content}) \\n     \\n    def get_response(self, user_input): \\n        self.add_user_message(user_input) \\n         \\n        response = self.client.chat.completions.create( \\n            model=\"gpt-4\", \\n            messages=self.messages, \\n            max_tokens=500, \\n            temperature=0.7 \\n        ) \\n         \\n        assistant_message = response.choices[0].message.content \\n        self.add_assistant_message(assistant_message) \\n         \\n        return assistant_message \\n     \\n    def clear_history(self, keep_system=True): \\n        if keep_system and self.messages[0][\"role\"] == \"system\": \\n            self.messages = [self.messages[0]] \\n        else: \\n            self.messages = [] \\n# Usage \\nchat = ChatManager() \\nresponse1 = chat.get_response(\"What is machine learning?\") \\nresponse2 = chat.get_response(\"Can you give me an example?\") \\nText Generation\\nLegacy Completions (GPT-3.5-turbo-instruct)\\ndef text_completion(): \\n    response = client.completions.create( \\n        model=\"gpt-3.5-turbo-instruct\", \\n        prompt=\"Complete this sentence: The future of AI is\", \\n        max_tokens=100, \\n        temperature=0.8, \\n        stop=[\"\\\\n\"] \\n    )\\n     \\n    return response.choices[0].text.strip() \\nCreative Writing Assistant'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:59+00:00', 'source': '../data/pdf_files/huggingface_tutorial.pdf', 'file_path': '../data/pdf_files/huggingface_tutorial.pdf', 'total_pages': 58, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:23:00+00:00', 'trapped': '', 'modDate': 'D:20250918142300Z', 'creationDate': \"D:20250918142259+00'00'\", 'page': 40}, page_content='def creative_writer(prompt, style=\"narrative\", length=\"medium\"): \\n    length_map = { \\n        \"short\": 200, \\n        \"medium\": 500, \\n        \"long\": 1000 \\n    }\\n     \\n    system_message = f\"\"\"You are a creative writing assistant specializing in {style} writing.  \\n    Create engaging, well-structured content that captures the reader\\'s attention.\"\"\" \\n     \\n    response = client.chat.completions.create( \\n        model=\"gpt-4\", \\n        messages=[ \\n            {\"role\": \"system\", \"content\": system_message}, \\n            {\"role\": \"user\", \"content\": f\"Write a {style} piece based on: {prompt}\"} \\n        ],\\n        max_tokens=length_map.get(length, 500), \\n        temperature=0.8, \\n        presence_penalty=0.1 \\n    )\\n     \\n    return response.choices[0].message.content \\nFunction Calling\\nBasic Function Calling'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:59+00:00', 'source': '../data/pdf_files/huggingface_tutorial.pdf', 'file_path': '../data/pdf_files/huggingface_tutorial.pdf', 'total_pages': 58, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:23:00+00:00', 'trapped': '', 'modDate': 'D:20250918142300Z', 'creationDate': \"D:20250918142259+00'00'\", 'page': 41}, page_content='import json \\ndef get_weather(location, unit=\"celsius\"): \\n    \"\"\"Simulate weather API call\"\"\" \\n    return { \\n        \"location\": location, \\n        \"temperature\": \"22\", \\n        \"unit\": unit, \\n        \"condition\": \"sunny\" \\n    }\\ndef function_calling_example(): \\n    # Define the function schema \\n    tools = [ \\n        { \\n            \"type\": \"function\", \\n            \"function\": { \\n                \"name\": \"get_weather\", \\n                \"description\": \"Get current weather for a location\", \\n                \"parameters\": { \\n                    \"type\": \"object\", \\n                    \"properties\": { \\n                        \"location\": { \\n                            \"type\": \"string\", \\n                            \"description\": \"City name\" \\n                        }, \\n                        \"unit\": { \\n                            \"type\": \"string\", \\n                            \"enum\": [\"celsius\", \"fahrenheit\"] \\n                        } \\n                    }, \\n                    \"required\": [\"location\"] \\n                } \\n            } \\n        } \\n    ]\\n     \\n    messages = [ \\n        {\"role\": \"user\", \"content\": \"What\\'s the weather like in Paris?\"} \\n    ]\\n     \\n    # First call - model decides to use function \\n    response = client.chat.completions.create( \\n        model=\"gpt-4\", \\n        messages=messages, \\n        tools=tools, \\n        tool_choice=\"auto\" \\n    )\\n     \\n    # Check if model wants to call a function \\n    if response.choices[0].message.tool_calls: \\n        tool_call = response.choices[0].message.tool_calls[0] \\n        function_name = tool_call.function.name \\n        function_args = json.loads(tool_call.function.arguments) \\n         \\n        # Execute the function \\n        if function_name == \"get_weather\": \\n            function_result = get_weather(**function_args) \\n             \\n            # Add function result to conversation'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:59+00:00', 'source': '../data/pdf_files/huggingface_tutorial.pdf', 'file_path': '../data/pdf_files/huggingface_tutorial.pdf', 'total_pages': 58, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:23:00+00:00', 'trapped': '', 'modDate': 'D:20250918142300Z', 'creationDate': \"D:20250918142259+00'00'\", 'page': 42}, page_content='messages.append(response.choices[0].message) \\n            messages.append({ \\n                \"role\": \"tool\", \\n                \"content\": json.dumps(function_result), \\n                \"tool_call_id\": tool_call.id \\n            }) \\n             \\n            # Get final response \\n            final_response = client.chat.completions.create( \\n                model=\"gpt-4\", \\n                messages=messages \\n            ) \\n             \\n            return final_response.choices[0].message.content \\n     \\n    return response.choices[0].message.content \\nMultiple Function Agent'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:59+00:00', 'source': '../data/pdf_files/huggingface_tutorial.pdf', 'file_path': '../data/pdf_files/huggingface_tutorial.pdf', 'total_pages': 58, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:23:00+00:00', 'trapped': '', 'modDate': 'D:20250918142300Z', 'creationDate': \"D:20250918142259+00'00'\", 'page': 43}, page_content='class FunctionAgent: \\n    def __init__(self): \\n        self.client = OpenAI() \\n        self.functions = { \\n            \"calculator\": self.calculate, \\n            \"search\": self.search, \\n            \"save_note\": self.save_note \\n        } \\n         \\n        self.tools = [ \\n            { \\n                \"type\": \"function\", \\n                \"function\": { \\n                    \"name\": \"calculator\", \\n                    \"description\": \"Perform mathematical calculations\", \\n                    \"parameters\": { \\n                        \"type\": \"object\", \\n                        \"properties\": { \\n                            \"expression\": {\"type\": \"string\", \"description\": \"Math expression\"} \\n                        }, \\n                        \"required\": [\"expression\"] \\n                    } \\n                } \\n            }, \\n            { \\n                \"type\": \"function\",  \\n                \"function\": { \\n                    \"name\": \"search\", \\n                    \"description\": \"Search for information\", \\n                    \"parameters\": { \\n                        \"type\": \"object\", \\n                        \"properties\": { \\n                            \"query\": {\"type\": \"string\", \"description\": \"Search query\"} \\n                        }, \\n                        \"required\": [\"query\"] \\n                    } \\n                } \\n            } \\n        ] \\n     \\n    def calculate(self, expression): \\n        try: \\n            result = eval(expression)  # Use safely in production! \\n            return f\"Result: {result}\" \\n        except: \\n            return \"Error in calculation\" \\n     \\n    def search(self, query): \\n        return f\"Search results for \\'{query}\\': [Simulated results]\" \\n     \\n    def save_note(self, content): \\n        # Simulate saving \\n        return f\"Note saved: {content[:50]}...\" \\n     \\n    def run(self, user_input): \\n        messages = [{\"role\": \"user\", \"content\": user_input}] \\n         \\n        response = self.client.chat.completions.create( \\n            model=\"gpt-4\", \\n            messages=messages,'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:59+00:00', 'source': '../data/pdf_files/huggingface_tutorial.pdf', 'file_path': '../data/pdf_files/huggingface_tutorial.pdf', 'total_pages': 58, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:23:00+00:00', 'trapped': '', 'modDate': 'D:20250918142300Z', 'creationDate': \"D:20250918142259+00'00'\", 'page': 44}, page_content='tools=self.tools, \\n            tool_choice=\"auto\" \\n        ) \\n         \\n        # Handle tool calls \\n        if response.choices[0].message.tool_calls: \\n            messages.append(response.choices[0].message) \\n             \\n            for tool_call in response.choices[0].message.tool_calls: \\n                function_name = tool_call.function.name \\n                function_args = json.loads(tool_call.function.arguments) \\n                 \\n                if function_name in self.functions: \\n                    result = self.functions[function_name](**function_args) \\n                    messages.append({ \\n                        \"role\": \"tool\", \\n                        \"content\": result, \\n                        \"tool_call_id\": tool_call.id \\n                    }) \\n             \\n            # Get final response \\n            final_response = self.client.chat.completions.create( \\n                model=\"gpt-4\", \\n                messages=messages \\n            ) \\n            return final_response.choices[0].message.content \\n         \\n        return response.choices[0].message.content \\n# Usage \\nagent = FunctionAgent() \\nresult = agent.run(\"What\\'s 25 * 47 + 123?\") \\nEmbeddings\\nBasic Embeddings\\ndef get_embeddings(texts): \\n    if isinstance(texts, str): \\n        texts = [texts] \\n     \\n    response = client.embeddings.create( \\n        model=\"text-embedding-3-large\",  # or text-embedding-3-small \\n        input=texts \\n    )\\n     \\n    return [embedding.embedding for embedding in response.data] \\n# Usage \\ntext = \"This is a sample sentence for embedding.\" \\nembeddings = get_embeddings(text) \\nprint(f\"Embedding dimension: {len(embeddings[0])}\") \\nSemantic Search System'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:59+00:00', 'source': '../data/pdf_files/huggingface_tutorial.pdf', 'file_path': '../data/pdf_files/huggingface_tutorial.pdf', 'total_pages': 58, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:23:00+00:00', 'trapped': '', 'modDate': 'D:20250918142300Z', 'creationDate': \"D:20250918142259+00'00'\", 'page': 45}, page_content='import numpy as np \\nfrom sklearn.metrics.pairwise import cosine_similarity \\nclass SemanticSearch: \\n    def __init__(self): \\n        self.client = OpenAI() \\n        self.documents = [] \\n        self.embeddings = [] \\n     \\n    def add_documents(self, docs): \\n        \"\"\"Add documents to search index\"\"\" \\n        self.documents.extend(docs) \\n         \\n        # Get embeddings for new documents \\n        response = self.client.embeddings.create( \\n            model=\"text-embedding-3-large\", \\n            input=docs \\n        ) \\n         \\n        new_embeddings = [emb.embedding for emb in response.data] \\n        self.embeddings.extend(new_embeddings) \\n     \\n    def search(self, query, top_k=5): \\n        \"\"\"Search for similar documents\"\"\" \\n        if not self.embeddings: \\n            return [] \\n         \\n        # Get query embedding \\n        query_response = self.client.embeddings.create( \\n            model=\"text-embedding-3-large\", \\n            input=[query] \\n        ) \\n        query_embedding = query_response.data[0].embedding \\n         \\n        # Calculate similarities \\n        similarities = cosine_similarity( \\n            [query_embedding],  \\n            self.embeddings \\n        )[0] \\n         \\n        # Get top results \\n        top_indices = np.argsort(similarities)[::-1][:top_k] \\n         \\n        results = [] \\n        for idx in top_indices: \\n            results.append({ \\n                \"document\": self.documents[idx], \\n                \"similarity\": similarities[idx] \\n            }) \\n         \\n        return results \\n# Usage \\nsearch_engine = SemanticSearch() \\nsearch_engine.add_documents([ \\n    \"Python is a programming language\", \\n    \"Machine learning is a subset of AI\", \\n    \"Deep learning uses neural networks\", \\n    \"Natural language processing handles text\" \\n])'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:59+00:00', 'source': '../data/pdf_files/huggingface_tutorial.pdf', 'file_path': '../data/pdf_files/huggingface_tutorial.pdf', 'total_pages': 58, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:23:00+00:00', 'trapped': '', 'modDate': 'D:20250918142300Z', 'creationDate': \"D:20250918142259+00'00'\", 'page': 46}, page_content='results = search_engine.search(\"What is AI?\", top_k=2) \\nfor result in results: \\n    print(f\"Similarity: {result[\\'similarity\\']:.3f}\") \\n    print(f\"Document: {result[\\'document\\']}\\\\n\") \\nVision Models\\nImage Analysis'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:59+00:00', 'source': '../data/pdf_files/huggingface_tutorial.pdf', 'file_path': '../data/pdf_files/huggingface_tutorial.pdf', 'total_pages': 58, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:23:00+00:00', 'trapped': '', 'modDate': 'D:20250918142300Z', 'creationDate': \"D:20250918142259+00'00'\", 'page': 47}, page_content='import base64 \\nimport requests \\ndef encode_image(image_path): \\n    \"\"\"Encode image to base64\"\"\" \\n    with open(image_path, \"rb\") as image_file: \\n        return base64.b64encode(image_file.read()).decode(\\'utf-8\\') \\ndef analyze_image(image_path, prompt=\"What\\'s in this image?\"): \\n    base64_image = encode_image(image_path) \\n     \\n    response = client.chat.completions.create( \\n        model=\"gpt-4-vision-preview\", \\n        messages=[ \\n            { \\n                \"role\": \"user\", \\n                \"content\": [ \\n                    {\"type\": \"text\", \"text\": prompt}, \\n                    { \\n                        \"type\": \"image_url\", \\n                        \"image_url\": { \\n                            \"url\": f\"data:image/jpeg;base64,{base64_image}\", \\n                            \"detail\": \"high\"  # or \"low\" for faster processing \\n                        } \\n                    } \\n                ] \\n            } \\n        ],\\n        max_tokens=300 \\n    )\\n     \\n    return response.choices[0].message.content \\n# Usage with URL \\ndef analyze_image_url(image_url, prompt=\"Describe this image\"): \\n    response = client.chat.completions.create( \\n        model=\"gpt-4-vision-preview\", \\n        messages=[ \\n            { \\n                \"role\": \"user\", \\n                \"content\": [ \\n                    {\"type\": \"text\", \"text\": prompt}, \\n                    {\"type\": \"image_url\", \"image_url\": {\"url\": image_url}} \\n                ] \\n            } \\n        ],\\n        max_tokens=300 \\n    )\\n     \\n    return response.choices[0].message.content \\nDocument OCR and Analysis'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:59+00:00', 'source': '../data/pdf_files/huggingface_tutorial.pdf', 'file_path': '../data/pdf_files/huggingface_tutorial.pdf', 'total_pages': 58, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:23:00+00:00', 'trapped': '', 'modDate': 'D:20250918142300Z', 'creationDate': \"D:20250918142259+00'00'\", 'page': 48}, page_content='def document_analyzer(image_path): \\n    \"\"\"Extract and analyze text from documents\"\"\" \\n    base64_image = encode_image(image_path) \\n     \\n    response = client.chat.completions.create( \\n        model=\"gpt-4-vision-preview\", \\n        messages=[ \\n            { \\n                \"role\": \"user\", \\n                \"content\": [ \\n                    { \\n                        \"type\": \"text\",  \\n                        \"text\": \"Extract all text from this document and provide a summary of its key points.\" \\n                    }, \\n                    { \\n                        \"type\": \"image_url\", \\n                        \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"} \\n                    } \\n                ] \\n            } \\n        ],\\n        max_tokens=1000 \\n    )\\n     \\n    return response.choices[0].message.content \\nAudio and Speech\\nSpeech-to-Text (Whisper)'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:59+00:00', 'source': '../data/pdf_files/huggingface_tutorial.pdf', 'file_path': '../data/pdf_files/huggingface_tutorial.pdf', 'total_pages': 58, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:23:00+00:00', 'trapped': '', 'modDate': 'D:20250918142300Z', 'creationDate': \"D:20250918142259+00'00'\", 'page': 49}, page_content='def transcribe_audio(audio_file_path): \\n    \"\"\"Transcribe audio file using Whisper\"\"\" \\n    with open(audio_file_path, \"rb\") as audio_file: \\n        transcription = client.audio.transcriptions.create( \\n            model=\"whisper-1\", \\n            file=audio_file, \\n            response_format=\"text\" \\n        ) \\n     \\n    return transcription \\ndef transcribe_with_timestamps(audio_file_path): \\n    \"\"\"Get transcription with timestamps\"\"\" \\n    with open(audio_file_path, \"rb\") as audio_file: \\n        transcription = client.audio.transcriptions.create( \\n            model=\"whisper-1\", \\n            file=audio_file, \\n            response_format=\"verbose_json\", \\n            timestamp_granularities=[\"word\"] \\n        ) \\n     \\n    return transcription \\n# Translation \\ndef translate_audio(audio_file_path): \\n    \"\"\"Translate foreign language audio to English\"\"\" \\n    with open(audio_file_path, \"rb\") as audio_file: \\n        translation = client.audio.translations.create( \\n            model=\"whisper-1\", \\n            file=audio_file \\n        ) \\n     \\n    return translation.text \\nText-to-Speech'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:59+00:00', 'source': '../data/pdf_files/huggingface_tutorial.pdf', 'file_path': '../data/pdf_files/huggingface_tutorial.pdf', 'total_pages': 58, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:23:00+00:00', 'trapped': '', 'modDate': 'D:20250918142300Z', 'creationDate': \"D:20250918142259+00'00'\", 'page': 50}, page_content='def text_to_speech(text, voice=\"alloy\", output_file=\"speech.mp3\"): \\n    \"\"\"Convert text to speech\"\"\" \\n    response = client.audio.speech.create( \\n        model=\"tts-1\",  # or \"tts-1-hd\" for higher quality \\n        voice=voice,    # alloy, echo, fable, onyx, nova, shimmer \\n        input=text, \\n        speed=1.0       # 0.25 to 4.0 \\n    )\\n     \\n    with open(output_file, \"wb\") as f: \\n        for chunk in response.iter_bytes(): \\n            f.write(chunk) \\n     \\n    return output_file \\n# Real-time streaming \\ndef streaming_text_to_speech(text, voice=\"alloy\"): \\n    \"\"\"Stream audio in real-time\"\"\" \\n    response = client.audio.speech.create( \\n        model=\"tts-1\", \\n        voice=voice, \\n        input=text, \\n        response_format=\"opus\"  # Better for streaming \\n    )\\n     \\n    # Play audio chunks as they arrive \\n    for chunk in response.iter_bytes(chunk_size=1024): \\n        # Send to audio player \\n        yield chunk \\nFine-tuning\\nPrepare Training Data'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:59+00:00', 'source': '../data/pdf_files/huggingface_tutorial.pdf', 'file_path': '../data/pdf_files/huggingface_tutorial.pdf', 'total_pages': 58, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:23:00+00:00', 'trapped': '', 'modDate': 'D:20250918142300Z', 'creationDate': \"D:20250918142259+00'00'\", 'page': 51}, page_content='import json \\ndef prepare_training_data(examples): \\n    \"\"\"Prepare data for fine-tuning\"\"\" \\n    training_data = [] \\n     \\n    for example in examples: \\n        training_data.append({ \\n            \"messages\": [ \\n                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, \\n                {\"role\": \"user\", \"content\": example[\"input\"]}, \\n                {\"role\": \"assistant\", \"content\": example[\"output\"]} \\n            ] \\n        })\\n     \\n    # Save to JSONL file \\n    with open(\"training_data.jsonl\", \"w\") as f: \\n        for item in training_data: \\n            f.write(json.dumps(item) + \"\\\\n\") \\n     \\n    return \"training_data.jsonl\" \\n# Example data \\nexamples = [ \\n    {\"input\": \"What is Python?\", \"output\": \"Python is a programming language...\"}, \\n    {\"input\": \"How do lists work?\", \"output\": \"Lists in Python are ordered collections...\"} \\n] \\ntraining_file = prepare_training_data(examples) \\nFine-tuning Process'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:59+00:00', 'source': '../data/pdf_files/huggingface_tutorial.pdf', 'file_path': '../data/pdf_files/huggingface_tutorial.pdf', 'total_pages': 58, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:23:00+00:00', 'trapped': '', 'modDate': 'D:20250918142300Z', 'creationDate': \"D:20250918142259+00'00'\", 'page': 52}, page_content='def create_fine_tuning_job(training_file, model=\"gpt-3.5-turbo\"): \\n    \"\"\"Create a fine-tuning job\"\"\" \\n    # Upload training file \\n    with open(training_file, \"rb\") as f: \\n        file_response = client.files.create( \\n            file=f, \\n            purpose=\"fine-tune\" \\n        ) \\n     \\n    # Create fine-tuning job \\n    job = client.fine_tuning.jobs.create( \\n        training_file=file_response.id, \\n        model=model, \\n        hyperparameters={ \\n            \"n_epochs\": 3, \\n            \"batch_size\": 1, \\n            \"learning_rate_multiplier\": 2 \\n        } \\n    )\\n     \\n    return job \\ndef monitor_fine_tuning(job_id): \\n    \"\"\"Monitor fine-tuning progress\"\"\" \\n    job = client.fine_tuning.jobs.retrieve(job_id) \\n     \\n    print(f\"Job ID: {job.id}\") \\n    print(f\"Status: {job.status}\") \\n    print(f\"Model: {job.fine_tuned_model}\") \\n     \\n    # Get events \\n    events = client.fine_tuning.jobs.list_events(job_id) \\n    for event in events.data[:5]:  # Show last 5 events \\n        print(f\"{event.created_at}: {event.message}\") \\n     \\n    return job \\ndef use_fine_tuned_model(model_id, prompt): \\n    \"\"\"Use your fine-tuned model\"\"\" \\n    response = client.chat.completions.create( \\n        model=model_id, \\n        messages=[ \\n            {\"role\": \"user\", \"content\": prompt} \\n        ] \\n    )\\n     \\n    return response.choices[0].message.content \\nBest Practices\\nError Handling'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:59+00:00', 'source': '../data/pdf_files/huggingface_tutorial.pdf', 'file_path': '../data/pdf_files/huggingface_tutorial.pdf', 'total_pages': 58, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:23:00+00:00', 'trapped': '', 'modDate': 'D:20250918142300Z', 'creationDate': \"D:20250918142259+00'00'\", 'page': 53}, page_content='from openai import RateLimitError, APIError \\nimport time \\ndef robust_api_call(func, max_retries=3, backoff_factor=2): \\n    \"\"\"Robust API call with retry logic\"\"\" \\n    for attempt in range(max_retries): \\n        try: \\n            return func() \\n        except RateLimitError: \\n            if attempt == max_retries - 1: \\n                raise \\n            wait_time = backoff_factor ** attempt \\n            print(f\"Rate limit hit, waiting {wait_time} seconds...\") \\n            time.sleep(wait_time) \\n        except APIError as e: \\n            print(f\"API Error: {e}\") \\n            if attempt == max_retries - 1: \\n                raise \\n            time.sleep(backoff_factor ** attempt) \\n# Usage \\ndef safe_chat_completion(message): \\n    return robust_api_call( \\n        lambda: client.chat.completions.create( \\n            model=\"gpt-4\", \\n            messages=[{\"role\": \"user\", \"content\": message}] \\n        ) \\n    )\\nToken Management'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:59+00:00', 'source': '../data/pdf_files/huggingface_tutorial.pdf', 'file_path': '../data/pdf_files/huggingface_tutorial.pdf', 'total_pages': 58, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:23:00+00:00', 'trapped': '', 'modDate': 'D:20250918142300Z', 'creationDate': \"D:20250918142259+00'00'\", 'page': 54}, page_content='import tiktoken \\ndef count_tokens(text, model=\"gpt-4\"): \\n    \"\"\"Count tokens in text\"\"\" \\n    encoding = tiktoken.encoding_for_model(model) \\n    return len(encoding.encode(text)) \\ndef truncate_text(text, max_tokens, model=\"gpt-4\"): \\n    \"\"\"Truncate text to fit within token limit\"\"\" \\n    encoding = tiktoken.encoding_for_model(model) \\n    tokens = encoding.encode(text) \\n     \\n    if len(tokens) <= max_tokens: \\n        return text \\n     \\n    truncated_tokens = tokens[:max_tokens] \\n    return encoding.decode(truncated_tokens) \\ndef smart_chunking(text, chunk_size=1000, model=\"gpt-4\"): \\n    \"\"\"Split text into chunks based on token count\"\"\" \\n    encoding = tiktoken.encoding_for_model(model) \\n    tokens = encoding.encode(text) \\n     \\n    chunks = [] \\n    for i in range(0, len(tokens), chunk_size): \\n        chunk_tokens = tokens[i:i + chunk_size] \\n        chunk_text = encoding.decode(chunk_tokens) \\n        chunks.append(chunk_text) \\n     \\n    return chunks \\nCost Optimization'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:59+00:00', 'source': '../data/pdf_files/huggingface_tutorial.pdf', 'file_path': '../data/pdf_files/huggingface_tutorial.pdf', 'total_pages': 58, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:23:00+00:00', 'trapped': '', 'modDate': 'D:20250918142300Z', 'creationDate': \"D:20250918142259+00'00'\", 'page': 55}, page_content='class CostTracker: \\n    def __init__(self): \\n        self.costs = { \\n            \"gpt-4\": {\"input\": 0.03, \"output\": 0.06},  # per 1K tokens \\n            \"gpt-3.5-turbo\": {\"input\": 0.001, \"output\": 0.002}, \\n            \"text-embedding-3-large\": {\"input\": 0.00013, \"output\": 0} \\n        } \\n        self.total_cost = 0 \\n     \\n    def calculate_cost(self, model, input_tokens, output_tokens): \\n        if model in self.costs: \\n            cost = ( \\n                (input_tokens / 1000) * self.costs[model][\"input\"] + \\n                (output_tokens / 1000) * self.costs[model][\"output\"] \\n            ) \\n            self.total_cost += cost \\n            return cost \\n        return 0 \\n     \\n    def tracked_completion(self, **kwargs): \\n        response = client.chat.completions.create(**kwargs) \\n         \\n        usage = response.usage \\n        cost = self.calculate_cost( \\n            kwargs[\"model\"], \\n            usage.prompt_tokens, \\n            usage.completion_tokens \\n        ) \\n         \\n        print(f\"Cost: ${cost:.4f} | Total: ${self.total_cost:.4f}\") \\n        return response \\n# Usage \\ntracker = CostTracker() \\nresponse = tracker.tracked_completion( \\n    model=\"gpt-4\", \\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}] \\n) \\nAsync Operations'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:59+00:00', 'source': '../data/pdf_files/huggingface_tutorial.pdf', 'file_path': '../data/pdf_files/huggingface_tutorial.pdf', 'total_pages': 58, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:23:00+00:00', 'trapped': '', 'modDate': 'D:20250918142300Z', 'creationDate': \"D:20250918142259+00'00'\", 'page': 56}, page_content='import asyncio \\nfrom openai import AsyncOpenAI \\nasync_client = AsyncOpenAI() \\nasync def async_chat_completion(message): \\n    \"\"\"Async chat completion\"\"\" \\n    response = await async_client.chat.completions.create( \\n        model=\"gpt-4\", \\n        messages=[{\"role\": \"user\", \"content\": message}] \\n    )\\n    return response.choices[0].message.content \\nasync def batch_completions(messages): \\n    \"\"\"Process multiple completions concurrently\"\"\" \\n    tasks = [async_chat_completion(msg) for msg in messages] \\n    results = await asyncio.gather(*tasks) \\n    return results \\n# Usage \\nasync def main(): \\n    messages = [ \\n        \"What is Python?\", \\n        \"What is JavaScript?\", \\n        \"What is Rust?\" \\n    ]\\n     \\n    results = await batch_completions(messages) \\n    for i, result in enumerate(results): \\n        print(f\"Question {i+1}: {result[:100]}...\") \\n# Run\\n# asyncio.run(main()) \\nProduction Configuration'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:59+00:00', 'source': '../data/pdf_files/huggingface_tutorial.pdf', 'file_path': '../data/pdf_files/huggingface_tutorial.pdf', 'total_pages': 58, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:23:00+00:00', 'trapped': '', 'modDate': 'D:20250918142300Z', 'creationDate': \"D:20250918142259+00'00'\", 'page': 57}, page_content='class ProductionOpenAI: \\n    def __init__(self, api_key=None): \\n        self.client = OpenAI( \\n            api_key=api_key or os.getenv(\"OPENAI_API_KEY\"), \\n            timeout=30, \\n            max_retries=3 \\n        ) \\n        self.default_params = { \\n            \"temperature\": 0.7, \\n            \"max_tokens\": 1000, \\n            \"top_p\": 0.9 \\n        } \\n     \\n    def chat(self, messages, **kwargs): \\n        params = {**self.default_params, **kwargs} \\n         \\n        try: \\n            response = self.client.chat.completions.create( \\n                messages=messages, \\n                **params \\n            ) \\n            return { \\n                \"success\": True, \\n                \"content\": response.choices[0].message.content, \\n                \"usage\": response.usage, \\n                \"model\": response.model \\n            } \\n        except Exception as e: \\n            return { \\n                \"success\": False, \\n                \"error\": str(e), \\n                \"content\": None \\n            } \\nConclusion\\nThe OpenAI API provides powerful capabilities for building AI-powered applications. This tutorial covered the essential patterns and best practices for:\\nChat completions and conversation management\\nFunction calling for tool integration\\nEmbeddings for semantic search\\nVision capabilities for image analysis\\nAudio processing with Whisper and TTS\\nFine-tuning for specialized models\\nProduction-ready error handling and optimization\\nNext Steps:\\n1. Experiment with different models and parameters\\n2. Build a complete application using multiple API features\\n3. Implement proper monitoring and cost tracking\\n4. Explore advanced techniques like RAG and agent frameworks\\nAdditional Resources:\\nOpenAI API Documentation (https://platform.openai.com/docs)\\nOpenAI Cookbook (https://github.com/openai/openai-cookbook)\\nBest Practices Guide (https://platform.openai.com/docs/guides/production-best-practices)'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:24:05+00:00', 'source': '../data/pdf_files/openai_api_tutorial.pdf', 'file_path': '../data/pdf_files/openai_api_tutorial.pdf', 'total_pages': 24, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:24:06+00:00', 'trapped': '', 'modDate': 'D:20250918142406Z', 'creationDate': \"D:20250918142405+00'00'\", 'page': 0}, page_content=\"OpenAI API Tutorial: Complete Guide\\nto GPT Models and Beyond\\nTable of Contents\\n1. Introduction to OpenAI API\\n2. Setup and Authentication\\n3. Chat Completions\\n4. Text Generation\\n5. Function Calling\\n6. Embeddings\\n7. Vision Models\\n8. Audio and Speech\\n9. Fine-tuning\\n10. Best Practices\\nIntroduction to OpenAI API\\nThe OpenAI API provides access to powerful AI models including GPT-4, GPT-3.5, DALL-E, Whisper, and more. This tutorial covers everything you\\nneed to know to integrate OpenAI's models into your applications.\\nAvailable Models:\\nGPT-4: Most capable model for complex reasoning\\nGPT-3.5: Fast and efficient for most tasks\\nGPT-4 Vision: Understands images and text\\nDALL-E 3: Generate images from text\\nWhisper: Speech-to-text transcription\\nTTS: Text-to-speech synthesis\\nSetup and Authentication\\nInstallation\\n# Install the OpenAI Python library \\npip install openai \\n# For async support \\npip install openai[async] \\n# Latest version with all features \\npip install openai>=1.0.0 \\nAPI Key Setup\"), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:24:05+00:00', 'source': '../data/pdf_files/openai_api_tutorial.pdf', 'file_path': '../data/pdf_files/openai_api_tutorial.pdf', 'total_pages': 24, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:24:06+00:00', 'trapped': '', 'modDate': 'D:20250918142406Z', 'creationDate': \"D:20250918142405+00'00'\", 'page': 1}, page_content='import openai \\nimport os \\nfrom openai import OpenAI \\n# Method 1: Environment variable (recommended) \\nos.environ[\"OPENAI_API_KEY\"] = \"your-api-key-here\" \\nclient = OpenAI() \\n# Method 2: Direct initialization \\nclient = OpenAI(api_key=\"your-api-key-here\") \\n# Method 3: Using Azure OpenAI \\nfrom openai import AzureOpenAI \\nazure_client = AzureOpenAI( \\n    api_key=\"your-azure-key\", \\n    api_version=\"2023-12-01-preview\", \\n    azure_endpoint=\"https://your-endpoint.openai.azure.com/\" \\n) \\nBasic Configuration\\n# Set default parameters \\nclient = OpenAI( \\n    api_key=\"your-key\", \\n    organization=\"your-org-id\",  # Optional \\n    project=\"your-project-id\",   # Optional \\n    base_url=\"https://api.openai.com/v1\",  # Custom endpoint if needed \\n    default_headers={\"Custom-Header\": \"value\"} \\n) \\nChat Completions\\nBasic Chat Completion\\ndef basic_chat_completion(): \\n    response = client.chat.completions.create( \\n        model=\"gpt-4\", \\n        messages=[ \\n            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, \\n            {\"role\": \"user\", \"content\": \"Hello! How can you help me today?\"} \\n        ],\\n        max_tokens=150, \\n        temperature=0.7 \\n    )\\n     \\n    return response.choices[0].message.content \\nprint(basic_chat_completion()) \\nAdvanced Parameters'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:24:05+00:00', 'source': '../data/pdf_files/openai_api_tutorial.pdf', 'file_path': '../data/pdf_files/openai_api_tutorial.pdf', 'total_pages': 24, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:24:06+00:00', 'trapped': '', 'modDate': 'D:20250918142406Z', 'creationDate': \"D:20250918142405+00'00'\", 'page': 2}, page_content='def advanced_chat_completion(): \\n    response = client.chat.completions.create( \\n        model=\"gpt-4\", \\n        messages=[ \\n            {\"role\": \"system\", \"content\": \"You are an expert Python developer.\"}, \\n            {\"role\": \"user\", \"content\": \"Explain list comprehensions with examples.\"} \\n        ],\\n        max_tokens=500, \\n        temperature=0.3,           # Lower = more focused \\n        top_p=0.9,                # Nucleus sampling \\n        frequency_penalty=0.0,     # Reduce repetition \\n        presence_penalty=0.0,      # Encourage new topics \\n        stop=[\"\\\\n\\\\n\", \"###\"],      # Stop sequences \\n        seed=42                    # For reproducible outputs \\n    )\\n     \\n    return response.choices[0].message.content \\nStreaming Responses\\ndef streaming_chat(): \\n    stream = client.chat.completions.create( \\n        model=\"gpt-4\", \\n        messages=[{\"role\": \"user\", \"content\": \"Tell me a long story about AI\"}], \\n        stream=True \\n    )\\n     \\n    full_response = \"\" \\n    for chunk in stream: \\n        if chunk.choices[0].delta.content is not None: \\n            content = chunk.choices[0].delta.content \\n            print(content, end=\"\", flush=True) \\n            full_response += content \\n     \\n    return full_response \\nConversation Management'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:24:05+00:00', 'source': '../data/pdf_files/openai_api_tutorial.pdf', 'file_path': '../data/pdf_files/openai_api_tutorial.pdf', 'total_pages': 24, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:24:06+00:00', 'trapped': '', 'modDate': 'D:20250918142406Z', 'creationDate': \"D:20250918142405+00'00'\", 'page': 3}, page_content='class ChatManager: \\n    def __init__(self, system_message=\"You are a helpful assistant.\"): \\n        self.messages = [{\"role\": \"system\", \"content\": system_message}] \\n        self.client = OpenAI() \\n     \\n    def add_user_message(self, content): \\n        self.messages.append({\"role\": \"user\", \"content\": content}) \\n     \\n    def add_assistant_message(self, content): \\n        self.messages.append({\"role\": \"assistant\", \"content\": content}) \\n     \\n    def get_response(self, user_input): \\n        self.add_user_message(user_input) \\n         \\n        response = self.client.chat.completions.create( \\n            model=\"gpt-4\", \\n            messages=self.messages, \\n            max_tokens=500, \\n            temperature=0.7 \\n        ) \\n         \\n        assistant_message = response.choices[0].message.content \\n        self.add_assistant_message(assistant_message) \\n         \\n        return assistant_message \\n     \\n    def clear_history(self, keep_system=True): \\n        if keep_system and self.messages[0][\"role\"] == \"system\": \\n            self.messages = [self.messages[0]] \\n        else: \\n            self.messages = [] \\n# Usage \\nchat = ChatManager() \\nresponse1 = chat.get_response(\"What is machine learning?\") \\nresponse2 = chat.get_response(\"Can you give me an example?\") \\nText Generation\\nLegacy Completions (GPT-3.5-turbo-instruct)\\ndef text_completion(): \\n    response = client.completions.create( \\n        model=\"gpt-3.5-turbo-instruct\", \\n        prompt=\"Complete this sentence: The future of AI is\", \\n        max_tokens=100, \\n        temperature=0.8, \\n        stop=[\"\\\\n\"] \\n    )\\n     \\n    return response.choices[0].text.strip()'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:24:05+00:00', 'source': '../data/pdf_files/openai_api_tutorial.pdf', 'file_path': '../data/pdf_files/openai_api_tutorial.pdf', 'total_pages': 24, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:24:06+00:00', 'trapped': '', 'modDate': 'D:20250918142406Z', 'creationDate': \"D:20250918142405+00'00'\", 'page': 4}, page_content='Creative Writing Assistant\\ndef creative_writer(prompt, style=\"narrative\", length=\"medium\"): \\n    length_map = { \\n        \"short\": 200, \\n        \"medium\": 500, \\n        \"long\": 1000 \\n    }\\n     \\n    system_message = f\"\"\"You are a creative writing assistant specializing in {style} writing.  \\n    Create engaging, well-structured content that captures the reader\\'s attention.\"\"\" \\n     \\n    response = client.chat.completions.create( \\n        model=\"gpt-4\", \\n        messages=[ \\n            {\"role\": \"system\", \"content\": system_message}, \\n            {\"role\": \"user\", \"content\": f\"Write a {style} piece based on: {prompt}\"} \\n        ],\\n        max_tokens=length_map.get(length, 500), \\n        temperature=0.8, \\n        presence_penalty=0.1 \\n    )\\n     \\n    return response.choices[0].message.content \\nFunction Calling\\nBasic Function Calling'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:24:05+00:00', 'source': '../data/pdf_files/openai_api_tutorial.pdf', 'file_path': '../data/pdf_files/openai_api_tutorial.pdf', 'total_pages': 24, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:24:06+00:00', 'trapped': '', 'modDate': 'D:20250918142406Z', 'creationDate': \"D:20250918142405+00'00'\", 'page': 5}, page_content='import json \\ndef get_weather(location, unit=\"celsius\"): \\n    \"\"\"Simulate weather API call\"\"\" \\n    return { \\n        \"location\": location, \\n        \"temperature\": \"22\", \\n        \"unit\": unit, \\n        \"condition\": \"sunny\" \\n    }\\ndef function_calling_example(): \\n    # Define the function schema \\n    tools = [ \\n        { \\n            \"type\": \"function\", \\n            \"function\": { \\n                \"name\": \"get_weather\", \\n                \"description\": \"Get current weather for a location\", \\n                \"parameters\": { \\n                    \"type\": \"object\", \\n                    \"properties\": { \\n                        \"location\": { \\n                            \"type\": \"string\", \\n                            \"description\": \"City name\" \\n                        }, \\n                        \"unit\": { \\n                            \"type\": \"string\", \\n                            \"enum\": [\"celsius\", \"fahrenheit\"] \\n                        } \\n                    }, \\n                    \"required\": [\"location\"] \\n                } \\n            } \\n        } \\n    ]\\n     \\n    messages = [ \\n        {\"role\": \"user\", \"content\": \"What\\'s the weather like in Paris?\"} \\n    ]\\n     \\n    # First call - model decides to use function \\n    response = client.chat.completions.create( \\n        model=\"gpt-4\", \\n        messages=messages, \\n        tools=tools, \\n        tool_choice=\"auto\" \\n    )\\n     \\n    # Check if model wants to call a function \\n    if response.choices[0].message.tool_calls: \\n        tool_call = response.choices[0].message.tool_calls[0] \\n        function_name = tool_call.function.name \\n        function_args = json.loads(tool_call.function.arguments)'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:24:05+00:00', 'source': '../data/pdf_files/openai_api_tutorial.pdf', 'file_path': '../data/pdf_files/openai_api_tutorial.pdf', 'total_pages': 24, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:24:06+00:00', 'trapped': '', 'modDate': 'D:20250918142406Z', 'creationDate': \"D:20250918142405+00'00'\", 'page': 6}, page_content='# Execute the function \\n        if function_name == \"get_weather\": \\n            function_result = get_weather(**function_args) \\n             \\n            # Add function result to conversation \\n            messages.append(response.choices[0].message) \\n            messages.append({ \\n                \"role\": \"tool\", \\n                \"content\": json.dumps(function_result), \\n                \"tool_call_id\": tool_call.id \\n            }) \\n             \\n            # Get final response \\n            final_response = client.chat.completions.create( \\n                model=\"gpt-4\", \\n                messages=messages \\n            ) \\n             \\n            return final_response.choices[0].message.content \\n     \\n    return response.choices[0].message.content \\nMultiple Function Agent'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:24:05+00:00', 'source': '../data/pdf_files/openai_api_tutorial.pdf', 'file_path': '../data/pdf_files/openai_api_tutorial.pdf', 'total_pages': 24, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:24:06+00:00', 'trapped': '', 'modDate': 'D:20250918142406Z', 'creationDate': \"D:20250918142405+00'00'\", 'page': 7}, page_content='class FunctionAgent: \\n    def __init__(self): \\n        self.client = OpenAI() \\n        self.functions = { \\n            \"calculator\": self.calculate, \\n            \"search\": self.search, \\n            \"save_note\": self.save_note \\n        } \\n         \\n        self.tools = [ \\n            { \\n                \"type\": \"function\", \\n                \"function\": { \\n                    \"name\": \"calculator\", \\n                    \"description\": \"Perform mathematical calculations\", \\n                    \"parameters\": { \\n                        \"type\": \"object\", \\n                        \"properties\": { \\n                            \"expression\": {\"type\": \"string\", \"description\": \"Math expression\"} \\n                        }, \\n                        \"required\": [\"expression\"] \\n                    } \\n                } \\n            }, \\n            { \\n                \"type\": \"function\",  \\n                \"function\": { \\n                    \"name\": \"search\", \\n                    \"description\": \"Search for information\", \\n                    \"parameters\": { \\n                        \"type\": \"object\", \\n                        \"properties\": { \\n                            \"query\": {\"type\": \"string\", \"description\": \"Search query\"} \\n                        }, \\n                        \"required\": [\"query\"] \\n                    } \\n                } \\n            } \\n        ] \\n     \\n    def calculate(self, expression): \\n        try: \\n            result = eval(expression)  # Use safely in production! \\n            return f\"Result: {result}\" \\n        except: \\n            return \"Error in calculation\" \\n     \\n    def search(self, query): \\n        return f\"Search results for \\'{query}\\': [Simulated results]\" \\n     \\n    def save_note(self, content): \\n        # Simulate saving \\n        return f\"Note saved: {content[:50]}...\"'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:24:05+00:00', 'source': '../data/pdf_files/openai_api_tutorial.pdf', 'file_path': '../data/pdf_files/openai_api_tutorial.pdf', 'total_pages': 24, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:24:06+00:00', 'trapped': '', 'modDate': 'D:20250918142406Z', 'creationDate': \"D:20250918142405+00'00'\", 'page': 8}, page_content='def run(self, user_input): \\n        messages = [{\"role\": \"user\", \"content\": user_input}] \\n         \\n        response = self.client.chat.completions.create( \\n            model=\"gpt-4\", \\n            messages=messages, \\n            tools=self.tools, \\n            tool_choice=\"auto\" \\n        ) \\n         \\n        # Handle tool calls \\n        if response.choices[0].message.tool_calls: \\n            messages.append(response.choices[0].message) \\n             \\n            for tool_call in response.choices[0].message.tool_calls: \\n                function_name = tool_call.function.name \\n                function_args = json.loads(tool_call.function.arguments) \\n                 \\n                if function_name in self.functions: \\n                    result = self.functions[function_name](**function_args) \\n                    messages.append({ \\n                        \"role\": \"tool\", \\n                        \"content\": result, \\n                        \"tool_call_id\": tool_call.id \\n                    }) \\n             \\n            # Get final response \\n            final_response = self.client.chat.completions.create( \\n                model=\"gpt-4\", \\n                messages=messages \\n            ) \\n            return final_response.choices[0].message.content \\n         \\n        return response.choices[0].message.content \\n# Usage \\nagent = FunctionAgent() \\nresult = agent.run(\"What\\'s 25 * 47 + 123?\") \\nEmbeddings\\nBasic Embeddings'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:24:05+00:00', 'source': '../data/pdf_files/openai_api_tutorial.pdf', 'file_path': '../data/pdf_files/openai_api_tutorial.pdf', 'total_pages': 24, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:24:06+00:00', 'trapped': '', 'modDate': 'D:20250918142406Z', 'creationDate': \"D:20250918142405+00'00'\", 'page': 9}, page_content='def get_embeddings(texts): \\n    if isinstance(texts, str): \\n        texts = [texts] \\n     \\n    response = client.embeddings.create( \\n        model=\"text-embedding-3-large\",  # or text-embedding-3-small \\n        input=texts \\n    )\\n     \\n    return [embedding.embedding for embedding in response.data] \\n# Usage \\ntext = \"This is a sample sentence for embedding.\" \\nembeddings = get_embeddings(text) \\nprint(f\"Embedding dimension: {len(embeddings[0])}\") \\nSemantic Search System'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:24:05+00:00', 'source': '../data/pdf_files/openai_api_tutorial.pdf', 'file_path': '../data/pdf_files/openai_api_tutorial.pdf', 'total_pages': 24, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:24:06+00:00', 'trapped': '', 'modDate': 'D:20250918142406Z', 'creationDate': \"D:20250918142405+00'00'\", 'page': 10}, page_content='import numpy as np \\nfrom sklearn.metrics.pairwise import cosine_similarity \\nclass SemanticSearch: \\n    def __init__(self): \\n        self.client = OpenAI() \\n        self.documents = [] \\n        self.embeddings = [] \\n     \\n    def add_documents(self, docs): \\n        \"\"\"Add documents to search index\"\"\" \\n        self.documents.extend(docs) \\n         \\n        # Get embeddings for new documents \\n        response = self.client.embeddings.create( \\n            model=\"text-embedding-3-large\", \\n            input=docs \\n        ) \\n         \\n        new_embeddings = [emb.embedding for emb in response.data] \\n        self.embeddings.extend(new_embeddings) \\n     \\n    def search(self, query, top_k=5): \\n        \"\"\"Search for similar documents\"\"\" \\n        if not self.embeddings: \\n            return [] \\n         \\n        # Get query embedding \\n        query_response = self.client.embeddings.create( \\n            model=\"text-embedding-3-large\", \\n            input=[query] \\n        ) \\n        query_embedding = query_response.data[0].embedding \\n         \\n        # Calculate similarities \\n        similarities = cosine_similarity( \\n            [query_embedding],  \\n            self.embeddings \\n        )[0] \\n         \\n        # Get top results \\n        top_indices = np.argsort(similarities)[::-1][:top_k] \\n         \\n        results = [] \\n        for idx in top_indices: \\n            results.append({ \\n                \"document\": self.documents[idx], \\n                \"similarity\": similarities[idx] \\n            }) \\n         \\n        return results \\n# Usage \\nsearch_engine = SemanticSearch()'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:24:05+00:00', 'source': '../data/pdf_files/openai_api_tutorial.pdf', 'file_path': '../data/pdf_files/openai_api_tutorial.pdf', 'total_pages': 24, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:24:06+00:00', 'trapped': '', 'modDate': 'D:20250918142406Z', 'creationDate': \"D:20250918142405+00'00'\", 'page': 11}, page_content='search_engine.add_documents([ \\n    \"Python is a programming language\", \\n    \"Machine learning is a subset of AI\", \\n    \"Deep learning uses neural networks\", \\n    \"Natural language processing handles text\" \\n]) \\nresults = search_engine.search(\"What is AI?\", top_k=2) \\nfor result in results: \\n    print(f\"Similarity: {result[\\'similarity\\']:.3f}\") \\n    print(f\"Document: {result[\\'document\\']}\\\\n\") \\nVision Models\\nImage Analysis'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:24:05+00:00', 'source': '../data/pdf_files/openai_api_tutorial.pdf', 'file_path': '../data/pdf_files/openai_api_tutorial.pdf', 'total_pages': 24, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:24:06+00:00', 'trapped': '', 'modDate': 'D:20250918142406Z', 'creationDate': \"D:20250918142405+00'00'\", 'page': 12}, page_content='import base64 \\nimport requests \\ndef encode_image(image_path): \\n    \"\"\"Encode image to base64\"\"\" \\n    with open(image_path, \"rb\") as image_file: \\n        return base64.b64encode(image_file.read()).decode(\\'utf-8\\') \\ndef analyze_image(image_path, prompt=\"What\\'s in this image?\"): \\n    base64_image = encode_image(image_path) \\n     \\n    response = client.chat.completions.create( \\n        model=\"gpt-4-vision-preview\", \\n        messages=[ \\n            { \\n                \"role\": \"user\", \\n                \"content\": [ \\n                    {\"type\": \"text\", \"text\": prompt}, \\n                    { \\n                        \"type\": \"image_url\", \\n                        \"image_url\": { \\n                            \"url\": f\"data:image/jpeg;base64,{base64_image}\", \\n                            \"detail\": \"high\"  # or \"low\" for faster processing \\n                        } \\n                    } \\n                ] \\n            } \\n        ],\\n        max_tokens=300 \\n    )\\n     \\n    return response.choices[0].message.content \\n# Usage with URL \\ndef analyze_image_url(image_url, prompt=\"Describe this image\"): \\n    response = client.chat.completions.create( \\n        model=\"gpt-4-vision-preview\", \\n        messages=[ \\n            { \\n                \"role\": \"user\", \\n                \"content\": [ \\n                    {\"type\": \"text\", \"text\": prompt}, \\n                    {\"type\": \"image_url\", \"image_url\": {\"url\": image_url}} \\n                ] \\n            } \\n        ],\\n        max_tokens=300 \\n    )\\n     \\n    return response.choices[0].message.content \\nDocument OCR and Analysis'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:24:05+00:00', 'source': '../data/pdf_files/openai_api_tutorial.pdf', 'file_path': '../data/pdf_files/openai_api_tutorial.pdf', 'total_pages': 24, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:24:06+00:00', 'trapped': '', 'modDate': 'D:20250918142406Z', 'creationDate': \"D:20250918142405+00'00'\", 'page': 13}, page_content='def document_analyzer(image_path): \\n    \"\"\"Extract and analyze text from documents\"\"\" \\n    base64_image = encode_image(image_path) \\n     \\n    response = client.chat.completions.create( \\n        model=\"gpt-4-vision-preview\", \\n        messages=[ \\n            { \\n                \"role\": \"user\", \\n                \"content\": [ \\n                    { \\n                        \"type\": \"text\",  \\n                        \"text\": \"Extract all text from this document and provide a summary of its key points.\"\\n                    }, \\n                    { \\n                        \"type\": \"image_url\", \\n                        \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"} \\n                    } \\n                ] \\n            } \\n        ],\\n        max_tokens=1000 \\n    )\\n     \\n    return response.choices[0].message.content \\nAudio and Speech\\nSpeech-to-Text (Whisper)'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:24:05+00:00', 'source': '../data/pdf_files/openai_api_tutorial.pdf', 'file_path': '../data/pdf_files/openai_api_tutorial.pdf', 'total_pages': 24, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:24:06+00:00', 'trapped': '', 'modDate': 'D:20250918142406Z', 'creationDate': \"D:20250918142405+00'00'\", 'page': 14}, page_content='def transcribe_audio(audio_file_path): \\n    \"\"\"Transcribe audio file using Whisper\"\"\" \\n    with open(audio_file_path, \"rb\") as audio_file: \\n        transcription = client.audio.transcriptions.create( \\n            model=\"whisper-1\", \\n            file=audio_file, \\n            response_format=\"text\" \\n        ) \\n     \\n    return transcription \\ndef transcribe_with_timestamps(audio_file_path): \\n    \"\"\"Get transcription with timestamps\"\"\" \\n    with open(audio_file_path, \"rb\") as audio_file: \\n        transcription = client.audio.transcriptions.create( \\n            model=\"whisper-1\", \\n            file=audio_file, \\n            response_format=\"verbose_json\", \\n            timestamp_granularities=[\"word\"] \\n        ) \\n     \\n    return transcription \\n# Translation \\ndef translate_audio(audio_file_path): \\n    \"\"\"Translate foreign language audio to English\"\"\" \\n    with open(audio_file_path, \"rb\") as audio_file: \\n        translation = client.audio.translations.create( \\n            model=\"whisper-1\", \\n            file=audio_file \\n        ) \\n     \\n    return translation.text \\nText-to-Speech'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:24:05+00:00', 'source': '../data/pdf_files/openai_api_tutorial.pdf', 'file_path': '../data/pdf_files/openai_api_tutorial.pdf', 'total_pages': 24, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:24:06+00:00', 'trapped': '', 'modDate': 'D:20250918142406Z', 'creationDate': \"D:20250918142405+00'00'\", 'page': 15}, page_content='def text_to_speech(text, voice=\"alloy\", output_file=\"speech.mp3\"): \\n    \"\"\"Convert text to speech\"\"\" \\n    response = client.audio.speech.create( \\n        model=\"tts-1\",  # or \"tts-1-hd\" for higher quality \\n        voice=voice,    # alloy, echo, fable, onyx, nova, shimmer \\n        input=text, \\n        speed=1.0       # 0.25 to 4.0 \\n    )\\n     \\n    with open(output_file, \"wb\") as f: \\n        for chunk in response.iter_bytes(): \\n            f.write(chunk) \\n     \\n    return output_file \\n# Real-time streaming \\ndef streaming_text_to_speech(text, voice=\"alloy\"): \\n    \"\"\"Stream audio in real-time\"\"\" \\n    response = client.audio.speech.create( \\n        model=\"tts-1\", \\n        voice=voice, \\n        input=text, \\n        response_format=\"opus\"  # Better for streaming \\n    )\\n     \\n    # Play audio chunks as they arrive \\n    for chunk in response.iter_bytes(chunk_size=1024): \\n        # Send to audio player \\n        yield chunk \\nFine-tuning\\nPrepare Training Data'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:24:05+00:00', 'source': '../data/pdf_files/openai_api_tutorial.pdf', 'file_path': '../data/pdf_files/openai_api_tutorial.pdf', 'total_pages': 24, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:24:06+00:00', 'trapped': '', 'modDate': 'D:20250918142406Z', 'creationDate': \"D:20250918142405+00'00'\", 'page': 16}, page_content='import json \\ndef prepare_training_data(examples): \\n    \"\"\"Prepare data for fine-tuning\"\"\" \\n    training_data = [] \\n     \\n    for example in examples: \\n        training_data.append({ \\n            \"messages\": [ \\n                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, \\n                {\"role\": \"user\", \"content\": example[\"input\"]}, \\n                {\"role\": \"assistant\", \"content\": example[\"output\"]} \\n            ] \\n        })\\n     \\n    # Save to JSONL file \\n    with open(\"training_data.jsonl\", \"w\") as f: \\n        for item in training_data: \\n            f.write(json.dumps(item) + \"\\\\n\") \\n     \\n    return \"training_data.jsonl\" \\n# Example data \\nexamples = [ \\n    {\"input\": \"What is Python?\", \"output\": \"Python is a programming language...\"}, \\n    {\"input\": \"How do lists work?\", \"output\": \"Lists in Python are ordered collections...\"} \\n] \\ntraining_file = prepare_training_data(examples) \\nFine-tuning Process'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:24:05+00:00', 'source': '../data/pdf_files/openai_api_tutorial.pdf', 'file_path': '../data/pdf_files/openai_api_tutorial.pdf', 'total_pages': 24, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:24:06+00:00', 'trapped': '', 'modDate': 'D:20250918142406Z', 'creationDate': \"D:20250918142405+00'00'\", 'page': 17}, page_content='def create_fine_tuning_job(training_file, model=\"gpt-3.5-turbo\"): \\n    \"\"\"Create a fine-tuning job\"\"\" \\n    # Upload training file \\n    with open(training_file, \"rb\") as f: \\n        file_response = client.files.create( \\n            file=f, \\n            purpose=\"fine-tune\" \\n        ) \\n     \\n    # Create fine-tuning job \\n    job = client.fine_tuning.jobs.create( \\n        training_file=file_response.id, \\n        model=model, \\n        hyperparameters={ \\n            \"n_epochs\": 3, \\n            \"batch_size\": 1, \\n            \"learning_rate_multiplier\": 2 \\n        } \\n    )\\n     \\n    return job \\ndef monitor_fine_tuning(job_id): \\n    \"\"\"Monitor fine-tuning progress\"\"\" \\n    job = client.fine_tuning.jobs.retrieve(job_id) \\n     \\n    print(f\"Job ID: {job.id}\") \\n    print(f\"Status: {job.status}\") \\n    print(f\"Model: {job.fine_tuned_model}\") \\n     \\n    # Get events \\n    events = client.fine_tuning.jobs.list_events(job_id) \\n    for event in events.data[:5]:  # Show last 5 events \\n        print(f\"{event.created_at}: {event.message}\") \\n     \\n    return job \\ndef use_fine_tuned_model(model_id, prompt): \\n    \"\"\"Use your fine-tuned model\"\"\" \\n    response = client.chat.completions.create( \\n        model=model_id, \\n        messages=[ \\n            {\"role\": \"user\", \"content\": prompt} \\n        ] \\n    )\\n     \\n    return response.choices[0].message.content \\nBest Practices\\nError Handling'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:24:05+00:00', 'source': '../data/pdf_files/openai_api_tutorial.pdf', 'file_path': '../data/pdf_files/openai_api_tutorial.pdf', 'total_pages': 24, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:24:06+00:00', 'trapped': '', 'modDate': 'D:20250918142406Z', 'creationDate': \"D:20250918142405+00'00'\", 'page': 18}, page_content='from openai import RateLimitError, APIError \\nimport time \\ndef robust_api_call(func, max_retries=3, backoff_factor=2): \\n    \"\"\"Robust API call with retry logic\"\"\" \\n    for attempt in range(max_retries): \\n        try: \\n            return func() \\n        except RateLimitError: \\n            if attempt == max_retries - 1: \\n                raise \\n            wait_time = backoff_factor ** attempt \\n            print(f\"Rate limit hit, waiting {wait_time} seconds...\") \\n            time.sleep(wait_time) \\n        except APIError as e: \\n            print(f\"API Error: {e}\") \\n            if attempt == max_retries - 1: \\n                raise \\n            time.sleep(backoff_factor ** attempt) \\n# Usage \\ndef safe_chat_completion(message): \\n    return robust_api_call( \\n        lambda: client.chat.completions.create( \\n            model=\"gpt-4\", \\n            messages=[{\"role\": \"user\", \"content\": message}] \\n        ) \\n    )\\nToken Management'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:24:05+00:00', 'source': '../data/pdf_files/openai_api_tutorial.pdf', 'file_path': '../data/pdf_files/openai_api_tutorial.pdf', 'total_pages': 24, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:24:06+00:00', 'trapped': '', 'modDate': 'D:20250918142406Z', 'creationDate': \"D:20250918142405+00'00'\", 'page': 19}, page_content='import tiktoken \\ndef count_tokens(text, model=\"gpt-4\"): \\n    \"\"\"Count tokens in text\"\"\" \\n    encoding = tiktoken.encoding_for_model(model) \\n    return len(encoding.encode(text)) \\ndef truncate_text(text, max_tokens, model=\"gpt-4\"): \\n    \"\"\"Truncate text to fit within token limit\"\"\" \\n    encoding = tiktoken.encoding_for_model(model) \\n    tokens = encoding.encode(text) \\n     \\n    if len(tokens) <= max_tokens: \\n        return text \\n     \\n    truncated_tokens = tokens[:max_tokens] \\n    return encoding.decode(truncated_tokens) \\ndef smart_chunking(text, chunk_size=1000, model=\"gpt-4\"): \\n    \"\"\"Split text into chunks based on token count\"\"\" \\n    encoding = tiktoken.encoding_for_model(model) \\n    tokens = encoding.encode(text) \\n     \\n    chunks = [] \\n    for i in range(0, len(tokens), chunk_size): \\n        chunk_tokens = tokens[i:i + chunk_size] \\n        chunk_text = encoding.decode(chunk_tokens) \\n        chunks.append(chunk_text) \\n     \\n    return chunks \\nCost Optimization'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:24:05+00:00', 'source': '../data/pdf_files/openai_api_tutorial.pdf', 'file_path': '../data/pdf_files/openai_api_tutorial.pdf', 'total_pages': 24, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:24:06+00:00', 'trapped': '', 'modDate': 'D:20250918142406Z', 'creationDate': \"D:20250918142405+00'00'\", 'page': 20}, page_content='class CostTracker: \\n    def __init__(self): \\n        self.costs = { \\n            \"gpt-4\": {\"input\": 0.03, \"output\": 0.06},  # per 1K tokens \\n            \"gpt-3.5-turbo\": {\"input\": 0.001, \"output\": 0.002}, \\n            \"text-embedding-3-large\": {\"input\": 0.00013, \"output\": 0} \\n        } \\n        self.total_cost = 0 \\n     \\n    def calculate_cost(self, model, input_tokens, output_tokens): \\n        if model in self.costs: \\n            cost = ( \\n                (input_tokens / 1000) * self.costs[model][\"input\"] + \\n                (output_tokens / 1000) * self.costs[model][\"output\"] \\n            ) \\n            self.total_cost += cost \\n            return cost \\n        return 0 \\n     \\n    def tracked_completion(self, **kwargs): \\n        response = client.chat.completions.create(**kwargs) \\n         \\n        usage = response.usage \\n        cost = self.calculate_cost( \\n            kwargs[\"model\"], \\n            usage.prompt_tokens, \\n            usage.completion_tokens \\n        ) \\n         \\n        print(f\"Cost: ${cost:.4f} | Total: ${self.total_cost:.4f}\") \\n        return response \\n# Usage \\ntracker = CostTracker() \\nresponse = tracker.tracked_completion( \\n    model=\"gpt-4\", \\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}] \\n) \\nAsync Operations'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:24:05+00:00', 'source': '../data/pdf_files/openai_api_tutorial.pdf', 'file_path': '../data/pdf_files/openai_api_tutorial.pdf', 'total_pages': 24, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:24:06+00:00', 'trapped': '', 'modDate': 'D:20250918142406Z', 'creationDate': \"D:20250918142405+00'00'\", 'page': 21}, page_content='import asyncio \\nfrom openai import AsyncOpenAI \\nasync_client = AsyncOpenAI() \\nasync def async_chat_completion(message): \\n    \"\"\"Async chat completion\"\"\" \\n    response = await async_client.chat.completions.create( \\n        model=\"gpt-4\", \\n        messages=[{\"role\": \"user\", \"content\": message}] \\n    )\\n    return response.choices[0].message.content \\nasync def batch_completions(messages): \\n    \"\"\"Process multiple completions concurrently\"\"\" \\n    tasks = [async_chat_completion(msg) for msg in messages] \\n    results = await asyncio.gather(*tasks) \\n    return results \\n# Usage \\nasync def main(): \\n    messages = [ \\n        \"What is Python?\", \\n        \"What is JavaScript?\", \\n        \"What is Rust?\" \\n    ]\\n     \\n    results = await batch_completions(messages) \\n    for i, result in enumerate(results): \\n        print(f\"Question {i+1}: {result[:100]}...\") \\n# Run\\n# asyncio.run(main()) \\nProduction Configuration'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:24:05+00:00', 'source': '../data/pdf_files/openai_api_tutorial.pdf', 'file_path': '../data/pdf_files/openai_api_tutorial.pdf', 'total_pages': 24, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:24:06+00:00', 'trapped': '', 'modDate': 'D:20250918142406Z', 'creationDate': \"D:20250918142405+00'00'\", 'page': 22}, page_content='class ProductionOpenAI: \\n    def __init__(self, api_key=None): \\n        self.client = OpenAI( \\n            api_key=api_key or os.getenv(\"OPENAI_API_KEY\"), \\n            timeout=30, \\n            max_retries=3 \\n        ) \\n        self.default_params = { \\n            \"temperature\": 0.7, \\n            \"max_tokens\": 1000, \\n            \"top_p\": 0.9 \\n        } \\n     \\n    def chat(self, messages, **kwargs): \\n        params = {**self.default_params, **kwargs} \\n         \\n        try: \\n            response = self.client.chat.completions.create( \\n                messages=messages, \\n                **params \\n            ) \\n            return { \\n                \"success\": True, \\n                \"content\": response.choices[0].message.content, \\n                \"usage\": response.usage, \\n                \"model\": response.model \\n            } \\n        except Exception as e: \\n            return { \\n                \"success\": False, \\n                \"error\": str(e), \\n                \"content\": None \\n            } \\nConclusion\\nThe OpenAI API provides powerful capabilities for building AI-powered applications. This tutorial covered the essential patterns and best practices for:\\nChat completions and conversation management\\nFunction calling for tool integration\\nEmbeddings for semantic search\\nVision capabilities for image analysis\\nAudio processing with Whisper and TTS\\nFine-tuning for specialized models\\nProduction-ready error handling and optimization\\nNext Steps:\\n1. Experiment with different models and parameters\\n2. Build a complete application using multiple API features\\n3. Implement proper monitoring and cost tracking\\n4. Explore advanced techniques like RAG and agent frameworks'), Document(metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:24:05+00:00', 'source': '../data/pdf_files/openai_api_tutorial.pdf', 'file_path': '../data/pdf_files/openai_api_tutorial.pdf', 'total_pages': 24, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:24:06+00:00', 'trapped': '', 'modDate': 'D:20250918142406Z', 'creationDate': \"D:20250918142405+00'00'\", 'page': 23}, page_content='Additional Resources:\\nOpenAI API Documentation (https://platform.openai.com/docs)\\nOpenAI Cookbook (https://github.com/openai/openai-cookbook)\\nBest Practices Guide (https://platform.openai.com/docs/guides/production-best-practices)')]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "# Example usage:\n",
    "loader = DirectoryLoader(\"../data/pdf_files/\", loader_cls=PyMuPDFLoader, show_progress=False, glob=\"*.pdf\")\n",
    "documents = loader.load()\n",
    "print(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bbdd821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Agentic AI Tutorial: Building Intelligent\n",
      "Autonomous Systems\n",
      "Table of Contents\n",
      "1. Introduction to Agentic AI\n",
      "2. Core Concepts and Architecture\n",
      "3. Agent Frameworks\n",
      "4. Planning and Reasoning\n",
      "5. Memory and Knowledge Management\n",
      "6. Tool Use and Actions\n",
      "7. Multi-Agent Systems\n",
      "8. Real-World Applications\n",
      "9. Implementation Guide\n",
      "10. Best Practices\n",
      "Introduction to Agentic AI\n",
      "Agentic AI refers to AI systems that can act autonomously to achieve goals, make decisions, and interact with their environment. Unlike traditional AI that responds to\n",
      "prompts, agentic systems proactively plan, execute, and adapt their behavior.\n",
      "Key Characteristics:\n",
      "Autonomy: Can operate without constant human supervision\n",
      "Goal-oriented: Pursues objectives over multiple steps\n",
      "Adaptive: Learns from experience and adjusts strategies\n",
      "Interactive: Communicates with humans and other systems\n",
      "Persistent: Maintains context and memory across sessions\n",
      "Agent Types:\n",
      "Reactive Agents: Respond to immediate stimuli\n",
      "Deliberative Agents: Plan before acting\n",
      "Hybrid Agents: Combine reactive and deliberative approaches\n",
      "Learning Agents: Improve performance over time\n",
      "Core Concepts and Architecture\n",
      "Agent Architecture Components' metadata={'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:04+00:00', 'source': '../data/pdf_files/agentic_ai_tutorial.pdf', 'file_path': '../data/pdf_files/agentic_ai_tutorial.pdf', 'total_pages': 74, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:22:04+00:00', 'trapped': '', 'modDate': 'D:20250918142204Z', 'creationDate': \"D:20250918142204+00'00'\", 'page': 0}\n"
     ]
    }
   ],
   "source": [
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fbf980f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def split_documents(documents,chunk_size=1000,chunk_overlap=200):\n",
    "    \"\"\"Split documents into smaller chunks for better RAG performance\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    split_docs = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(split_docs)} chunks\")\n",
    "    \n",
    "    # Show example of a chunk\n",
    "    if split_docs:\n",
    "        print(f\"\\nExample chunk:\")\n",
    "        print(f\"Content: {split_docs[0].page_content[:200]}...\")\n",
    "        print(f\"Metadata: {split_docs[0].metadata}\")\n",
    "    \n",
    "    return split_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08003fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 167 documents into 316 chunks\n",
      "\n",
      "Example chunk:\n",
      "Content: Agentic AI Tutorial: Building Intelligent\n",
      "Autonomous Systems\n",
      "Table of Contents\n",
      "1. Introduction to Agentic AI\n",
      "2. Core Concepts and Architecture\n",
      "3. Agent Frameworks\n",
      "4. Planning and Reasoning\n",
      "5. Memory a...\n",
      "Metadata: {'producer': 'PDFCreator Online (www.pdfforge.org/online)', 'creator': 'PDFCreator Online (www.pdfforge.org/online)', 'creationdate': '2025-09-18T14:22:04+00:00', 'source': '../data/pdf_files/agentic_ai_tutorial.pdf', 'file_path': '../data/pdf_files/agentic_ai_tutorial.pdf', 'total_pages': 74, 'format': 'PDF 1.4', 'title': 'Merged with PDFCreator Online', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-18T14:22:04+00:00', 'trapped': '', 'modDate': 'D:20250918142204Z', 'creationDate': \"D:20250918142204+00'00'\", 'page': 0}\n"
     ]
    }
   ],
   "source": [
    "chunks = split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "babcb23b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: all-MiniLM-L6-v2\n",
      "Model loaded successfully. Embedding dimension: 384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.EmbeddingManager at 0x33be989e0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "class EmbeddingManager:\n",
    "    \"\"\"Handles document embedding generation using SentenceTransformer\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        \"\"\"\n",
    "        Initialize the embedding manager\n",
    "        \n",
    "        Args:\n",
    "            model_name: HuggingFace model name for sentence embeddings\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self._load_model()\n",
    "\n",
    "    def _load_model(self):\n",
    "        \"\"\"Load the SentenceTransformer model\"\"\"\n",
    "        try:\n",
    "            print(f\"Loading embedding model: {self.model_name}\")\n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "            print(f\"Model loaded successfully. Embedding dimension: {self.model.get_sentence_embedding_dimension()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model {self.model_name}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def generate_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate embeddings for a list of texts\n",
    "        \n",
    "        Args:\n",
    "            texts: List of text strings to embed\n",
    "            \n",
    "        Returns:\n",
    "            numpy array of embeddings with shape (len(texts), embedding_dim)\n",
    "        \"\"\"\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model not loaded\")\n",
    "        \n",
    "        print(f\"Generating embeddings for {len(texts)} texts...\")\n",
    "        embeddings = self.model.encode(texts, show_progress_bar=True)\n",
    "        print(f\"Generated embeddings with shape: {embeddings.shape}\")\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "## initialize the embedding manager\n",
    "\n",
    "embedding_manager=EmbeddingManager()\n",
    "embedding_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb56624b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store initialized. Collection: pdf_documents\n",
      "Existing documents in collection: 1896\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.VectorStore at 0x33d0b5d00>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import chromadb\n",
    "import uuid\n",
    "\n",
    "class VectorStore:\n",
    "    \"\"\"Manages document embeddings in a ChromaDB vector store\"\"\"\n",
    "    \n",
    "    def __init__(self, collection_name: str = \"pdf_documents\", persist_directory: str = \"../data/vector_store\"):\n",
    "        \"\"\"\n",
    "        Initialize the vector store\n",
    "        \n",
    "        Args:\n",
    "            collection_name: Name of the ChromaDB collection\n",
    "            persist_directory: Directory to persist the vector store\n",
    "        \"\"\"\n",
    "        self.collection_name = collection_name\n",
    "        self.persist_directory = persist_directory\n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "        self._initialize_store()\n",
    "\n",
    "    def _initialize_store(self):\n",
    "        \"\"\"Initialize ChromaDB client and collection\"\"\"\n",
    "        try:\n",
    "            # Create persistent ChromaDB client\n",
    "            os.makedirs(self.persist_directory, exist_ok=True)\n",
    "            self.client = chromadb.PersistentClient(path=self.persist_directory)\n",
    "            \n",
    "            # Get or create collection\n",
    "            self.collection = self.client.get_or_create_collection(\n",
    "                name=self.collection_name,\n",
    "                metadata={\"description\": \"PDF document embeddings for RAG\"}\n",
    "            )\n",
    "            print(f\"Vector store initialized. Collection: {self.collection_name}\")\n",
    "            print(f\"Existing documents in collection: {self.collection.count()}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "    def add_documents(self, documents: List[Any], embeddings: np.ndarray):\n",
    "        \"\"\"\n",
    "        Add documents and their embeddings to the vector store\n",
    "        \n",
    "        Args:\n",
    "            documents: List of LangChain documents\n",
    "            embeddings: Corresponding embeddings for the documents\n",
    "        \"\"\"\n",
    "        if len(documents) != len(embeddings):\n",
    "            raise ValueError(\"Number of documents must match number of embeddings\")\n",
    "        \n",
    "        print(f\"Adding {len(documents)} documents to vector store...\")\n",
    "        \n",
    "        # Prepare data for ChromaDB\n",
    "        ids = []\n",
    "        metadatas = []\n",
    "        documents_text = []\n",
    "        embeddings_list = []\n",
    "        \n",
    "        for i, (doc, embedding) in enumerate(zip(documents, embeddings)):\n",
    "            # Generate unique ID\n",
    "            doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
    "            ids.append(doc_id)\n",
    "            \n",
    "            # Prepare metadata\n",
    "            metadata = dict(doc.metadata)\n",
    "            metadata['doc_index'] = i\n",
    "            metadata['content_length'] = len(doc.page_content)\n",
    "            metadatas.append(metadata)\n",
    "            \n",
    "            # Document content\n",
    "            documents_text.append(doc.page_content)\n",
    "            \n",
    "            # Embedding\n",
    "            embeddings_list.append(embedding.tolist())\n",
    "        \n",
    "        # Add to collection\n",
    "        try:\n",
    "            self.collection.add(\n",
    "                ids=ids,\n",
    "                embeddings=embeddings_list,\n",
    "                metadatas=metadatas,\n",
    "                documents=documents_text \n",
    "            )\n",
    "            print(f\"Successfully added {len(documents)} documents to vector store\")\n",
    "            print(f\"Total documents in collection: {self.collection.count()}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error adding documents to vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "vectorstore=VectorStore()\n",
    "vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df1f7cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for 316 texts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db035c2e51494ac98e89dd16275f5ba9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (316, 384)\n",
      "Adding 316 documents to vector store...\n",
      "Successfully added 316 documents to vector store\n",
      "Total documents in collection: 2212\n"
     ]
    }
   ],
   "source": [
    "texts=[doc.page_content for doc in chunks]\n",
    "\n",
    "embeddings = embedding_manager.generate_embeddings(texts)\n",
    "\n",
    "vectorstore.add_documents(chunks, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66dbc0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGRetriever:\n",
    "    \"\"\"Handles query-based retrieval from the vector store\"\"\"\n",
    "    \n",
    "    def __init__(self, vector_store: VectorStore, embedding_manager: EmbeddingManager):\n",
    "        \"\"\"\n",
    "        Initialize the retriever\n",
    "        \n",
    "        Args:\n",
    "            vector_store: Vector store containing document embeddings\n",
    "            embedding_manager: Manager for generating query embeddings\n",
    "        \"\"\"\n",
    "        self.vector_store = vector_store\n",
    "        self.embedding_manager = embedding_manager\n",
    "\n",
    "    def retrieve(self, query: str, top_k: int = 5, score_threshold: float = 0.0) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Retrieve relevant documents for a query\n",
    "        \n",
    "        Args:\n",
    "            query: The search query\n",
    "            top_k: Number of top results to return\n",
    "            score_threshold: Minimum similarity score threshold\n",
    "            \n",
    "        Returns:\n",
    "            List of dictionaries containing retrieved documents and metadata\n",
    "        \"\"\"\n",
    "        print(f\"Retrieving documents for query: '{query}'\")\n",
    "        print(f\"Top K: {top_k}, Score threshold: {score_threshold}\")\n",
    "        \n",
    "        # Generate query embedding\n",
    "        query_embedding = self.embedding_manager.generate_embeddings([query])\n",
    "        \n",
    "        # Search in vector store\n",
    "        try:\n",
    "            results = self.vector_store.collection.query(\n",
    "                query_embeddings=query_embedding.tolist(),\n",
    "                n_results=top_k\n",
    "            )\n",
    "            \n",
    "            # Process results\n",
    "            retrieved_docs = []\n",
    "            \n",
    "            if results['documents'] and results['documents'][0]:\n",
    "                documents = results['documents'][0]\n",
    "                metadatas = results['metadatas'][0]\n",
    "                distances = results['distances'][0]\n",
    "                ids = results['ids'][0]\n",
    "                \n",
    "                for i, (doc_id, document, metadata, distance) in enumerate(zip(ids, documents, metadatas, distances)):\n",
    "                    # Convert distance to similarity score (ChromaDB uses cosine distance)\n",
    "                    similarity_score = 1 - distance\n",
    "                    \n",
    "                    if similarity_score >= score_threshold:\n",
    "                        retrieved_docs.append({\n",
    "                            'id': doc_id,\n",
    "                            'content': document,\n",
    "                            'metadata': metadata,\n",
    "                            'similarity_score': similarity_score,\n",
    "                            'distance': distance,\n",
    "                            'rank': i + 1\n",
    "                        })\n",
    "                \n",
    "                print(f\"Retrieved {len(retrieved_docs)} documents (after filtering)\")\n",
    "            else:\n",
    "                print(\"No documents found\")\n",
    "            \n",
    "            return retrieved_docs\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during retrieval: {e}\")\n",
    "            return []\n",
    "\n",
    "rag_retriever=RAGRetriever(vectorstore,embedding_manager)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69ada310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'What is agentic AI?'\n",
      "Top K: 5, Score threshold: 0.0\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50cdef1b20da4307a5a8dfeb9c2c9a65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 5 documents (after filtering)\n",
      "Retrieval successful!\n",
      "Found 5 relevant documents\n",
      "Top result similarity: 0.3206\n",
      "Content preview: Agentic AI Tutorial: Building Intelligent\n",
      "Autonomous Systems\n",
      "Table of Contents\n",
      "1. Introduction to Agentic AI\n",
      "2. Core Concepts and Architecture\n",
      "3. Agent Frameworks\n",
      "4. Planning and Reasoning\n",
      "5. Memory and Knowledge Management\n",
      "6. Tool Use and Actions\n",
      "7. Multi-Agent Systems\n",
      "8. Real-World Applications\n",
      "9. Implementation Guide\n",
      "10. Best Practices\n",
      "Introduction to Agentic AI\n",
      "Agentic AI refers to AI systems that can act autonomously to achieve goals, make decisions, and interact with their environment. Unlike traditional AI that responds to\n",
      "prompts, agentic systems proactively plan, execute, and adapt their behavior.\n",
      "Key Characteristics:\n",
      "Autonomy: Can operate without constant human supervision\n",
      "Goal-oriented: Pursues objectives over multiple steps\n",
      "Adaptive: Learns from experience and adjusts strategies\n",
      "Interactive: Communicates with humans and other systems\n",
      "Persistent: Maintains context and memory across sessions\n",
      "Agent Types:\n",
      "Reactive Agents: Respond to immediate stimuli...\n"
     ]
    }
   ],
   "source": [
    "results = rag_retriever.retrieve(\"What is agentic AI?\")\n",
    "print(\"Retrieval successful!\")\n",
    "if results:\n",
    "    print(f\"Found {len(results)} relevant documents\")\n",
    "    print(f\"Top result similarity: {results[0]['similarity_score']:.4f}\")\n",
    "    print(f\"Content preview: {results[0]['content']}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6296ab3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from agents import Agent, Runner, trace, function_tool\n",
    "import asyncio\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b3c3404e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@function_tool\n",
    "def retrieve_with_rag_retriever(query: str):\n",
    "    \"\"\"Retrieve relevant documents using rag_retriever.\"\"\"\n",
    "    return rag_retriever.retrieve(query)\n",
    "\n",
    "instructions = \"Use the retrieve_with_rag_retriever tool to answer the user's question using retrieved data.\"\n",
    "rag_agent = Agent(\n",
    "    name=\"RAG-Agent\",\n",
    "    tools=[retrieve_with_rag_retriever],\n",
    "    instructions=instructions,\n",
    "    model=\"gpt-4o-mini\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "280fe3b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'What is agentic AI?'\n",
      "Top K: 5, Score threshold: 0.0\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90c5202fc0f94794b71ce738b59589e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 5 documents (after filtering)\n",
      "RunResult:\n",
      "- Last agent: Agent(name=\"RAG-Agent\", ...)\n",
      "- Final output (str):\n",
      "    Agentic AI refers to AI systems designed to act autonomously, achieving goals, making decisions, and interacting with their environments. Unlike traditional AI, which typically responds to user prompts, agentic systems can proactively plan, execute tasks, and adapt their behaviors based on experiences.\n",
      "    \n",
      "    ### Key Characteristics of Agentic AI:\n",
      "    - **Autonomy**: Functions independently without constant human oversight.\n",
      "    - **Goal-Oriented**: Pursues specific objectives through a series of steps.\n",
      "    - **Adaptive**: Learns and adjusts strategies based on experiences.\n",
      "    - **Interactive**: Capable of communication with humans and other systems.\n",
      "    - **Persistent**: Maintains context and memory across sessions.\n",
      "    \n",
      "    ### Agent Types:\n",
      "    1. **Reactive Agents**: Respond immediately to stimuli.\n",
      "    \n",
      "    Overall, agentic AI represents a shift towards more intelligent and self-sufficient autonomous systems in various applications.\n",
      "- 3 new item(s)\n",
      "- 2 raw response(s)\n",
      "- 0 input guardrail result(s)\n",
      "- 0 output guardrail result(s)\n",
      "(See `RunResult` for more details)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "async def run_rag_agent(query:str):\n",
    "    with trace(\"rag_agent\"):\n",
    "        response = await Runner.run(rag_agent, query)\n",
    "        return response\n",
    "\n",
    "response = await run_rag_agent(\"What is agentic AI?\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "2ebc519d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'what is agentic AI?'\n",
      "Top K: 5, Score threshold: 0.0\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ea89c79775044d6bf067408511f5ef4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 5 documents (after filtering)\n",
      "Retrieving documents for query: 'how to install langgraph'\n",
      "Top K: 5, Score threshold: 0.0\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3be5c7bffcac483cabc22f62574316f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 0 documents (after filtering)\n",
      "Retrieving documents for query: 'What all information do you have?'\n",
      "Top K: 5, Score threshold: 0.0\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f41daa7bf4847fc8a962e3286842a26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 0 documents (after filtering)\n",
      "Retrieving documents for query: 'What is Hugging Face?'\n",
      "Top K: 5, Score threshold: 0.0\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "511a699ea8cd45509848f9311eb1c044",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 0 documents (after filtering)\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def gradio_rag_agent(query):\n",
    "    # Use asyncio.run to avoid event loop issues in worker threads\n",
    "    return asyncio.run(run_rag_agent(query))\n",
    "\n",
    "with gr.Blocks() as iface:\n",
    "    gr.Markdown(\"# 🦙 RAG Agent Demo\")\n",
    "    gr.Markdown(\"Ask a question and get an answer using the RAG agent. Try something like: **What is agentic AI?**\")\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            query = gr.Textbox(lines=3, label=\"Ask a question\", placeholder=\"Type your question here...\")\n",
    "            submit_btn = gr.Button(\"Submit\", variant=\"primary\")\n",
    "        with gr.Column():\n",
    "            output = gr.Textbox(label=\"RAG Agent Response\", lines=8, interactive=False)\n",
    "    submit_btn.click(fn=gradio_rag_agent, inputs=query, outputs=output)\n",
    "\n",
    "iface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963b4a12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfb91f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
