{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cb3f9e2",
   "metadata": {},
   "source": [
    "# Data Ingestion Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b8b2758",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader \n",
    "\n",
    "loader = TextLoader(\"../data/text_files/llm_learning.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b889d58b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '../data/text_files/llm_learning.txt'}, page_content=\"Large Language Models (LLMs) are advanced artificial intelligence systems designed to understand and generate human-like text. They are trained on vast amounts of textual data and can perform a variety of tasks such as answering questions, summarizing content, translating languages, and even writing code.\\n\\n### Key Concepts\\n\\n1. **Training Data**: LLMs learn from large datasets containing books, articles, websites, and more. The quality and diversity of this data influence the model's capabilities.\\n\\n2. **Tokens**: LLMs process text as sequences of tokens, which are often words or subwords. Understanding tokenization is important for working with LLMs.\\n\\n3. **Prompting**: You interact with an LLM by providing a prompt—a piece of text or a question. The model generates a response based on this input.\\n\\n4. **Fine-tuning**: While base LLMs are trained on general data, they can be fine-tuned on specific datasets to specialize in certain tasks or domains.\\n\\n### Common Use Cases\\n\\n- **Chatbots and Virtual Assistants**\\n- **Content Generation**\\n- **Text Summarization**\\n- **Code Generation**\\n- **Question Answering**\\n\\n### Getting Started\\n\\nTo use an LLM, you typically need access to an API or a library such as OpenAI's GPT, Google's PaLM, or open-source models like Llama or Falcon. You can interact with these models using Python libraries such as `langchain`.\\n\\nExample (pseudo-code):\\n\")]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document = loader.load()\n",
    "document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "77d3d4cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': '../data/text_files/llm_learning.txt'}, page_content=\"Large Language Models (LLMs) are advanced artificial intelligence systems designed to understand and generate human-like text. They are trained on vast amounts of textual data and can perform a variety of tasks such as answering questions, summarizing content, translating languages, and even writing code.\\n\\n### Key Concepts\\n\\n1. **Training Data**: LLMs learn from large datasets containing books, articles, websites, and more. The quality and diversity of this data influence the model's capabilities.\\n\\n2. **Tokens**: LLMs process text as sequences of tokens, which are often words or subwords. Understanding tokenization is important for working with LLMs.\\n\\n3. **Prompting**: You interact with an LLM by providing a prompt—a piece of text or a question. The model generates a response based on this input.\\n\\n4. **Fine-tuning**: While base LLMs are trained on general data, they can be fine-tuned on specific datasets to specialize in certain tasks or domains.\\n\\n### Common Use Cases\\n\\n- **Chatbots and Virtual Assistants**\\n- **Content Generation**\\n- **Text Summarization**\\n- **Code Generation**\\n- **Question Answering**\\n\\n### Getting Started\\n\\nTo use an LLM, you typically need access to an API or a library such as OpenAI's GPT, Google's PaLM, or open-source models like Llama or Falcon. You can interact with these models using Python libraries such as `langchain`.\\n\\nExample (pseudo-code):\\n\"), Document(metadata={'source': '../data/text_files/langchain_learning.txt'}, page_content='LangChain is a powerful framework designed to simplify the development of applications powered by large language models (LLMs). It provides tools and abstractions for connecting LLMs to various data sources, chaining together multiple components, and building advanced workflows such as Retrieval-Augmented Generation (RAG), chatbots, and more.\\n\\n### Key Concepts\\n\\n1. **Chains**: Chains are sequences of calls (to LLMs, APIs, or other utilities) that can be combined to perform complex tasks. For example, you can chain together a prompt template, an LLM, and a post-processing step.\\n\\n2. **Agents**: Agents use LLMs to decide which actions to take, such as querying a database or searching the web, based on user input.\\n\\n3. **Tools**: Tools are external functions or APIs that agents can use to perform specific tasks, like searching documents or performing calculations.\\n\\n4. **Memory**: LangChain provides memory modules to help applications remember previous interactions, enabling more context-aware conversations.\\n\\n5. **Document Loaders and Embeddings**: LangChain can ingest data from various sources (PDFs, CSVs, web pages, etc.), split them into chunks, and generate embeddings for semantic search and retrieval.\\n\\n### Common Use Cases\\n\\n- **Conversational AI and Chatbots**\\n- **Retrieval-Augmented Generation (RAG)**\\n- **Question Answering over Documents**\\n- **Automated Workflows and Agents**\\n- **Custom LLM Applications**\\n\\n### Getting Started\\n\\nTo use LangChain, install it via pip:\\n1. Install LangChain and its dependencies using pip:\\n\\n   ```\\n   pip install langchain\\n   ```\\n\\n2. Import the necessary modules in your Python script. For example, to use a language model and a prompt template:\\n\\n   ```python\\n   from langchain.llms import OpenAI\\n   from langchain.prompts import PromptTemplate\\n   ```\\n\\n3. Create a prompt template to structure the input for your language model. Prompt templates help standardize the way you interact with LLMs:\\n\\n   ```python\\n   prompt = PromptTemplate(\\n       input_variables=[\"topic\"],\\n       template=\"Tell me about {topic}.\"\\n   )\\n   ```\\n\\n4. Initialize your language model. LangChain supports various providers, including OpenAI, Hugging Face, and more. Here’s an example with OpenAI:\\n\\n   ```python\\n   llm = OpenAI(openai_api_key=\"your-api-key\")\\n   ```\\n\\n5. Combine the prompt and the LLM into a chain. Chains allow you to link together multiple steps, such as formatting a prompt and generating a response:\\n\\n   ```python\\n   from langchain.chains import LLMChain\\n\\n   chain = LLMChain(llm=llm, prompt=prompt)\\n   ```\\n\\n6. Run the chain with your desired input to generate a response:\\n\\n   ```python\\n   response = chain.run(\"LangChain\")\\n   print(response)\\n   ```\\n\\n7. To work with documents, use document loaders. LangChain provides loaders for PDFs, text files, web pages, and more. For example, to load a text file:\\n\\n   ```python\\n   from langchain.document_loaders import TextLoader\\n\\n   loader = TextLoader(\"example.txt\")\\n   documents = loader.load()\\n   ```\\n\\n8. Split documents into manageable chunks for processing and embedding. This is useful for tasks like semantic search:\\n\\n   ```python\\n   from langchain.text_splitter import CharacterTextSplitter\\n\\n   text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\\n   docs = text_splitter.split_documents(documents)\\n   ```\\n\\n9. Generate embeddings for each chunk to enable semantic search and retrieval. LangChain integrates with embedding providers like OpenAI and Hugging Face:\\n\\n   ```python\\n   from langchain.embeddings import OpenAIEmbeddings\\n\\n   embeddings = OpenAIEmbeddings(openai_api_key=\"your-api-key\")\\n   doc_embeddings = embeddings.embed_documents([doc.page_content for doc in docs])\\n   ```\\n\\n10. Store and search embeddings using a vector database. LangChain supports vector stores like FAISS, Pinecone, and Chroma. For example, using FAISS:\\n\\n    ```python\\n    from langchain.vectorstores import FAISS\\n\\n    db = FAISS.from_documents(docs, embeddings)\\n    results = db.similarity_search(\"What is LangChain?\", k=3)\\n    for result in results:\\n        print(result.page_content)\\n    ```\\n\\nThese steps provide a basic workflow for building applications with LangChain. The framework is highly modular, allowing you to customize each component to fit your specific needs. You can chain together multiple LLMs, integrate external APIs as tools, and build sophisticated agents that reason and act based on user input.\\n\\nLangChain also supports memory modules, which enable your applications to remember previous interactions. This is especially useful for building conversational agents that need to maintain context over multiple turns.\\n\\nFor more advanced use cases, you can create custom tools and integrate them with agents. Tools can be anything from a calculator to a web search API, and agents can decide when and how to use them based on the conversation.\\n\\nLangChain’s document loaders and text splitters make it easy to ingest and preprocess data from a variety of sources. This is essential for building retrieval-augmented generation (RAG) systems that combine LLMs with external knowledge.\\n\\nThe framework is open-source and has an active community. You can find extensive documentation, tutorials, and example projects on the official LangChain GitHub repository and website.\\n\\nBy leveraging LangChain, you can rapidly prototype and deploy powerful LLM-driven applications, from chatbots and virtual assistants to knowledge management systems and automated workflows.\\n\\nWhether you are a beginner or an experienced developer, LangChain provides the building blocks you need to harness the full potential of large language models in your projects.\\n\\n')]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "# Example usage:\n",
    "loader = DirectoryLoader(\"../data/text_files/\", loader_cls=TextLoader, loader_kwargs={\"encoding\": \"utf-8\"}, show_progress=False, glob=\"*.txt\")\n",
    "documents = loader.load()\n",
    "print(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948629ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
